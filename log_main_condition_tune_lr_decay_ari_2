Using backend: pytorch






generate file in current dir...
+ results/2021-07-18 23:25:31


generate file in current dir...
+ results/2021-07-18 23:25:31/train


generate file in current dir...
+ results/2021-07-18 23:25:31/eval


generate file in current dir...
+ results/2021-07-18 23:25:31/visualize






--------------
time size: 101
node size: 44
edge size: 2
--------------
[I 2021-07-18 23:25:42,315] Using an existing study with name 'condition_tune_twitter' instead of creating a new one.
Epoch: [1/1000]:
train loss: 7044561.250000
 valid loss: 780304.500000
Epoch: [2/1000]:
train loss: 6954650.937500
 valid loss: 773340.750000
Epoch: [3/1000]:
train loss: 6918860.281250
 valid loss: 769910.250000
Epoch: [4/1000]:
train loss: 6879599.429688
 valid loss: 766966.500000
Epoch: [5/1000]:
train loss: 6864327.828125
 valid loss: 765967.812500
Epoch: [6/1000]:
train loss: 6855550.164062
 valid loss: 764767.250000
Epoch: [7/1000]:
train loss: 6847896.718750
 valid loss: 764329.375000
Epoch: [8/1000]:
train loss: 6843893.437500
 valid loss: 762463.062500
Epoch: [9/1000]:
train loss: 6815960.445312
 valid loss: 760429.625000
Epoch: [10/1000]:
train loss: 6808946.421875
 valid loss: 760006.250000
Epoch: [11/1000]:
train loss: 6802896.132812
 valid loss: 759031.000000
Epoch: [12/1000]:
train loss: 6795980.304688
 valid loss: 758576.125000
Epoch: [13/1000]:
train loss: 6792535.398438
 valid loss: 758255.250000
Epoch: [14/1000]:
train loss: 6788196.750000
 valid loss: 757485.312500
Epoch: [15/1000]:
train loss: 6782961.679688
 valid loss: 754856.625000
Epoch: [16/1000]:
train loss: 6740875.359375
 valid loss: 751785.125000
Epoch: [17/1000]:
train loss: 6731232.476562
 valid loss: 751142.687500
Epoch: [18/1000]:
train loss: 6727274.304688
 valid loss: 750925.750000
Epoch: [19/1000]:
train loss: 6724360.886719
 valid loss: 750188.062500
Epoch: [20/1000]:
train loss: 6715716.468750
 valid loss: 749252.187500
Epoch: [21/1000]:
train loss: 6700579.507812
 valid loss: 746792.687500
Epoch: [22/1000]:
train loss: 6688087.500000
 valid loss: 746258.625000
Epoch: [23/1000]:
train loss: 6683542.265625
 valid loss: 745848.000000
Epoch: [24/1000]:
train loss: 6672953.187500
 valid loss: 744070.687500
Epoch: [25/1000]:
train loss: 6664470.312500
 valid loss: 743544.187500
Epoch: [26/1000]:
train loss: 6660049.410156
 valid loss: 743062.937500
Epoch: [27/1000]:
train loss: 6655387.101562
 valid loss: 742527.625000
Epoch: [28/1000]:
train loss: 6650726.160156
 valid loss: 742075.187500
Epoch: [29/1000]:
train loss: 6647451.546875
 valid loss: 741858.187500
Epoch: [30/1000]:
train loss: 6645607.289062
 valid loss: 741702.937500
Epoch: [31/1000]:
train loss: 6643569.109375
 valid loss: 741433.875000
Epoch: [32/1000]:
train loss: 6640124.546875
 valid loss: 740836.875000
Epoch: [33/1000]:
train loss: 6635896.515625
 valid loss: 740509.375000
Epoch: [34/1000]:
train loss: 6633025.375000
 valid loss: 739900.875000
Epoch: [35/1000]:
train loss: 6627722.175781
 valid loss: 739360.687500
Epoch: [36/1000]:
train loss: 6624549.812500
 valid loss: 739117.562500
Epoch: [37/1000]:
train loss: 6622449.601562
 valid loss: 738994.062500
Epoch: [38/1000]:
train loss: 6621272.093750
 valid loss: 738853.625000
Epoch: [39/1000]:
train loss: 6619870.171875
 valid loss: 738743.000000
Epoch: [40/1000]:
train loss: 6618904.656250
 valid loss: 738681.375000
Epoch: [41/1000]:
train loss: 6617879.851562
 valid loss: 738582.187500
Epoch: [42/1000]:
train loss: 6616305.726562
 valid loss: 738303.625000
Epoch: [43/1000]:
train loss: 6613463.898438
 valid loss: 737918.937500
Epoch: [44/1000]:
train loss: 6610390.570312
 valid loss: 737651.562500
Epoch: [45/1000]:
train loss: 6608569.839844
 valid loss: 737521.625000
Epoch: [46/1000]:
train loss: 6607399.261719
 valid loss: 737428.187500
Epoch: [47/1000]:
train loss: 6606298.367188
 valid loss: 737325.812500
Epoch: [48/1000]:
train loss: 6605008.386719
 valid loss: 736994.187500
Epoch: [49/1000]:
train loss: 6602352.898438
 valid loss: 736796.812500
Epoch: [50/1000]:
train loss: 6599685.164062
 valid loss: 735930.187500
Epoch: [51/1000]:
train loss: 6593368.421875
 valid loss: 735624.875000
Epoch: [52/1000]:
train loss: 6590320.726562
 valid loss: 735242.562500
Epoch: [53/1000]:
train loss: 6588167.007812
 valid loss: 735100.375000
Epoch: [54/1000]:
train loss: 6587063.742188
 valid loss: 735015.625000
Epoch: [55/1000]:
train loss: 6585782.996094
 valid loss: 734839.937500
Epoch: [56/1000]:
train loss: 6584473.210938
 valid loss: 734653.625000
Epoch: [57/1000]:
train loss: 6581933.925781
 valid loss: 734307.437500
Epoch: [58/1000]:
train loss: 6579434.328125
 valid loss: 734133.125000
Epoch: [59/1000]:
train loss: 6578221.570312
 valid loss: 733952.812500
Epoch: [60/1000]:
train loss: 6576228.132812
 valid loss: 733828.812500
Epoch: [61/1000]:
train loss: 6575360.781250
 valid loss: 733761.000000
Epoch: [62/1000]:
train loss: 6574569.421875
 valid loss: 733722.562500
Epoch: [63/1000]:
train loss: 6573796.570312
 valid loss: 733653.250000
Epoch: [64/1000]:
train loss: 6573253.554688
 valid loss: 733592.062500
Epoch: [65/1000]:
train loss: 6572830.453125
 valid loss: 733561.750000
Epoch: [66/1000]:
train loss: 6572002.601562
 valid loss: 733416.687500
Epoch: [67/1000]:
train loss: 6570768.910156
 valid loss: 733320.625000
Epoch: [68/1000]:
train loss: 6569916.472656
 valid loss: 733201.937500
Epoch: [69/1000]:
train loss: 6568745.375000
 valid loss: 733123.187500
Epoch: [70/1000]:
train loss: 6567722.984375
 valid loss: 733005.312500
Epoch: [71/1000]:
train loss: 6565350.828125
 valid loss: 732652.937500
Epoch: [72/1000]:
train loss: 6562197.515625
 valid loss: 732289.062500
Epoch: [73/1000]:
train loss: 6559815.625000
 valid loss: 732079.437500
Epoch: [74/1000]:
train loss: 6558749.367188
 valid loss: 732001.750000
Epoch: [75/1000]:
train loss: 6558086.839844
 valid loss: 731980.937500
Epoch: [76/1000]:
train loss: 6556752.011719
 valid loss: 731779.187500
Epoch: [77/1000]:
train loss: 6555301.511719
 valid loss: 731635.875000
Epoch: [78/1000]:
train loss: 6554081.070312
 valid loss: 731507.625000
Epoch: [79/1000]:
train loss: 6552545.503906
 valid loss: 731269.875000
Epoch: [80/1000]:
train loss: 6551164.539062
 valid loss: 731263.812500
Epoch: [81/1000]:
train loss: 6550548.980469
 valid loss: 731178.937500
Epoch: [82/1000]:
train loss: 6549850.652344
 valid loss: 731148.687500
Epoch: [83/1000]:
train loss: 6549344.792969
 valid loss: 731114.812500
Epoch: [84/1000]:
train loss: 6546577.664062
 valid loss: 730636.312500
Epoch: [85/1000]:
train loss: 6544804.066406
 valid loss: 730527.250000
Epoch: [86/1000]:
train loss: 6544024.476562
 valid loss: 730504.750000
Epoch: [87/1000]:
train loss: 6542630.273438
 valid loss: 730358.500000
Epoch: [88/1000]:
train loss: 6541309.988281
 valid loss: 730188.500000
Epoch: [89/1000]:
train loss: 6539727.429688
 valid loss: 729846.875000
Epoch: [90/1000]:
train loss: 6537230.476562
[W 2021-07-18 23:42:03,076] Trial 45 failed because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 156.00 MiB (GPU 0; 5.81 GiB total capacity; 3.91 GiB already allocated; 168.38 MiB free; 4.38 GiB reserved in total by PyTorch)')
Traceback (most recent call last):
  File "/home/yokoyama/.pyenv/versions/3.7.6/lib/python3.7/site-packages/optuna/study.py", line 709, in _run_trial
    result = func(trial)
  File "/home/yokoyama/networkgenerate/tune.py", line 184, in tuning_trial
    mu,sigma, *result = vae(valid_dataset)
  File "/home/yokoyama/.pyenv/versions/3.7.6/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/yokoyama/networkgenerate/model.py", line 394, in forward
    tu, tv, lu, lv, le = self.decoder(z, x)
  File "/home/yokoyama/.pyenv/versions/3.7.6/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/yokoyama/networkgenerate/model.py", line 167, in forward
    le = self.softmax(self.f_le(x))
  File "/home/yokoyama/.pyenv/versions/3.7.6/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/yokoyama/.pyenv/versions/3.7.6/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 96, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/yokoyama/.pyenv/versions/3.7.6/lib/python3.7/site-packages/torch/nn/functional.py", line 1847, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA out of memory. Tried to allocate 156.00 MiB (GPU 0; 5.81 GiB total capacity; 3.91 GiB already allocated; 168.38 MiB free; 4.38 GiB reserved in total by PyTorch)
Traceback (most recent call last):
  File "main.py", line 88, in <module>
    main(args)
  File "main.py", line 33, in main
    tune.conditional_tune(args)
  File "/home/yokoyama/networkgenerate/tune.py", line 219, in conditional_tune
    study.optimize(tuning_trial, n_trials=opt_epoch)
  File "/home/yokoyama/.pyenv/versions/3.7.6/lib/python3.7/site-packages/optuna/study.py", line 292, in optimize
    func, n_trials, timeout, catch, callbacks, gc_after_trial, None
  File "/home/yokoyama/.pyenv/versions/3.7.6/lib/python3.7/site-packages/optuna/study.py", line 654, in _optimize_sequential
    self._run_trial_and_callbacks(func, catch, callbacks, gc_after_trial)
  File "/home/yokoyama/.pyenv/versions/3.7.6/lib/python3.7/site-packages/optuna/study.py", line 685, in _run_trial_and_callbacks
    trial = self._run_trial(func, catch, gc_after_trial)
  File "/home/yokoyama/.pyenv/versions/3.7.6/lib/python3.7/site-packages/optuna/study.py", line 709, in _run_trial
    result = func(trial)
  File "/home/yokoyama/networkgenerate/tune.py", line 184, in tuning_trial
    mu,sigma, *result = vae(valid_dataset)
  File "/home/yokoyama/.pyenv/versions/3.7.6/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/yokoyama/networkgenerate/model.py", line 394, in forward
    tu, tv, lu, lv, le = self.decoder(z, x)
  File "/home/yokoyama/.pyenv/versions/3.7.6/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/yokoyama/networkgenerate/model.py", line 167, in forward
    le = self.softmax(self.f_le(x))
  File "/home/yokoyama/.pyenv/versions/3.7.6/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/yokoyama/.pyenv/versions/3.7.6/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 96, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/yokoyama/.pyenv/versions/3.7.6/lib/python3.7/site-packages/torch/nn/functional.py", line 1847, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA out of memory. Tried to allocate 156.00 MiB (GPU 0; 5.81 GiB total capacity; 3.91 GiB already allocated; 168.38 MiB free; 4.38 GiB reserved in total by PyTorch)
