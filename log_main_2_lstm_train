Using backend: pytorch
start preprocess...
--------------
time size: 101
node size: 44
edge size: 2
conditional size: 1
--------------
model_param = {'batch_size': 21, 'lr': 0.001, 'weight_decay': 0, 'clip_th': 0.00018132953639126497, 'de_hidden_size': 252, 'emb_size': 233, 'en_hidden_size': 108, 'rep_size': 65}
start conditional train...
Epoch: [1/1000]:
train:
----------------------------
 tu:
     loss:22321.479958
     acc:0.014076
 tv:
     loss:22065.398574
     acc:0.076209
 lu:
     loss:17654.326876
     acc:0.186506
 lv:
     loss:17975.740530
     acc:0.110295
 le:
     loss:1643.660064
     acc:0.989216
 encoder:
     loss:7.146228
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:21268.015332
     acc:0.014134
 tv:
     loss:20951.593164
     acc:0.088802
 lu:
     loss:16294.553027
     acc:0.318118
 lv:
     loss:17064.818555
     acc:0.119465
 le:
     loss:1464.794043
     acc:0.995667
 encoder:
     loss:0.028755
----------------------------


Epoch: [2/1000]:
train:
----------------------------
 tu:
     loss:22286.026095
     acc:0.024773
 tv:
     loss:21978.066997
     acc:0.091319
 lu:
     loss:16894.265784
     acc:0.340455
 lv:
     loss:17893.227017
     acc:0.122776
 le:
     loss:1537.141773
     acc:0.995668
 encoder:
     loss:0.026886
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:21193.182910
     acc:0.034371
 tv:
     loss:20934.415625
     acc:0.093135
 lu:
     loss:15838.993848
     acc:0.444077
 lv:
     loss:17036.380469
     acc:0.122899
 le:
     loss:1464.755322
     acc:0.995667
 encoder:
     loss:0.017942
----------------------------


Epoch: [3/1000]:
train:
----------------------------
 tu:
     loss:22213.858239
     acc:0.045357
 tv:
     loss:21969.204578
     acc:0.093001
 lu:
     loss:16414.358296
     acc:0.446650
 lv:
     loss:17874.604299
     acc:0.126819
 le:
     loss:1537.145678
     acc:0.995667
 encoder:
     loss:0.009102
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:21124.968457
     acc:0.067887
 tv:
     loss:20933.629492
     acc:0.093135
 lu:
     loss:15536.279590
     acc:0.456757
 lv:
     loss:17024.183789
     acc:0.129651
 le:
     loss:1464.783710
     acc:0.995667
 encoder:
     loss:0.002546
----------------------------


Epoch: [4/1000]:
train:
----------------------------
 tu:
     loss:22097.893850
     acc:0.074485
 tv:
     loss:21942.300168
     acc:0.099429
 lu:
     loss:16318.230321
     acc:0.453762
 lv:
     loss:17866.422613
     acc:0.129445
 le:
     loss:1537.189323
     acc:0.995667
 encoder:
     loss:0.004175
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:21002.206152
     acc:0.082606
 tv:
     loss:20834.590820
     acc:0.117821
 lu:
     loss:15518.165527
     acc:0.459984
 lv:
     loss:17020.529980
     acc:0.129531
 le:
     loss:1464.808563
     acc:0.995667
 encoder:
     loss:0.004835
----------------------------


Epoch: [5/1000]:
train:
----------------------------
 tu:
     loss:22010.501363
     acc:0.087919
 tv:
     loss:21852.123149
     acc:0.118345
 lu:
     loss:16294.653695
     acc:0.457409
 lv:
     loss:17844.792356
     acc:0.133069
 le:
     loss:1537.240466
     acc:0.995669
 encoder:
     loss:0.003716
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:20955.436816
     acc:0.089467
 tv:
     loss:20806.895996
     acc:0.121106
 lu:
     loss:15361.893848
     acc:0.499789
 lv:
     loss:16990.574609
     acc:0.135479
 le:
     loss:1464.876080
     acc:0.995667
 encoder:
     loss:0.005729
----------------------------


Epoch: [6/1000]:
train:
----------------------------
 tu:
     loss:21962.467387
     acc:0.098723
 tv:
     loss:21841.918094
     acc:0.120141
 lu:
     loss:15621.828000
     acc:0.608206
 lv:
     loss:17831.931062
     acc:0.135923
 le:
     loss:1537.436320
     acc:0.995668
 encoder:
     loss:0.023892
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:20895.892969
     acc:0.102921
 tv:
     loss:20802.350098
     acc:0.121946
 lu:
     loss:14587.596094
     acc:0.663756
 lv:
     loss:16977.608691
     acc:0.138731
 le:
     loss:1464.975574
     acc:0.995667
 encoder:
     loss:0.007093
----------------------------


Epoch: [7/1000]:
train:
----------------------------
 tu:
     loss:21909.226971
     acc:0.108258
 tv:
     loss:21832.176678
     acc:0.121564
 lu:
     loss:15319.436103
     acc:0.662072
 lv:
     loss:17796.634539
     acc:0.144321
 le:
     loss:1537.406173
     acc:0.995667
 encoder:
     loss:0.003858
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:20845.711328
     acc:0.113473
 tv:
     loss:20796.958594
     acc:0.123097
 lu:
     loss:14546.827051
     acc:0.670105
 lv:
     loss:16934.195996
     acc:0.149764
 le:
     loss:1464.903284
     acc:0.995667
 encoder:
     loss:0.004979
----------------------------


Epoch: [8/1000]:
train:
----------------------------
 tu:
     loss:21822.422329
     acc:0.126914
 tv:
     loss:21831.099087
     acc:0.121646
 lu:
     loss:15294.920864
     acc:0.665259
 lv:
     loss:17767.725961
     acc:0.151054
 le:
     loss:1537.313538
     acc:0.995668
 encoder:
     loss:0.006630
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:20744.798340
     acc:0.136617
 tv:
     loss:20797.724316
     acc:0.122650
 lu:
     loss:14537.748926
     acc:0.671258
 lv:
     loss:16924.209766
     acc:0.150539
 le:
     loss:1464.874329
     acc:0.995667
 encoder:
     loss:0.006415
----------------------------


Epoch: [9/1000]:
train:
----------------------------
 tu:
     loss:21726.234284
     acc:0.146169
 tv:
     loss:21827.128634
     acc:0.122502
 lu:
     loss:15285.759323
     acc:0.666528
 lv:
     loss:17758.282692
     acc:0.152577
 le:
     loss:1537.267985
     acc:0.995669
 encoder:
     loss:0.006528
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:20690.145508
     acc:0.145235
 tv:
     loss:20772.289062
     acc:0.128950
 lu:
     loss:14529.518213
     acc:0.672683
 lv:
     loss:16918.750684
     acc:0.152578
 le:
     loss:1464.836066
     acc:0.995667
 encoder:
     loss:0.010661
----------------------------


Epoch: [10/1000]:
train:
----------------------------
 tu:
     loss:21696.662859
     acc:0.150626
 tv:
     loss:21788.087970
     acc:0.131804
 lu:
     loss:15116.328988
     acc:0.706991
 lv:
     loss:17753.906727
     acc:0.153847
 le:
     loss:1537.239459
     acc:0.995667
 encoder:
     loss:0.013248
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:20670.070020
     acc:0.148852
 tv:
     loss:20749.253320
     acc:0.135003
 lu:
     loss:14297.929736
     acc:0.725234
 lv:
     loss:16916.017383
     acc:0.152288
 le:
     loss:1464.808575
     acc:0.995667
 encoder:
     loss:0.010131
----------------------------


Epoch: [11/1000]:
train:
----------------------------
 tu:
     loss:21660.566156
     acc:0.157887
 tv:
     loss:21766.403229
     acc:0.136095
 lu:
     loss:15016.438897
     acc:0.724411
 lv:
     loss:17751.120923
     acc:0.154032
 le:
     loss:1537.218761
     acc:0.995670
 encoder:
     loss:0.009890
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:20624.208789
     acc:0.158698
 tv:
     loss:20740.078125
     acc:0.135797
 lu:
     loss:14282.506348
     acc:0.727543
 lv:
     loss:16913.350586
     acc:0.152847
 le:
     loss:1464.815802
     acc:0.995667
 encoder:
     loss:0.014098
----------------------------


Epoch: [12/1000]:
train:
----------------------------
 tu:
     loss:21618.845533
     acc:0.165977
 tv:
     loss:21756.372388
     acc:0.137434
 lu:
     loss:14999.893918
     acc:0.726768
 lv:
     loss:17746.226517
     acc:0.154991
 le:
     loss:1537.213545
     acc:0.995669
 encoder:
     loss:0.010286
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:20599.840527
     acc:0.163605
 tv:
     loss:20735.346094
     acc:0.136594
 lu:
     loss:14271.795215
     acc:0.728911
 lv:
     loss:16909.117969
     acc:0.154011
 le:
     loss:1464.806726
     acc:0.995667
 encoder:
     loss:0.007436
----------------------------


Epoch: [13/1000]:
train:
----------------------------
 tu:
     loss:21582.988974
     acc:0.173518
 tv:
     loss:21751.819563
     acc:0.138098
 lu:
     loss:14989.349337
     acc:0.728555
 lv:
     loss:17742.475983
     acc:0.156325
 le:
     loss:1537.197305
     acc:0.995668
 encoder:
     loss:0.008165
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:20567.292871
     acc:0.171234
 tv:
     loss:20731.853418
     acc:0.136936
 lu:
     loss:14263.884326
     acc:0.730551
 lv:
     loss:16907.150098
     acc:0.154210
 le:
     loss:1464.794019
     acc:0.995667
 encoder:
     loss:0.009565
----------------------------


Epoch: [14/1000]:
train:
----------------------------
 tu:
     loss:21552.358943
     acc:0.180056
 tv:
     loss:21742.214072
     acc:0.140314
 lu:
     loss:14982.405705
     acc:0.729694
 lv:
     loss:17739.965923
     acc:0.156480
 le:
     loss:1537.183044
     acc:0.995668
 encoder:
     loss:0.011729
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:20516.430176
     acc:0.184983
 tv:
     loss:20720.770410
     acc:0.139753
 lu:
     loss:14258.533301
     acc:0.731695
 lv:
     loss:16905.895410
     acc:0.154945
 le:
     loss:1464.780719
     acc:0.995667
 encoder:
     loss:0.007299
----------------------------


Epoch: [15/1000]:
train:
----------------------------
 tu:
     loss:21494.756768
     acc:0.192592
 tv:
     loss:21736.538404
     acc:0.141254
 lu:
     loss:14977.769929
     acc:0.730712
 lv:
     loss:17737.534339
     acc:0.157395
 le:
     loss:1537.166954
     acc:0.995667
 encoder:
     loss:0.015389
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:20471.369141
     acc:0.192785
 tv:
     loss:20717.278516
     acc:0.140703
 lu:
     loss:14253.797461
     acc:0.732736
 lv:
     loss:16904.797168
     acc:0.154945
 le:
     loss:1464.780750
     acc:0.995667
 encoder:
     loss:0.013991
----------------------------


Epoch: [16/1000]:
train:
----------------------------
 tu:
     loss:21464.572107
     acc:0.197575
 tv:
     loss:21729.281171
     acc:0.142825
 lu:
     loss:14970.773608
     acc:0.731915
 lv:
     loss:17736.647597
     acc:0.157380
 le:
     loss:1537.162629
     acc:0.995670
 encoder:
     loss:0.015492
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:20447.196484
     acc:0.197647
 tv:
     loss:20706.199219
     acc:0.143092
 lu:
     loss:14245.275342
     acc:0.734917
 lv:
     loss:16902.967871
     acc:0.155429
 le:
     loss:1464.772217
     acc:0.995667
 encoder:
     loss:0.011771
----------------------------


Epoch: [17/1000]:
train:
----------------------------
 tu:
     loss:21441.710642
     acc:0.201891
 tv:
     loss:21718.503373
     acc:0.145048
 lu:
     loss:14938.188976
     acc:0.741294
 lv:
     loss:17734.105639
     acc:0.158070
 le:
     loss:1537.156954
     acc:0.995666
 encoder:
     loss:0.014489
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:20437.942871
     acc:0.198839
 tv:
     loss:20702.953418
     acc:0.144340
 lu:
     loss:14079.384473
     acc:0.776468
 lv:
     loss:16902.164551
     acc:0.154702
 le:
     loss:1464.765051
     acc:0.995667
 encoder:
     loss:0.020189
----------------------------


Epoch: [18/1000]:
train:
----------------------------
 tu:
     loss:21434.591604
     acc:0.203314
 tv:
     loss:21716.501238
     acc:0.145459
 lu:
     loss:14800.519463
     acc:0.771270
 lv:
     loss:17732.479027
     acc:0.158529
 le:
     loss:1537.140155
     acc:0.995664
 encoder:
     loss:0.016273
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:20428.200684
     acc:0.200607
 tv:
     loss:20699.305664
     acc:0.144490
 lu:
     loss:14049.712646
     acc:0.778249
 lv:
     loss:16900.947852
     acc:0.155669
 le:
     loss:1464.762604
     acc:0.995667
 encoder:
     loss:0.013568
----------------------------


Epoch: [19/1000]:
train:
----------------------------
 tu:
     loss:21422.329204
     acc:0.205654
 tv:
     loss:21711.934650
     acc:0.146327
 lu:
     loss:14779.613122
     acc:0.773339
 lv:
     loss:17731.664528
     acc:0.158152
 le:
     loss:1537.145332
     acc:0.995667
 encoder:
     loss:0.014573
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:20420.472852
     acc:0.202031
 tv:
     loss:20696.348828
     acc:0.145233
 lu:
     loss:14039.763721
     acc:0.779647
 lv:
     loss:16899.927148
     acc:0.155506
 le:
     loss:1464.766553
     acc:0.995667
 encoder:
     loss:0.018029
----------------------------


Epoch: [20/1000]:
train:
----------------------------
 tu:
     loss:21395.547670
     acc:0.212294
 tv:
     loss:21708.817224
     acc:0.146829
 lu:
     loss:14767.577955
     acc:0.774569
 lv:
     loss:17730.308957
     acc:0.158440
 le:
     loss:1537.143678
     acc:0.995670
 encoder:
     loss:0.018517
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:20365.781250
     acc:0.216429
 tv:
     loss:20687.237695
     acc:0.147774
 lu:
     loss:14032.036865
     acc:0.781360
 lv:
     loss:16896.793750
     acc:0.157265
 le:
     loss:1464.759357
     acc:0.995667
 encoder:
     loss:0.017583
----------------------------


Epoch: [21/1000]:
train:
----------------------------
 tu:
     loss:21351.284702
     acc:0.221578
 tv:
     loss:21699.338038
     acc:0.149322
 lu:
     loss:14761.735692
     acc:0.775946
 lv:
     loss:17728.461676
     acc:0.159225
 le:
     loss:1537.136732
     acc:0.995665
 encoder:
     loss:0.018399
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:20325.516895
     acc:0.225255
 tv:
     loss:20680.642285
     acc:0.149058
 lu:
     loss:14027.921680
     acc:0.781636
 lv:
     loss:16897.558984
     acc:0.156087
 le:
     loss:1464.758984
     acc:0.995667
 encoder:
     loss:0.019887
----------------------------


Epoch: [22/1000]:
train:
----------------------------
 tu:
     loss:21300.089639
     acc:0.233031
 tv:
     loss:21689.959859
     acc:0.151343
 lu:
     loss:14757.708871
     acc:0.776649
 lv:
     loss:17727.032579
     acc:0.159286
 le:
     loss:1537.132003
     acc:0.995668
 encoder:
     loss:0.022941
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:20276.779199
     acc:0.235278
 tv:
     loss:20669.036719
     acc:0.151731
 lu:
     loss:14024.582178
     acc:0.782469
 lv:
     loss:16894.426074
     acc:0.157457
 le:
     loss:1464.753271
     acc:0.995667
 encoder:
     loss:0.023566
----------------------------


Epoch: [23/1000]:
train:
----------------------------
 tu:
     loss:21259.813976
     acc:0.241202
 tv:
     loss:21676.234591
     acc:0.154399
 lu:
     loss:14752.710211
     acc:0.777510
 lv:
     loss:17727.598224
     acc:0.159184
 le:
     loss:1537.132265
     acc:0.995667
 encoder:
     loss:0.020339
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:20236.060254
     acc:0.244777
 tv:
     loss:20660.550195
     acc:0.153353
 lu:
     loss:14020.001221
     acc:0.783103
 lv:
     loss:16895.297754
     acc:0.157098
 le:
     loss:1464.754150
     acc:0.995667
 encoder:
     loss:0.028287
----------------------------


Epoch: [24/1000]:
train:
----------------------------
 tu:
     loss:21221.387468
     acc:0.249087
 tv:
     loss:21670.410111
     acc:0.155328
 lu:
     loss:14748.146359
     acc:0.778273
 lv:
     loss:17726.046523
     acc:0.159423
 le:
     loss:1537.127649
     acc:0.995666
 encoder:
     loss:0.023451
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:20202.330273
     acc:0.251687
 tv:
     loss:20657.112793
     acc:0.154197
 lu:
     loss:14013.772754
     acc:0.784571
 lv:
     loss:16894.611523
     acc:0.156968
 le:
     loss:1464.755371
     acc:0.995667
 encoder:
     loss:0.018502
----------------------------


Epoch: [25/1000]:
train:
----------------------------
 tu:
     loss:21181.767260
     acc:0.257105
 tv:
     loss:21667.107104
     acc:0.155993
 lu:
     loss:14741.007631
     acc:0.779874
 lv:
     loss:17724.447788
     acc:0.159772
 le:
     loss:1537.130286
     acc:0.995667
 encoder:
     loss:0.018850
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:20171.910156
     acc:0.257360
 tv:
     loss:20655.144824
     acc:0.154144
 lu:
     loss:14008.292578
     acc:0.786151
 lv:
     loss:16894.030859
     acc:0.157127
 le:
     loss:1464.755359
     acc:0.995667
 encoder:
     loss:0.018265
----------------------------


Epoch: [26/1000]:
train:
----------------------------
 tu:
     loss:21153.710517
     acc:0.262128
 tv:
     loss:21664.149357
     acc:0.156419
 lu:
     loss:14734.948606
     acc:0.781081
 lv:
     loss:17722.038109
     acc:0.160325
 le:
     loss:1537.129051
     acc:0.995667
 encoder:
     loss:0.021751
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:20136.210352
     acc:0.264809
 tv:
     loss:20654.861328
     acc:0.154360
 lu:
     loss:14005.664551
     acc:0.786080
 lv:
     loss:16892.688770
     acc:0.158072
 le:
     loss:1464.753595
     acc:0.995667
 encoder:
     loss:0.018858
----------------------------


Epoch: [27/1000]:
train:
----------------------------
 tu:
     loss:21121.869742
     acc:0.268988
 tv:
     loss:21662.108898
     acc:0.156703
 lu:
     loss:14730.154024
     acc:0.781841
 lv:
     loss:17720.727902
     acc:0.160445
 le:
     loss:1537.125802
     acc:0.995669
 encoder:
     loss:0.020229
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:20100.924512
     acc:0.274440
 tv:
     loss:20652.802539
     acc:0.154890
 lu:
     loss:13997.671191
     acc:0.788315
 lv:
     loss:16889.957422
     acc:0.158268
 le:
     loss:1464.750079
     acc:0.995667
 encoder:
     loss:0.023635
----------------------------


Epoch: [28/1000]:
train:
----------------------------
 tu:
     loss:21070.221407
     acc:0.280922
 tv:
     loss:21660.646587
     acc:0.157016
 lu:
     loss:14726.510470
     acc:0.782724
 lv:
     loss:17719.604254
     acc:0.160750
 le:
     loss:1537.123508
     acc:0.995668
 encoder:
     loss:0.019350
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:20057.255664
     acc:0.283240
 tv:
     loss:20652.707324
     acc:0.154845
 lu:
     loss:13995.188281
     acc:0.788548
 lv:
     loss:16891.550000
     acc:0.158235
 le:
     loss:1464.748651
     acc:0.995667
 encoder:
     loss:0.016678
----------------------------


Epoch: [29/1000]:
train:
----------------------------
 tu:
     loss:21041.206066
     acc:0.285949
 tv:
     loss:21658.485068
     acc:0.157455
 lu:
     loss:14721.093387
     acc:0.783814
 lv:
     loss:17717.747922
     acc:0.161238
 le:
     loss:1537.122882
     acc:0.995667
 encoder:
     loss:0.018797
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:20043.304688
     acc:0.285071
 tv:
     loss:20651.680273
     acc:0.154896
 lu:
     loss:13991.917090
     acc:0.788851
 lv:
     loss:16887.973145
     acc:0.159335
 le:
     loss:1464.746826
     acc:0.995667
 encoder:
     loss:0.021736
----------------------------


Epoch: [30/1000]:
train:
----------------------------
 tu:
     loss:21027.429483
     acc:0.287948
 tv:
     loss:21656.123751
     acc:0.157867
 lu:
     loss:14716.154990
     acc:0.784634
 lv:
     loss:17716.415857
     acc:0.161423
 le:
     loss:1537.120728
     acc:0.995668
 encoder:
     loss:0.026284
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:20036.825293
     acc:0.286199
 tv:
     loss:20650.876270
     acc:0.155063
 lu:
     loss:13985.444043
     acc:0.790316
 lv:
     loss:16887.878125
     acc:0.158483
 le:
     loss:1464.748920
     acc:0.995667
 encoder:
     loss:0.030108
----------------------------


Epoch: [31/1000]:
train:
----------------------------
 tu:
     loss:21020.005451
     acc:0.289582
 tv:
     loss:21655.214310
     acc:0.158067
 lu:
     loss:14651.191088
     acc:0.801393
 lv:
     loss:17714.632824
     acc:0.162048
 le:
     loss:1537.115951
     acc:0.995668
 encoder:
     loss:0.024469
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:20008.285352
     acc:0.294121
 tv:
     loss:20643.212109
     acc:0.157114
 lu:
     loss:13878.381738
     acc:0.814727
 lv:
     loss:16886.515039
     acc:0.158625
 le:
     loss:1464.744037
     acc:0.995667
 encoder:
     loss:0.017775
----------------------------


Epoch: [32/1000]:
train:
----------------------------
 tu:
     loss:20983.891022
     acc:0.298606
 tv:
     loss:21645.133119
     acc:0.160392
 lu:
     loss:14576.900175
     acc:0.816636
 lv:
     loss:17715.137207
     acc:0.161587
 le:
     loss:1537.112092
     acc:0.995666
 encoder:
     loss:0.024831
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19966.129199
     acc:0.302978
 tv:
     loss:20625.870605
     acc:0.161102
 lu:
     loss:13795.810303
     acc:0.833572
 lv:
     loss:16887.349023
     acc:0.159204
 le:
     loss:1464.743573
     acc:0.995667
 encoder:
     loss:0.018156
----------------------------


Epoch: [33/1000]:
train:
----------------------------
 tu:
     loss:20949.749943
     acc:0.305277
 tv:
     loss:21633.602709
     acc:0.162846
 lu:
     loss:14527.819983
     acc:0.825709
 lv:
     loss:17713.983887
     acc:0.162059
 le:
     loss:1537.112393
     acc:0.995667
 encoder:
     loss:0.023347
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19956.880762
     acc:0.304044
 tv:
     loss:20623.319043
     acc:0.161328
 lu:
     loss:13785.258203
     acc:0.834860
 lv:
     loss:16887.189355
     acc:0.159122
 le:
     loss:1464.743225
     acc:0.995667
 encoder:
     loss:0.016647
----------------------------


Epoch: [34/1000]:
train:
----------------------------
 tu:
     loss:20931.682379
     acc:0.308063
 tv:
     loss:21629.514569
     acc:0.163467
 lu:
     loss:14517.865087
     acc:0.827035
 lv:
     loss:17710.880144
     acc:0.162914
 le:
     loss:1537.113413
     acc:0.995666
 encoder:
     loss:0.022599
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19926.870508
     acc:0.310442
 tv:
     loss:20621.127832
     acc:0.161812
 lu:
     loss:13778.767773
     acc:0.836101
 lv:
     loss:16885.845508
     acc:0.159344
 le:
     loss:1464.743463
     acc:0.995667
 encoder:
     loss:0.022316
----------------------------


Epoch: [35/1000]:
train:
----------------------------
 tu:
     loss:20905.766227
     acc:0.313126
 tv:
     loss:21626.294831
     acc:0.164123
 lu:
     loss:14508.309525
     acc:0.828781
 lv:
     loss:17709.180925
     acc:0.163180
 le:
     loss:1537.111364
     acc:0.995665
 encoder:
     loss:0.018704
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19923.559375
     acc:0.310878
 tv:
     loss:20619.270898
     acc:0.162252
 lu:
     loss:13777.817969
     acc:0.835942
 lv:
     loss:16885.630859
     acc:0.159234
 le:
     loss:1464.741882
     acc:0.995667
 encoder:
     loss:0.018053
----------------------------


Epoch: [36/1000]:
train:
----------------------------
 tu:
     loss:20901.830703
     acc:0.313591
 tv:
     loss:21621.387241
     acc:0.165118
 lu:
     loss:14503.531125
     acc:0.829368
 lv:
     loss:17709.060172
     acc:0.163080
 le:
     loss:1537.109243
     acc:0.995667
 encoder:
     loss:0.023619
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19916.484082
     acc:0.312593
 tv:
     loss:20608.926172
     acc:0.164594
 lu:
     loss:13772.405469
     acc:0.837582
 lv:
     loss:16883.941602
     acc:0.160013
 le:
     loss:1464.741156
     acc:0.995667
 encoder:
     loss:0.023098
----------------------------


Epoch: [37/1000]:
train:
----------------------------
 tu:
     loss:20881.387820
     acc:0.318811
 tv:
     loss:21603.127180
     acc:0.169393
 lu:
     loss:14499.036712
     acc:0.829966
 lv:
     loss:17707.031557
     acc:0.163448
 le:
     loss:1537.108282
     acc:0.995669
 encoder:
     loss:0.023215
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19893.688770
     acc:0.317969
 tv:
     loss:20584.174512
     acc:0.170000
 lu:
     loss:13768.086377
     acc:0.838330
 lv:
     loss:16883.507617
     acc:0.159552
 le:
     loss:1464.739978
     acc:0.995667
 encoder:
     loss:0.017513
----------------------------


Epoch: [38/1000]:
train:
----------------------------
 tu:
     loss:20867.115461
     acc:0.321287
 tv:
     loss:21586.224382
     acc:0.172723
 lu:
     loss:14495.319506
     acc:0.830658
 lv:
     loss:17706.154376
     acc:0.163880
 le:
     loss:1537.106438
     acc:0.995667
 encoder:
     loss:0.024217
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19886.589062
     acc:0.319080
 tv:
     loss:20578.721094
     acc:0.170810
 lu:
     loss:13764.753564
     acc:0.839178
 lv:
     loss:16884.050684
     acc:0.159422
 le:
     loss:1464.739697
     acc:0.995667
 encoder:
     loss:0.026733
----------------------------


Epoch: [39/1000]:
train:
----------------------------
 tu:
     loss:20859.349519
     acc:0.322553
 tv:
     loss:21574.801837
     acc:0.175146
 lu:
     loss:14490.819359
     acc:0.831723
 lv:
     loss:17704.394088
     acc:0.164268
 le:
     loss:1537.106398
     acc:0.995665
 encoder:
     loss:0.032027
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19883.316309
     acc:0.319622
 tv:
     loss:20559.697070
     acc:0.175496
 lu:
     loss:13765.121973
     acc:0.838791
 lv:
     loss:16881.392383
     acc:0.160525
 le:
     loss:1464.740869
     acc:0.995667
 encoder:
     loss:0.031624
----------------------------


Epoch: [40/1000]:
train:
----------------------------
 tu:
     loss:20852.710358
     acc:0.323572
 tv:
     loss:21563.755258
     acc:0.177300
 lu:
     loss:14488.052712
     acc:0.832387
 lv:
     loss:17703.576388
     acc:0.164509
 le:
     loss:1537.107104
     acc:0.995665
 encoder:
     loss:0.054616
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19873.949902
     acc:0.321757
 tv:
     loss:20555.249609
     acc:0.175889
 lu:
     loss:13759.560352
     acc:0.840136
 lv:
     loss:16882.006836
     acc:0.159800
 le:
     loss:1464.741852
     acc:0.995667
 encoder:
     loss:0.037966
----------------------------


Epoch: [41/1000]:
train:
----------------------------
 tu:
     loss:20844.863804
     acc:0.325322
 tv:
     loss:21537.561910
     acc:0.183287
 lu:
     loss:14483.988327
     acc:0.833160
 lv:
     loss:17701.603811
     acc:0.164885
 le:
     loss:1537.111520
     acc:0.995666
 encoder:
     loss:0.077257
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19870.739844
     acc:0.322589
 tv:
     loss:20510.851660
     acc:0.185582
 lu:
     loss:13756.943896
     acc:0.840330
 lv:
     loss:16880.286426
     acc:0.160375
 le:
     loss:1464.741003
     acc:0.995667
 encoder:
     loss:0.020584
----------------------------


Epoch: [42/1000]:
train:
----------------------------
 tu:
     loss:20832.141658
     acc:0.328393
 tv:
     loss:21515.091876
     acc:0.187915
 lu:
     loss:14480.067917
     acc:0.833616
 lv:
     loss:17701.789880
     acc:0.164576
 le:
     loss:1537.108799
     acc:0.995670
 encoder:
     loss:0.040523
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19838.135254
     acc:0.330475
 tv:
     loss:20506.210254
     acc:0.186679
 lu:
     loss:13752.764355
     acc:0.841990
 lv:
     loss:16880.776074
     acc:0.159978
 le:
     loss:1464.741504
     acc:0.995667
 encoder:
     loss:0.027438
----------------------------


Epoch: [43/1000]:
train:
----------------------------
 tu:
     loss:20806.769531
     acc:0.333902
 tv:
     loss:21497.403616
     acc:0.191669
 lu:
     loss:14476.102528
     acc:0.834678
 lv:
     loss:17699.172965
     acc:0.165399
 le:
     loss:1537.112292
     acc:0.995665
 encoder:
     loss:0.031699
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19833.324023
     acc:0.330746
 tv:
     loss:20488.149219
     acc:0.190539
 lu:
     loss:13749.868896
     acc:0.842648
 lv:
     loss:16878.593555
     acc:0.161273
 le:
     loss:1464.741663
     acc:0.995667
 encoder:
     loss:0.027606
----------------------------


Epoch: [44/1000]:
train:
----------------------------
 tu:
     loss:20800.834620
     acc:0.334543
 tv:
     loss:21490.332429
     acc:0.192915
 lu:
     loss:14472.510742
     acc:0.835290
 lv:
     loss:17698.532840
     acc:0.165238
 le:
     loss:1537.108708
     acc:0.995668
 encoder:
     loss:0.034326
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19830.002246
     acc:0.331460
 tv:
     loss:20484.549707
     acc:0.191663
 lu:
     loss:13751.381885
     acc:0.841696
 lv:
     loss:16878.807324
     acc:0.160216
 le:
     loss:1464.742285
     acc:0.995667
 encoder:
     loss:0.027284
----------------------------


Epoch: [45/1000]:
train:
----------------------------
 tu:
     loss:20797.327671
     acc:0.335123
 tv:
     loss:21485.430278
     acc:0.193860
 lu:
     loss:14470.847588
     acc:0.835399
 lv:
     loss:17697.105560
     acc:0.165488
 le:
     loss:1537.106738
     acc:0.995668
 encoder:
     loss:0.036286
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19828.810352
     acc:0.331513
 tv:
     loss:20484.943555
     acc:0.191389
 lu:
     loss:13748.214941
     acc:0.842265
 lv:
     loss:16876.979102
     acc:0.160575
 le:
     loss:1464.741058
     acc:0.995667
 encoder:
     loss:0.024244
----------------------------


Epoch: [46/1000]:
train:
----------------------------
 tu:
     loss:20792.041561
     acc:0.336350
 tv:
     loss:21468.137218
     acc:0.197857
 lu:
     loss:14456.309831
     acc:0.838726
 lv:
     loss:17694.831952
     acc:0.166187
 le:
     loss:1537.107059
     acc:0.995668
 encoder:
     loss:0.034960
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19804.710156
     acc:0.338239
 tv:
     loss:20448.887793
     acc:0.199923
 lu:
     loss:13728.941406
     acc:0.846618
 lv:
     loss:16877.715723
     acc:0.160235
 le:
     loss:1464.740137
     acc:0.995667
 encoder:
     loss:0.042021
----------------------------


Epoch: [47/1000]:
train:
----------------------------
 tu:
     loss:20767.391522
     acc:0.342259
 tv:
     loss:21449.694847
     acc:0.201847
 lu:
     loss:14449.102232
     acc:0.839979
 lv:
     loss:17694.352335
     acc:0.166165
 le:
     loss:1537.104303
     acc:0.995668
 encoder:
     loss:0.033363
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19793.544141
     acc:0.339841
 tv:
     loss:20442.532617
     acc:0.200597
 lu:
     loss:13726.673047
     acc:0.846721
 lv:
     loss:16876.297266
     acc:0.160427
 le:
     loss:1464.739429
     acc:0.995667
 encoder:
     loss:0.018896
----------------------------


Epoch: [48/1000]:
train:
----------------------------
 tu:
     loss:20760.503043
     acc:0.343034
 tv:
     loss:21443.210744
     acc:0.202838
 lu:
     loss:14444.703863
     acc:0.840983
 lv:
     loss:17691.969340
     acc:0.166896
 le:
     loss:1537.106339
     acc:0.995667
 encoder:
     loss:0.061077
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19791.115430
     acc:0.340430
 tv:
     loss:20439.781445
     acc:0.201272
 lu:
     loss:13724.828223
     acc:0.847247
 lv:
     loss:16876.119531
     acc:0.160494
 le:
     loss:1464.739178
     acc:0.995667
 encoder:
     loss:0.025816
----------------------------


Epoch: [49/1000]:
train:
----------------------------
 tu:
     loss:20756.656636
     acc:0.343571
 tv:
     loss:21434.612509
     acc:0.204774
 lu:
     loss:14441.640682
     acc:0.841666
 lv:
     loss:17692.029320
     acc:0.166692
 le:
     loss:1537.103831
     acc:0.995667
 encoder:
     loss:0.035275
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19790.297852
     acc:0.340390
 tv:
     loss:20417.414844
     acc:0.206047
 lu:
     loss:13721.545361
     acc:0.847628
 lv:
     loss:16874.030957
     acc:0.162021
 le:
     loss:1464.739191
     acc:0.995667
 encoder:
     loss:0.046210
----------------------------


Epoch: [50/1000]:
train:
----------------------------
 tu:
     loss:20754.522609
     acc:0.343737
 tv:
     loss:21412.912268
     acc:0.209317
 lu:
     loss:14439.629928
     acc:0.841919
 lv:
     loss:17690.421546
     acc:0.167316
 le:
     loss:1537.103988
     acc:0.995667
 encoder:
     loss:0.028204
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19788.682520
     acc:0.340589
 tv:
     loss:20407.394141
     acc:0.208205
 lu:
     loss:13718.543262
     acc:0.848934
 lv:
     loss:16871.962598
     acc:0.162585
 le:
     loss:1464.737048
     acc:0.995667
 encoder:
     loss:0.021915
----------------------------


Epoch: [51/1000]:
train:
----------------------------
 tu:
     loss:20752.948106
     acc:0.344155
 tv:
     loss:21406.423306
     acc:0.210583
 lu:
     loss:14436.583530
     acc:0.842317
 lv:
     loss:17682.513002
     acc:0.169038
 le:
     loss:1537.099819
     acc:0.995668
 encoder:
     loss:0.035882
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19784.240332
     acc:0.341567
 tv:
     loss:20405.169727
     acc:0.208455
 lu:
     loss:13716.493652
     acc:0.849395
 lv:
     loss:16853.373242
     acc:0.166334
 le:
     loss:1464.736633
     acc:0.995667
 encoder:
     loss:0.024870
----------------------------


Epoch: [52/1000]:
train:
----------------------------
 tu:
     loss:20727.604890
     acc:0.350435
 tv:
     loss:21401.782158
     acc:0.211372
 lu:
     loss:14433.269531
     acc:0.843362
 lv:
     loss:17666.164880
     acc:0.172530
 le:
     loss:1537.097602
     acc:0.995667
 encoder:
     loss:0.028466
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19727.528418
     acc:0.355244
 tv:
     loss:20402.145898
     acc:0.209344
 lu:
     loss:13713.104248
     acc:0.850452
 lv:
     loss:16853.375977
     acc:0.165492
 le:
     loss:1464.734857
     acc:0.995667
 encoder:
     loss:0.027986
----------------------------


Epoch: [53/1000]:
train:
----------------------------
 tu:
     loss:20690.614792
     acc:0.358312
 tv:
     loss:21399.672307
     acc:0.211488
 lu:
     loss:14431.413904
     acc:0.843745
 lv:
     loss:17665.861748
     acc:0.172492
 le:
     loss:1537.099344
     acc:0.995667
 encoder:
     loss:0.028239
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19717.408496
     acc:0.356561
 tv:
     loss:20399.727539
     acc:0.209662
 lu:
     loss:13714.857080
     acc:0.849716
 lv:
     loss:16855.374414
     acc:0.165181
 le:
     loss:1464.735718
     acc:0.995667
 encoder:
     loss:0.024356
----------------------------


Epoch: [54/1000]:
train:
----------------------------
 tu:
     loss:20681.091172
     acc:0.359713
 tv:
     loss:21383.288188
     acc:0.215150
 lu:
     loss:14429.059434
     acc:0.843976
 lv:
     loss:17664.826728
     acc:0.172698
 le:
     loss:1537.092724
     acc:0.995668
 encoder:
     loss:0.028940
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19711.845117
     acc:0.357284
 tv:
     loss:20386.498926
     acc:0.212218
 lu:
     loss:13709.857471
     acc:0.850673
 lv:
     loss:16850.952344
     acc:0.166693
 le:
     loss:1464.732404
     acc:0.995667
 encoder:
     loss:0.026294
----------------------------


Epoch: [55/1000]:
train:
----------------------------
 tu:
     loss:20675.790630
     acc:0.360401
 tv:
     loss:21377.177371
     acc:0.216383
 lu:
     loss:14426.163109
     acc:0.844712
 lv:
     loss:17662.686966
     acc:0.172929
 le:
     loss:1537.088698
     acc:0.995668
 encoder:
     loss:0.035999
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19709.728809
     acc:0.358036
 tv:
     loss:20378.518262
     acc:0.213734
 lu:
     loss:13705.279443
     acc:0.852124
 lv:
     loss:16851.203613
     acc:0.166575
 le:
     loss:1464.728278
     acc:0.995667
 encoder:
     loss:0.029307
----------------------------


Epoch: [56/1000]:
train:
----------------------------
 tu:
     loss:20673.375874
     acc:0.360708
 tv:
     loss:21373.228039
     acc:0.217085
 lu:
     loss:14422.286689
     acc:0.845395
 lv:
     loss:17660.836619
     acc:0.173559
 le:
     loss:1537.070210
     acc:0.995667
 encoder:
     loss:0.028717
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19708.999902
     acc:0.357863
 tv:
     loss:20379.209766
     acc:0.213623
 lu:
     loss:13703.818408
     acc:0.852419
 lv:
     loss:16850.153809
     acc:0.166367
 le:
     loss:1464.714203
     acc:0.995667
 encoder:
     loss:0.029148
----------------------------


Epoch: [57/1000]:
train:
----------------------------
 tu:
     loss:20671.279388
     acc:0.361161
 tv:
     loss:21364.376147
     acc:0.218964
 lu:
     loss:14419.129747
     acc:0.846115
 lv:
     loss:17659.954363
     acc:0.173854
 le:
     loss:1536.394213
     acc:0.995740
 encoder:
     loss:0.028182
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19707.570020
     acc:0.358225
 tv:
     loss:20354.469629
     acc:0.219347
 lu:
     loss:13701.584814
     acc:0.852524
 lv:
     loss:16850.540332
     acc:0.167156
 le:
     loss:1457.336462
     acc:0.997147
 encoder:
     loss:0.035471
----------------------------


Epoch: [58/1000]:
train:
----------------------------
 tu:
     loss:20669.877941
     acc:0.361326
 tv:
     loss:21322.652446
     acc:0.228162
 lu:
     loss:14417.957338
     acc:0.846400
 lv:
     loss:17658.909316
     acc:0.174008
 le:
     loss:1523.421960
     acc:0.998630
 encoder:
     loss:0.035509
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19706.670117
     acc:0.358354
 tv:
     loss:20315.698242
     acc:0.228014
 lu:
     loss:13700.641016
     acc:0.852391
 lv:
     loss:16849.818066
     acc:0.167320
 le:
     loss:1448.692804
     acc:0.999149
 encoder:
     loss:0.029635
----------------------------


Epoch: [59/1000]:
train:
----------------------------
 tu:
     loss:20668.164460
     acc:0.361779
 tv:
     loss:21309.734227
     acc:0.230848
 lu:
     loss:14394.534747
     acc:0.852102
 lv:
     loss:17656.217558
     acc:0.174582
 le:
     loss:1521.707891
     acc:0.998895
 encoder:
     loss:0.070045
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19706.642969
     acc:0.358272
 tv:
     loss:20310.405469
     acc:0.229041
 lu:
     loss:13660.668848
     acc:0.861824
 lv:
     loss:16848.972754
     acc:0.167641
 le:
     loss:1448.622540
     acc:0.999148
 encoder:
     loss:0.023012
----------------------------


Epoch: [60/1000]:
train:
----------------------------
 tu:
     loss:20667.733637
     acc:0.361746
 tv:
     loss:21294.861657
     acc:0.234354
 lu:
     loss:14364.012218
     acc:0.858356
 lv:
     loss:17655.684820
     acc:0.174729
 le:
     loss:1521.384688
     acc:0.998959
 encoder:
     loss:0.042405
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19706.251172
     acc:0.358355
 tv:
     loss:20292.554102
     acc:0.232796
 lu:
     loss:13657.464844
     acc:0.862010
 lv:
     loss:16844.989063
     acc:0.168406
 le:
     loss:1448.421906
     acc:0.999191
 encoder:
     loss:0.026289
----------------------------


Epoch: [61/1000]:
train:
----------------------------
 tu:
     loss:20666.447799
     acc:0.361819
 tv:
     loss:21281.707667
     acc:0.236652
 lu:
     loss:14358.172954
     acc:0.859123
 lv:
     loss:17652.433174
     acc:0.175334
 le:
     loss:1521.254522
     acc:0.998989
 encoder:
     loss:0.031355
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19706.121777
     acc:0.358517
 tv:
     loss:20286.718164
     acc:0.234052
 lu:
     loss:13652.236865
     acc:0.863719
 lv:
     loss:16845.917090
     acc:0.168360
 le:
     loss:1448.388538
     acc:0.999229
 encoder:
     loss:0.027217
----------------------------


Epoch: [62/1000]:
train:
----------------------------
 tu:
     loss:20650.741381
     acc:0.365799
 tv:
     loss:21274.316860
     acc:0.238326
 lu:
     loss:14354.342966
     acc:0.859875
 lv:
     loss:17650.794252
     acc:0.175769
 le:
     loss:1521.161225
     acc:0.998989
 encoder:
     loss:0.029519
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19666.447559
     acc:0.367092
 tv:
     loss:20276.719727
     acc:0.236177
 lu:
     loss:13651.237451
     acc:0.863574
 lv:
     loss:16847.468359
     acc:0.167452
 le:
     loss:1448.492450
     acc:0.999131
 encoder:
     loss:0.028064
----------------------------


Epoch: [63/1000]:
train:
----------------------------
 tu:
     loss:20627.103107
     acc:0.370543
 tv:
     loss:21265.798397
     acc:0.239953
 lu:
     loss:14353.165300
     acc:0.860023
 lv:
     loss:17650.009856
     acc:0.176015
 le:
     loss:1521.081664
     acc:0.999017
 encoder:
     loss:0.036519
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19663.530273
     acc:0.367700
 tv:
     loss:20272.486621
     acc:0.237364
 lu:
     loss:13649.913623
     acc:0.864005
 lv:
     loss:16843.903516
     acc:0.169343
 le:
     loss:1448.396045
     acc:0.999191
 encoder:
     loss:0.021044
----------------------------


Epoch: [64/1000]:
train:
----------------------------
 tu:
     loss:20614.599280
     acc:0.373427
 tv:
     loss:21262.931686
     acc:0.240347
 lu:
     loss:14349.396155
     acc:0.860764
 lv:
     loss:17647.624818
     acc:0.176347
 le:
     loss:1521.124444
     acc:0.998991
 encoder:
     loss:0.074516
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19631.877930
     acc:0.375407
 tv:
     loss:20271.081445
     acc:0.237517
 lu:
     loss:13649.499268
     acc:0.863474
 lv:
     loss:16843.171094
     acc:0.168953
 le:
     loss:1448.535217
     acc:0.999111
 encoder:
     loss:0.034407
----------------------------


Epoch: [65/1000]:
train:
----------------------------
 tu:
     loss:20591.787530
     acc:0.378287
 tv:
     loss:21259.910781
     acc:0.240809
 lu:
     loss:14347.991109
     acc:0.860694
 lv:
     loss:17646.470260
     acc:0.176717
 le:
     loss:1521.024742
     acc:0.999003
 encoder:
     loss:0.028942
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19627.664258
     acc:0.375744
 tv:
     loss:20274.208789
     acc:0.236950
 lu:
     loss:13645.914551
     acc:0.864620
 lv:
     loss:16845.345898
     acc:0.167518
 le:
     loss:1448.451764
     acc:0.999132
 encoder:
     loss:0.020069
----------------------------


Epoch: [66/1000]:
train:
----------------------------
 tu:
     loss:20586.719806
     acc:0.378912
 tv:
     loss:21258.800940
     acc:0.241035
 lu:
     loss:14344.404603
     acc:0.861492
 lv:
     loss:17643.729719
     acc:0.177003
 le:
     loss:1520.951707
     acc:0.999034
 encoder:
     loss:0.031061
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19625.677539
     acc:0.376105
 tv:
     loss:20272.453027
     acc:0.237063
 lu:
     loss:13644.646484
     acc:0.864735
 lv:
     loss:16841.741016
     acc:0.168932
 le:
     loss:1448.421997
     acc:0.999170
 encoder:
     loss:0.019237
----------------------------


Epoch: [67/1000]:
train:
----------------------------
 tu:
     loss:20584.631632
     acc:0.379175
 tv:
     loss:21248.202239
     acc:0.243587
 lu:
     loss:14342.828216
     acc:0.861749
 lv:
     loss:17643.818825
     acc:0.177048
 le:
     loss:1520.917483
     acc:0.999021
 encoder:
     loss:0.026675
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19623.441699
     acc:0.376418
 tv:
     loss:20250.296875
     acc:0.241959
 lu:
     loss:13640.919189
     acc:0.865563
 lv:
     loss:16841.683496
     acc:0.169654
 le:
     loss:1448.410486
     acc:0.999131
 encoder:
     loss:0.034203
----------------------------


Epoch: [68/1000]:
train:
----------------------------
 tu:
     loss:20582.041084
     acc:0.379575
 tv:
     loss:21236.064044
     acc:0.246255
 lu:
     loss:14340.159259
     acc:0.862256
 lv:
     loss:17641.487089
     acc:0.177613
 le:
     loss:1520.791014
     acc:0.999045
 encoder:
     loss:0.035885
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19607.941602
     acc:0.380045
 tv:
     loss:20246.376855
     acc:0.242657
 lu:
     loss:13639.404395
     acc:0.866258
 lv:
     loss:16840.511523
     acc:0.169199
 le:
     loss:1448.437939
     acc:0.999149
 encoder:
     loss:0.026488
----------------------------


Epoch: [69/1000]:
train:
----------------------------
 tu:
     loss:20558.312841
     acc:0.385166
 tv:
     loss:21230.664449
     acc:0.247048
 lu:
     loss:14338.955396
     acc:0.862838
 lv:
     loss:17638.277991
     acc:0.178557
 le:
     loss:1520.832505
     acc:0.999034
 encoder:
     loss:0.038032
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19587.976367
     acc:0.384258
 tv:
     loss:20241.610449
     acc:0.243532
 lu:
     loss:13640.268945
     acc:0.865676
 lv:
     loss:16839.831738
     acc:0.169180
 le:
     loss:1448.505859
     acc:0.999148
 encoder:
     loss:0.028915
----------------------------


Epoch: [70/1000]:
train:
----------------------------
 tu:
     loss:20546.900618
     acc:0.387132
 tv:
     loss:21225.286326
     acc:0.248204
 lu:
     loss:14336.965957
     acc:0.862853
 lv:
     loss:17638.673170
     acc:0.178659
 le:
     loss:1520.847056
     acc:0.999024
 encoder:
     loss:0.039687
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19585.757031
     acc:0.384502
 tv:
     loss:20229.438281
     acc:0.246164
 lu:
     loss:13639.217578
     acc:0.865651
 lv:
     loss:16838.208105
     acc:0.169599
 le:
     loss:1448.514056
     acc:0.999131
 encoder:
     loss:0.030795
----------------------------


Epoch: [71/1000]:
train:
----------------------------
 tu:
     loss:20537.527071
     acc:0.389222
 tv:
     loss:21213.349507
     acc:0.250752
 lu:
     loss:14335.180823
     acc:0.863186
 lv:
     loss:17638.464821
     acc:0.178100
 le:
     loss:1520.731488
     acc:0.999047
 encoder:
     loss:0.032071
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19555.943262
     acc:0.391624
 tv:
     loss:20224.272656
     acc:0.247218
 lu:
     loss:13637.029492
     acc:0.866289
 lv:
     loss:16840.113770
     acc:0.169631
 le:
     loss:1448.551617
     acc:0.999111
 encoder:
     loss:0.025654
----------------------------


Epoch: [72/1000]:
train:
----------------------------
 tu:
     loss:20513.371548
     acc:0.394545
 tv:
     loss:21208.024198
     acc:0.251777
 lu:
     loss:14333.927780
     acc:0.863667
 lv:
     loss:17636.564612
     acc:0.178734
 le:
     loss:1520.717370
     acc:0.999070
 encoder:
     loss:0.043077
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19551.106445
     acc:0.392387
 tv:
     loss:20223.716602
     acc:0.247355
 lu:
     loss:13634.748340
     acc:0.866804
 lv:
     loss:16838.784375
     acc:0.169796
 le:
     loss:1448.552716
     acc:0.999131
 encoder:
     loss:0.036134
----------------------------


Epoch: [73/1000]:
train:
----------------------------
 tu:
     loss:20507.718489
     acc:0.395313
 tv:
     loss:21201.790993
     acc:0.253015
 lu:
     loss:14331.309116
     acc:0.863944
 lv:
     loss:17634.531840
     acc:0.179022
 le:
     loss:1520.711275
     acc:0.999059
 encoder:
     loss:0.036019
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19549.559180
     acc:0.392609
 tv:
     loss:20217.309668
     acc:0.249137
 lu:
     loss:13637.509082
     acc:0.865967
 lv:
     loss:16839.360645
     acc:0.170101
 le:
     loss:1448.498376
     acc:0.999131
 encoder:
     loss:0.048121
----------------------------


Epoch: [74/1000]:
train:
----------------------------
 tu:
     loss:20504.647404
     acc:0.395697
 tv:
     loss:21194.841184
     acc:0.254736
 lu:
     loss:14330.268487
     acc:0.864349
 lv:
     loss:17633.135118
     acc:0.179339
 le:
     loss:1520.725502
     acc:0.999068
 encoder:
     loss:0.037348
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19548.547949
     acc:0.392721
 tv:
     loss:20200.204688
     acc:0.252740
 lu:
     loss:13632.979102
     acc:0.867342
 lv:
     loss:16839.575977
     acc:0.169103
 le:
     loss:1448.470947
     acc:0.999111
 encoder:
     loss:0.025410
----------------------------


Epoch: [75/1000]:
train:
----------------------------
 tu:
     loss:20503.883641
     acc:0.395735
 tv:
     loss:21177.712675
     acc:0.258301
 lu:
     loss:14328.311399
     acc:0.864550
 lv:
     loss:17629.580498
     acc:0.180045
 le:
     loss:1520.652382
     acc:0.999063
 encoder:
     loss:0.041942
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19548.019727
     acc:0.392837
 tv:
     loss:20193.795703
     acc:0.254045
 lu:
     loss:13631.887012
     acc:0.867390
 lv:
     loss:16837.635937
     acc:0.169081
 le:
     loss:1448.711597
     acc:0.999047
 encoder:
     loss:0.036617
----------------------------


Epoch: [76/1000]:
train:
----------------------------
 tu:
     loss:20502.524176
     acc:0.395898
 tv:
     loss:21164.451217
     acc:0.261051
 lu:
     loss:14327.905818
     acc:0.864613
 lv:
     loss:17628.110760
     acc:0.180540
 le:
     loss:1520.578070
     acc:0.999087
 encoder:
     loss:0.031383
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19547.205469
     acc:0.392964
 tv:
     loss:20180.953613
     acc:0.256619
 lu:
     loss:13633.571094
     acc:0.866889
 lv:
     loss:16838.082422
     acc:0.169718
 le:
     loss:1448.635693
     acc:0.999064
 encoder:
     loss:0.025843
----------------------------


Epoch: [77/1000]:
train:
----------------------------
 tu:
     loss:20501.860931
     acc:0.396002
 tv:
     loss:21157.682049
     acc:0.262112
 lu:
     loss:14325.519679
     acc:0.865101
 lv:
     loss:17626.300338
     acc:0.181083
 le:
     loss:1520.748823
     acc:0.999047
 encoder:
     loss:0.037709
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19548.115918
     acc:0.392811
 tv:
     loss:20177.523828
     acc:0.257687
 lu:
     loss:13629.041113
     acc:0.868256
 lv:
     loss:16835.744434
     acc:0.169681
 le:
     loss:1448.586621
     acc:0.999069
 encoder:
     loss:0.037516
----------------------------


Epoch: [78/1000]:
train:
----------------------------
 tu:
     loss:20500.537246
     acc:0.396231
 tv:
     loss:21153.873149
     acc:0.262858
 lu:
     loss:14323.183128
     acc:0.865711
 lv:
     loss:17624.156375
     acc:0.181644
 le:
     loss:1520.606390
     acc:0.999085
 encoder:
     loss:0.039150
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19547.372559
     acc:0.392819
 tv:
     loss:20174.022070
     acc:0.258186
 lu:
     loss:13629.100879
     acc:0.868129
 lv:
     loss:16834.182227
     acc:0.170606
 le:
     loss:1448.439508
     acc:0.999152
 encoder:
     loss:0.029828
----------------------------


Epoch: [79/1000]:
train:
----------------------------
 tu:
     loss:20500.454567
     acc:0.396163
 tv:
     loss:21151.020122
     acc:0.263512
 lu:
     loss:14322.151503
     acc:0.865878
 lv:
     loss:17620.050724
     acc:0.182489
 le:
     loss:1520.705501
     acc:0.999052
 encoder:
     loss:0.037875
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19547.028809
     acc:0.392985
 tv:
     loss:20163.982227
     acc:0.260435
 lu:
     loss:13627.438330
     acc:0.868211
 lv:
     loss:16832.986719
     acc:0.170552
 le:
     loss:1448.531750
     acc:0.999111
 encoder:
     loss:0.028007
----------------------------


Epoch: [80/1000]:
train:
----------------------------
 tu:
     loss:20499.912212
     acc:0.396250
 tv:
     loss:21137.520689
     acc:0.266420
 lu:
     loss:14321.024925
     acc:0.865884
 lv:
     loss:17619.836676
     acc:0.182289
 le:
     loss:1520.553996
     acc:0.999099
 encoder:
     loss:0.038514
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19547.060059
     acc:0.392956
 tv:
     loss:20156.978711
     acc:0.261801
 lu:
     loss:13627.017480
     acc:0.868409
 lv:
     loss:16837.805859
     acc:0.169350
 le:
     loss:1448.419592
     acc:0.999131
 encoder:
     loss:0.049580
----------------------------


Epoch: [81/1000]:
train:
----------------------------
 tu:
     loss:20498.709473
     acc:0.396693
 tv:
     loss:21131.431118
     acc:0.267517
 lu:
     loss:14319.128918
     acc:0.866440
 lv:
     loss:17616.284407
     acc:0.183105
 le:
     loss:1520.531212
     acc:0.999075
 encoder:
     loss:0.043770
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19546.434961
     acc:0.393126
 tv:
     loss:20153.287793
     acc:0.262497
 lu:
     loss:13626.486963
     acc:0.868593
 lv:
     loss:16831.677539
     acc:0.170034
 le:
     loss:1448.422101
     acc:0.999152
 encoder:
     loss:0.034077
----------------------------


Epoch: [82/1000]:
train:
----------------------------
 tu:
     loss:20499.012775
     acc:0.396395
 tv:
     loss:21122.343614
     acc:0.269869
 lu:
     loss:14317.982240
     acc:0.866500
 lv:
     loss:17614.115882
     acc:0.183311
 le:
     loss:1520.532817
     acc:0.999085
 encoder:
     loss:0.028588
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19546.335352
     acc:0.393082
 tv:
     loss:20145.184668
     acc:0.264450
 lu:
     loss:13625.174268
     acc:0.868844
 lv:
     loss:16832.115625
     acc:0.169778
 le:
     loss:1448.479919
     acc:0.999131
 encoder:
     loss:0.030349
----------------------------


Epoch: [83/1000]:
train:
----------------------------
 tu:
     loss:20479.769225
     acc:0.401151
 tv:
     loss:21117.724064
     acc:0.270715
 lu:
     loss:14316.496060
     acc:0.866888
 lv:
     loss:17612.097781
     acc:0.183749
 le:
     loss:1520.488936
     acc:0.999088
 encoder:
     loss:0.044942
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19513.122363
     acc:0.400846
 tv:
     loss:20141.348438
     acc:0.265697
 lu:
     loss:13622.687451
     acc:0.869363
 lv:
     loss:16831.301172
     acc:0.169131
 le:
     loss:1448.567609
     acc:0.999089
 encoder:
     loss:0.024381
----------------------------


Epoch: [84/1000]:
train:
----------------------------
 tu:
     loss:20466.082111
     acc:0.403848
 tv:
     loss:21115.797159
     acc:0.271010
 lu:
     loss:14315.886878
     acc:0.867138
 lv:
     loss:17609.081089
     acc:0.184277
 le:
     loss:1520.311434
     acc:0.999139
 encoder:
     loss:0.028842
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19511.988965
     acc:0.400855
 tv:
     loss:20141.994727
     acc:0.265012
 lu:
     loss:13622.131055
     acc:0.869567
 lv:
     loss:16826.765039
     acc:0.170637
 le:
     loss:1448.559882
     acc:0.999147
 encoder:
     loss:0.036176
----------------------------


Epoch: [85/1000]:
train:
----------------------------
 tu:
     loss:20463.907703
     acc:0.404028
 tv:
     loss:21113.205555
     acc:0.271488
 lu:
     loss:14313.461040
     acc:0.867495
 lv:
     loss:17606.366131
     acc:0.184907
 le:
     loss:1520.401018
     acc:0.999124
 encoder:
     loss:0.032686
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19510.958789
     acc:0.400750
 tv:
     loss:20136.720020
     acc:0.266379
 lu:
     loss:13621.644336
     acc:0.869496
 lv:
     loss:16825.799121
     acc:0.171091
 le:
     loss:1448.450824
     acc:0.999132
 encoder:
     loss:0.032196
----------------------------


Epoch: [86/1000]:
train:
----------------------------
 tu:
     loss:20462.364190
     acc:0.404219
 tv:
     loss:21106.404251
     acc:0.273141
 lu:
     loss:14312.081316
     acc:0.868116
 lv:
     loss:17602.698719
     acc:0.185761
 le:
     loss:1520.358721
     acc:0.999127
 encoder:
     loss:0.041274
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19511.000977
     acc:0.400826
 tv:
     loss:20121.984766
     acc:0.269884
 lu:
     loss:13620.990625
     acc:0.869825
 lv:
     loss:16820.139648
     acc:0.172139
 le:
     loss:1448.468372
     acc:0.999125
 encoder:
     loss:0.049968
----------------------------


Epoch: [87/1000]:
train:
----------------------------
 tu:
     loss:20461.773619
     acc:0.404395
 tv:
     loss:21092.942701
     acc:0.276022
 lu:
     loss:14311.844091
     acc:0.867921
 lv:
     loss:17599.657102
     acc:0.186261
 le:
     loss:1520.434081
     acc:0.999102
 encoder:
     loss:0.037606
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19510.508105
     acc:0.400996
 tv:
     loss:20111.429004
     acc:0.272562
 lu:
     loss:13621.270752
     acc:0.869351
 lv:
     loss:16820.539551
     acc:0.172892
 le:
     loss:1448.518127
     acc:0.999110
 encoder:
     loss:0.030019
----------------------------


Epoch: [88/1000]:
train:
----------------------------
 tu:
     loss:20461.452648
     acc:0.404375
 tv:
     loss:21076.174646
     acc:0.279777
 lu:
     loss:14310.751238
     acc:0.868063
 lv:
     loss:17596.755417
     acc:0.187238
 le:
     loss:1520.338441
     acc:0.999111
 encoder:
     loss:0.031126
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19510.334961
     acc:0.401056
 tv:
     loss:20086.491699
     acc:0.277567
 lu:
     loss:13618.227197
     acc:0.870199
 lv:
     loss:16818.888867
     acc:0.173216
 le:
     loss:1448.448505
     acc:0.999132
 encoder:
     loss:0.028974
----------------------------


Epoch: [89/1000]:
train:
----------------------------
 tu:
     loss:20460.970783
     acc:0.404408
 tv:
     loss:21054.534112
     acc:0.284193
 lu:
     loss:14309.487657
     acc:0.868238
 lv:
     loss:17592.926735
     acc:0.187867
 le:
     loss:1520.379157
     acc:0.999122
 encoder:
     loss:0.088418
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19510.334961
     acc:0.400954
 tv:
     loss:20074.490918
     acc:0.280225
 lu:
     loss:13618.627344
     acc:0.870199
 lv:
     loss:16817.425977
     acc:0.173330
 le:
     loss:1448.609802
     acc:0.999123
 encoder:
     loss:0.050314
----------------------------


Epoch: [90/1000]:
train:
----------------------------
 tu:
     loss:20461.316304
     acc:0.404178
 tv:
     loss:21044.413199
     acc:0.286301
 lu:
     loss:14299.416390
     acc:0.870816
 lv:
     loss:17588.479186
     acc:0.188887
 le:
     loss:1520.243344
     acc:0.999157
 encoder:
     loss:0.053345
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19509.965527
     acc:0.401090
 tv:
     loss:20068.407520
     acc:0.281777
 lu:
     loss:13595.268262
     acc:0.875579
 lv:
     loss:16812.595605
     acc:0.174588
 le:
     loss:1448.423706
     acc:0.999132
 encoder:
     loss:0.046688
----------------------------


Epoch: [91/1000]:
train:
----------------------------
 tu:
     loss:20460.889399
     acc:0.404334
 tv:
     loss:21040.921148
     acc:0.286808
 lu:
     loss:14277.947084
     acc:0.875507
 lv:
     loss:17588.997116
     acc:0.188725
 le:
     loss:1520.280750
     acc:0.999128
 encoder:
     loss:0.060593
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19509.658203
     acc:0.401328
 tv:
     loss:20065.809473
     acc:0.282062
 lu:
     loss:13587.241650
     acc:0.877190
 lv:
     loss:16811.910938
     acc:0.174954
 le:
     loss:1448.510101
     acc:0.999145
 encoder:
     loss:0.035965
----------------------------


Epoch: [92/1000]:
train:
----------------------------
 tu:
     loss:20460.662393
     acc:0.404343
 tv:
     loss:21032.155478
     acc:0.288779
 lu:
     loss:14273.200740
     acc:0.876313
 lv:
     loss:17584.221191
     acc:0.189875
 le:
     loss:1520.255747
     acc:0.999149
 encoder:
     loss:0.250653
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19493.499316
     acc:0.405387
 tv:
     loss:20053.119629
     acc:0.284972
 lu:
     loss:13584.424219
     acc:0.878177
 lv:
     loss:16810.187598
     acc:0.175035
 le:
     loss:1448.599359
     acc:0.999085
 encoder:
     loss:0.113501
----------------------------


Epoch: [93/1000]:
train:
----------------------------
 tu:
     loss:20415.124932
     acc:0.415181
 tv:
     loss:21025.581725
     acc:0.290177
 lu:
     loss:14268.842490
     acc:0.877139
 lv:
     loss:17582.602982
     acc:0.190098
 le:
     loss:1520.259351
     acc:0.999142
 encoder:
     loss:0.083891
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19427.545312
     acc:0.419554
 tv:
     loss:20051.362402
     acc:0.285009
 lu:
     loss:13588.277881
     acc:0.877106
 lv:
     loss:16811.161230
     acc:0.175099
 le:
     loss:1448.524005
     acc:0.999108
 encoder:
     loss:0.026190
----------------------------


Epoch: [94/1000]:
train:
----------------------------
 tu:
     loss:20380.984454
     acc:0.421788
 tv:
     loss:21019.364882
     acc:0.291168
 lu:
     loss:14267.540084
     acc:0.877473
 lv:
     loss:17578.125840
     acc:0.191065
 le:
     loss:1520.211907
     acc:0.999156
 encoder:
     loss:0.041576
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19425.764941
     acc:0.419660
 tv:
     loss:20045.972266
     acc:0.286463
 lu:
     loss:13583.364209
     acc:0.877917
 lv:
     loss:16808.674609
     acc:0.175415
 le:
     loss:1448.493182
     acc:0.999149
 encoder:
     loss:0.039784
----------------------------


Epoch: [95/1000]:
train:
----------------------------
 tu:
     loss:20371.268805
     acc:0.423903
 tv:
     loss:21017.869788
     acc:0.291526
 lu:
     loss:14266.894236
     acc:0.877174
 lv:
     loss:17578.760526
     acc:0.190442
 le:
     loss:1520.265425
     acc:0.999122
 encoder:
     loss:0.073047
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19396.740527
     acc:0.426624
 tv:
     loss:20047.646973
     acc:0.285975
 lu:
     loss:13583.187988
     acc:0.877479
 lv:
     loss:16801.344043
     acc:0.177971
 le:
     loss:1448.342303
     acc:0.999187
 encoder:
     loss:0.050094
----------------------------


Epoch: [96/1000]:
train:
----------------------------
 tu:
     loss:20349.010402
     acc:0.428905
 tv:
     loss:21008.614950
     acc:0.293366
 lu:
     loss:14264.126840
     acc:0.877824
 lv:
     loss:17572.724223
     acc:0.192440
 le:
     loss:1520.206286
     acc:0.999163
 encoder:
     loss:0.037525
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19390.230664
     acc:0.427356
 tv:
     loss:20031.283105
     acc:0.289831
 lu:
     loss:13581.123389
     acc:0.878021
 lv:
     loss:16800.595410
     acc:0.177890
 le:
     loss:1448.374677
     acc:0.999170
 encoder:
     loss:0.043966
----------------------------


Epoch: [97/1000]:
train:
----------------------------
 tu:
     loss:20342.535406
     acc:0.429866
 tv:
     loss:20994.348235
     acc:0.296527
 lu:
     loss:14263.575400
     acc:0.878049
 lv:
     loss:17571.329999
     acc:0.192802
 le:
     loss:1520.157134
     acc:0.999168
 encoder:
     loss:0.042571
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19388.154980
     acc:0.427929
 tv:
     loss:20019.706055
     acc:0.292508
 lu:
     loss:13580.790039
     acc:0.878419
 lv:
     loss:16799.207422
     acc:0.178051
 le:
     loss:1448.448322
     acc:0.999145
 encoder:
     loss:0.045568
----------------------------


Epoch: [98/1000]:
train:
----------------------------
 tu:
     loss:20339.631143
     acc:0.430245
 tv:
     loss:20986.302507
     acc:0.298122
 lu:
     loss:14260.882472
     acc:0.878361
 lv:
     loss:17566.345022
     acc:0.193686
 le:
     loss:1520.282127
     acc:0.999137
 encoder:
     loss:0.064993
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19387.227539
     acc:0.427952
 tv:
     loss:20016.171191
     acc:0.292837
 lu:
     loss:13580.207373
     acc:0.878517
 lv:
     loss:16796.790430
     acc:0.178848
 le:
     loss:1448.440430
     acc:0.999131
 encoder:
     loss:0.034421
----------------------------


Epoch: [99/1000]:
train:
----------------------------
 tu:
     loss:20337.883812
     acc:0.430390
 tv:
     loss:20983.791901
     acc:0.298571
 lu:
     loss:14260.228947
     acc:0.878671
 lv:
     loss:17565.887593
     acc:0.193598
 le:
     loss:1520.154342
     acc:0.999168
 encoder:
     loss:0.046955
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19386.438477
     acc:0.428101
 tv:
     loss:20015.036133
     acc:0.293481
 lu:
     loss:13579.542773
     acc:0.878682
 lv:
     loss:16793.799902
     acc:0.178992
 le:
     loss:1448.466473
     acc:0.999167
 encoder:
     loss:0.037478
----------------------------


Epoch: [100/1000]:
train:
----------------------------
 tu:
     loss:20336.892987
     acc:0.430488
 tv:
     loss:20968.277741
     acc:0.302058
 lu:
     loss:14258.926031
     acc:0.878833
 lv:
     loss:17561.200752
     acc:0.194626
 le:
     loss:1519.957163
     acc:0.999209
 encoder:
     loss:0.041247
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19385.704297
     acc:0.428121
 tv:
     loss:19997.999902
     acc:0.296922
 lu:
     loss:13577.411963
     acc:0.879145
 lv:
     loss:16796.996191
     acc:0.178847
 le:
     loss:1448.467749
     acc:0.999147
 encoder:
     loss:0.021891
----------------------------


Epoch: [101/1000]:
train:
----------------------------
 tu:
     loss:20311.856434
     acc:0.436165
 tv:
     loss:20951.276606
     acc:0.305537
 lu:
     loss:14256.054529
     acc:0.879475
 lv:
     loss:17558.175179
     acc:0.195275
 le:
     loss:1520.016572
     acc:0.999197
 encoder:
     loss:0.060363
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19347.424121
     acc:0.436563
 tv:
     loss:19981.423438
     acc:0.300407
 lu:
     loss:13576.834229
     acc:0.879045
 lv:
     loss:16794.720215
     acc:0.179176
 le:
     loss:1448.563312
     acc:0.999108
 encoder:
     loss:0.169750
----------------------------


Epoch: [102/1000]:
train:
----------------------------
 tu:
     loss:20282.182322
     acc:0.442350
 tv:
     loss:20943.537813
     acc:0.307024
 lu:
     loss:14253.819915
     acc:0.879865
 lv:
     loss:17555.651731
     acc:0.195849
 le:
     loss:1520.180013
     acc:0.999157
 encoder:
     loss:0.076346
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19310.814355
     acc:0.444786
 tv:
     loss:19980.416504
     acc:0.300039
 lu:
     loss:13568.552588
     acc:0.881230
 lv:
     loss:16790.514453
     acc:0.179728
 le:
     loss:1448.452979
     acc:0.999170
 encoder:
     loss:0.030443
----------------------------


Epoch: [103/1000]:
train:
----------------------------
 tu:
     loss:20263.810354
     acc:0.445819
 tv:
     loss:20938.354106
     acc:0.307942
 lu:
     loss:14249.052598
     acc:0.881135
 lv:
     loss:17552.047068
     acc:0.196523
 le:
     loss:1519.940059
     acc:0.999191
 encoder:
     loss:0.039779
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19309.709277
     acc:0.444788
 tv:
     loss:19974.943066
     acc:0.301638
 lu:
     loss:13562.008594
     acc:0.882794
 lv:
     loss:16789.374609
     acc:0.180007
 le:
     loss:1448.336761
     acc:0.999208
 encoder:
     loss:0.034624
----------------------------


Epoch: [104/1000]:
train:
----------------------------
 tu:
     loss:20248.560297
     acc:0.449397
 tv:
     loss:20935.773176
     acc:0.308384
 lu:
     loss:14244.778752
     acc:0.882100
 lv:
     loss:17550.073901
     acc:0.197198
 le:
     loss:1520.021738
     acc:0.999195
 encoder:
     loss:0.046479
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19272.722070
     acc:0.453205
 tv:
     loss:19976.709082
     acc:0.301099
 lu:
     loss:13561.598242
     acc:0.882804
 lv:
     loss:16791.409668
     acc:0.179989
 le:
     loss:1448.579993
     acc:0.999125
 encoder:
     loss:0.060713
----------------------------


Epoch: [105/1000]:
train:
----------------------------
 tu:
     loss:20223.743630
     acc:0.454552
 tv:
     loss:20935.653968
     acc:0.308253
 lu:
     loss:14241.992971
     acc:0.882468
 lv:
     loss:17545.366075
     acc:0.198046
 le:
     loss:1520.016920
     acc:0.999227
 encoder:
     loss:0.068318
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19270.148535
     acc:0.453501
 tv:
     loss:19974.787012
     acc:0.301290
 lu:
     loss:13560.163379
     acc:0.883180
 lv:
     loss:16785.680566
     acc:0.180720
 le:
     loss:1448.340265
     acc:0.999229
 encoder:
     loss:0.088334
----------------------------


Epoch: [106/1000]:
train:
----------------------------
 tu:
     loss:20220.575933
     acc:0.454941
 tv:
     loss:20924.829192
     acc:0.310981
 lu:
     loss:14239.692848
     acc:0.883013
 lv:
     loss:17542.578273
     acc:0.198964
 le:
     loss:1520.051458
     acc:0.999180
 encoder:
     loss:0.077886
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19269.239648
     acc:0.453831
 tv:
     loss:19960.843945
     acc:0.304340
 lu:
     loss:13557.736621
     acc:0.883703
 lv:
     loss:16786.180273
     acc:0.180284
 le:
     loss:1448.387897
     acc:0.999188
 encoder:
     loss:0.048008
----------------------------


Epoch: [107/1000]:
train:
----------------------------
 tu:
     loss:20218.045422
     acc:0.455337
 tv:
     loss:20917.325729
     acc:0.312242
 lu:
     loss:14239.352857
     acc:0.883122
 lv:
     loss:17539.689351
     acc:0.199240
 le:
     loss:1519.929401
     acc:0.999215
 encoder:
     loss:0.044449
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19267.704102
     acc:0.454130
 tv:
     loss:19956.097559
     acc:0.305466
 lu:
     loss:13555.317041
     acc:0.884250
 lv:
     loss:16785.279883
     acc:0.181137
 le:
     loss:1448.425513
     acc:0.999167
 encoder:
     loss:0.061552
----------------------------


Epoch: [108/1000]:
train:
----------------------------
 tu:
     loss:20217.449650
     acc:0.455554
 tv:
     loss:20914.515148
     acc:0.312871
 lu:
     loss:14237.328863
     acc:0.883347
 lv:
     loss:17533.526401
     acc:0.200609
 le:
     loss:1519.997732
     acc:0.999210
 encoder:
     loss:0.042394
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19267.267578
     acc:0.454191
 tv:
     loss:19957.640820
     acc:0.304755
 lu:
     loss:13556.850781
     acc:0.883466
 lv:
     loss:16782.206445
     acc:0.182718
 le:
     loss:1448.229468
     acc:0.999208
 encoder:
     loss:0.035845
----------------------------


Epoch: [109/1000]:
train:
----------------------------
 tu:
     loss:20216.198435
     acc:0.455581
 tv:
     loss:20904.710483
     acc:0.315401
 lu:
     loss:14237.421602
     acc:0.883073
 lv:
     loss:17532.211051
     acc:0.200862
 le:
     loss:1519.944150
     acc:0.999213
 encoder:
     loss:0.056135
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19266.587305
     acc:0.454198
 tv:
     loss:19927.771484
     acc:0.312623
 lu:
     loss:13554.364844
     acc:0.884328
 lv:
     loss:16780.483594
     acc:0.182573
 le:
     loss:1448.310828
     acc:0.999188
 encoder:
     loss:0.073061
----------------------------


Epoch: [110/1000]:
train:
----------------------------
 tu:
     loss:20215.556925
     acc:0.455603
 tv:
     loss:20879.651640
     acc:0.321077
 lu:
     loss:14234.951058
     acc:0.883855
 lv:
     loss:17527.171421
     acc:0.202088
 le:
     loss:1519.960421
     acc:0.999197
 encoder:
     loss:0.057598
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19266.560254
     acc:0.454113
 tv:
     loss:19909.297363
     acc:0.315954
 lu:
     loss:13554.401514
     acc:0.884381
 lv:
     loss:16777.426172
     acc:0.183443
 le:
     loss:1448.399115
     acc:0.999208
 encoder:
     loss:0.051852
----------------------------


Epoch: [111/1000]:
train:
----------------------------
 tu:
     loss:20214.905353
     acc:0.455757
 tv:
     loss:20868.843864
     acc:0.323239
 lu:
     loss:14234.864292
     acc:0.883814
 lv:
     loss:17525.871934
     acc:0.202310
 le:
     loss:1520.031118
     acc:0.999195
 encoder:
     loss:0.045265
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19265.328906
     acc:0.454414
 tv:
     loss:19901.866602
     acc:0.317563
 lu:
     loss:13556.055127
     acc:0.883870
 lv:
     loss:16783.570703
     acc:0.182183
 le:
     loss:1448.274921
     acc:0.999246
 encoder:
     loss:0.040620
----------------------------


Epoch: [112/1000]:
train:
----------------------------
 tu:
     loss:20187.290539
     acc:0.461913
 tv:
     loss:20859.376669
     acc:0.325168
 lu:
     loss:14233.743085
     acc:0.883959
 lv:
     loss:17522.149198
     acc:0.203341
 le:
     loss:1519.982291
     acc:0.999207
 encoder:
     loss:0.055793
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19228.435156
     acc:0.462678
 tv:
     loss:19899.495605
     acc:0.318155
 lu:
     loss:13553.413037
     acc:0.884420
 lv:
     loss:16781.759180
     acc:0.181834
 le:
     loss:1448.378174
     acc:0.999204
 encoder:
     loss:0.057118
----------------------------


Epoch: [113/1000]:
train:
----------------------------
 tu:
     loss:20177.555005
     acc:0.463637
 tv:
     loss:20855.363508
     acc:0.325643
 lu:
     loss:14232.823299
     acc:0.884137
 lv:
     loss:17519.049839
     acc:0.203786
 le:
     loss:1519.935154
     acc:0.999205
 encoder:
     loss:0.071014
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19226.673633
     acc:0.463096
 tv:
     loss:19896.603906
     acc:0.318545
 lu:
     loss:13553.637646
     acc:0.884390
 lv:
     loss:16780.572559
     acc:0.181871
 le:
     loss:1448.485486
     acc:0.999166
 encoder:
     loss:0.060212
----------------------------


Epoch: [114/1000]:
train:
----------------------------
 tu:
     loss:20175.984102
     acc:0.463942
 tv:
     loss:20851.533373
     acc:0.326218
 lu:
     loss:14231.318246
     acc:0.884513
 lv:
     loss:17515.877669
     acc:0.204484
 le:
     loss:1519.944394
     acc:0.999203
 encoder:
     loss:0.043022
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19213.837109
     acc:0.465824
 tv:
     loss:19895.230078
     acc:0.319184
 lu:
     loss:13551.464746
     acc:0.884725
 lv:
     loss:16779.550684
     acc:0.183084
 le:
     loss:1448.369397
     acc:0.999224
 encoder:
     loss:0.043038
----------------------------


Epoch: [115/1000]:
train:
----------------------------
 tu:
     loss:20142.727312
     acc:0.471528
 tv:
     loss:20847.344954
     acc:0.327093
 lu:
     loss:14231.250034
     acc:0.884386
 lv:
     loss:17513.235863
     acc:0.204766
 le:
     loss:1519.857491
     acc:0.999222
 encoder:
     loss:0.056970
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19183.453125
     acc:0.472380
 tv:
     loss:19883.620312
     acc:0.321583
 lu:
     loss:13551.152246
     acc:0.884535
 lv:
     loss:16777.248926
     acc:0.183382
 le:
     loss:1448.450586
     acc:0.999186
 encoder:
     loss:0.040865
----------------------------


Epoch: [116/1000]:
train:
----------------------------
 tu:
     loss:20133.437364
     acc:0.473044
 tv:
     loss:20838.874296
     acc:0.328990
 lu:
     loss:14229.530830
     acc:0.884701
 lv:
     loss:17513.614508
     acc:0.204540
 le:
     loss:1519.765365
     acc:0.999257
 encoder:
     loss:0.080884
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19182.326465
     acc:0.472627
 tv:
     loss:19878.447070
     acc:0.322999
 lu:
     loss:13553.294238
     acc:0.884212
 lv:
     loss:16776.615527
     acc:0.183539
 le:
     loss:1448.351727
     acc:0.999208
 encoder:
     loss:0.065492
----------------------------


Epoch: [117/1000]:
train:
----------------------------
 tu:
     loss:20109.366767
     acc:0.478607
 tv:
     loss:20832.118959
     acc:0.330378
 lu:
     loss:14229.501215
     acc:0.884792
 lv:
     loss:17508.119674
     acc:0.205894
 le:
     loss:1519.815966
     acc:0.999250
 encoder:
     loss:0.051203
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19148.797461
     acc:0.479957
 tv:
     loss:19875.429590
     acc:0.323070
 lu:
     loss:13549.749707
     acc:0.885080
 lv:
     loss:16776.354590
     acc:0.184110
 le:
     loss:1448.445715
     acc:0.999144
 encoder:
     loss:0.062125
----------------------------


Epoch: [118/1000]:
train:
----------------------------
 tu:
     loss:20095.587993
     acc:0.481254
 tv:
     loss:20830.533510
     acc:0.330409
 lu:
     loss:14227.976983
     acc:0.885003
 lv:
     loss:17504.625602
     acc:0.206591
 le:
     loss:1519.888425
     acc:0.999220
 encoder:
     loss:0.048154
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19148.591406
     acc:0.479783
 tv:
     loss:19875.370117
     acc:0.323179
 lu:
     loss:13549.898584
     acc:0.885003
 lv:
     loss:16775.439648
     acc:0.183622
 le:
     loss:1448.334357
     acc:0.999149
 encoder:
     loss:0.041701
----------------------------


Epoch: [119/1000]:
train:
----------------------------
 tu:
     loss:20093.028729
     acc:0.481452
 tv:
     loss:20828.254610
     acc:0.330994
 lu:
     loss:14226.756359
     acc:0.885459
 lv:
     loss:17503.370549
     acc:0.207240
 le:
     loss:1519.866119
     acc:0.999234
 encoder:
     loss:0.050574
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19147.354297
     acc:0.480131
 tv:
     loss:19875.373145
     acc:0.323059
 lu:
     loss:13549.123926
     acc:0.885344
 lv:
     loss:16773.890332
     acc:0.184647
 le:
     loss:1448.356177
     acc:0.999207
 encoder:
     loss:0.041139
----------------------------


Epoch: [120/1000]:
train:
----------------------------
 tu:
     loss:20091.798215
     acc:0.481496
 tv:
     loss:20826.147779
     acc:0.331260
 lu:
     loss:14225.855764
     acc:0.885661
 lv:
     loss:17499.568711
     acc:0.208133
 le:
     loss:1519.807135
     acc:0.999236
 encoder:
     loss:0.052705
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19146.136328
     acc:0.480351
 tv:
     loss:19872.001660
     acc:0.323611
 lu:
     loss:13548.576025
     acc:0.885373
 lv:
     loss:16773.291016
     acc:0.184326
 le:
     loss:1448.450464
     acc:0.999204
 encoder:
     loss:0.048676
----------------------------


Epoch: [121/1000]:
train:
----------------------------
 tu:
     loss:20090.119583
     acc:0.481872
 tv:
     loss:20823.135890
     acc:0.331748
 lu:
     loss:14225.306800
     acc:0.885631
 lv:
     loss:17497.213402
     acc:0.208198
 le:
     loss:1519.671817
     acc:0.999260
 encoder:
     loss:0.064490
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19146.711719
     acc:0.480082
 tv:
     loss:19871.817871
     acc:0.324113
 lu:
     loss:13546.652100
     acc:0.885729
 lv:
     loss:16777.409570
     acc:0.183524
 le:
     loss:1448.542505
     acc:0.999144
 encoder:
     loss:0.039835
----------------------------


Epoch: [122/1000]:
train:
----------------------------
 tu:
     loss:20067.222656
     acc:0.487350
 tv:
     loss:20811.487202
     acc:0.334135
 lu:
     loss:14223.432844
     acc:0.885966
 lv:
     loss:17494.315475
     acc:0.209199
 le:
     loss:1519.796431
     acc:0.999243
 encoder:
     loss:0.059942
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19111.217090
     acc:0.487941
 tv:
     loss:19857.369922
     acc:0.326784
 lu:
     loss:13547.741895
     acc:0.885512
 lv:
     loss:16774.710059
     acc:0.183974
 le:
     loss:1448.485168
     acc:0.999166
 encoder:
     loss:0.121010
----------------------------


Epoch: [123/1000]:
train:
----------------------------
 tu:
     loss:20055.785463
     acc:0.489407
 tv:
     loss:20808.310002
     acc:0.334749
 lu:
     loss:14223.995549
     acc:0.885964
 lv:
     loss:17492.113440
     acc:0.209843
 le:
     loss:1519.806703
     acc:0.999244
 encoder:
     loss:0.078437
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19110.166992
     acc:0.487849
 tv:
     loss:19857.921582
     acc:0.326758
 lu:
     loss:13547.352979
     acc:0.885573
 lv:
     loss:16773.531934
     acc:0.184125
 le:
     loss:1448.711169
     acc:0.999138
 encoder:
     loss:0.042092
----------------------------


Epoch: [124/1000]:
train:
----------------------------
 tu:
     loss:20053.302008
     acc:0.489692
 tv:
     loss:20804.678484
     acc:0.335672
 lu:
     loss:14223.143714
     acc:0.885993
 lv:
     loss:17488.512775
     acc:0.210342
 le:
     loss:1519.723461
     acc:0.999264
 encoder:
     loss:0.055536
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19108.991699
     acc:0.488190
 tv:
     loss:19856.039062
     acc:0.327081
 lu:
     loss:13548.135986
     acc:0.885421
 lv:
     loss:16775.292383
     acc:0.183966
 le:
     loss:1448.381317
     acc:0.999228
 encoder:
     loss:0.044833
----------------------------


Epoch: [125/1000]:
train:
----------------------------
 tu:
     loss:20037.149573
     acc:0.493084
 tv:
     loss:20796.346112
     acc:0.337333
 lu:
     loss:14223.245560
     acc:0.885884
 lv:
     loss:17488.153877
     acc:0.210412
 le:
     loss:1519.828336
     acc:0.999220
 encoder:
     loss:0.051283
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19086.509961
     acc:0.493201
 tv:
     loss:19837.473633
     acc:0.331617
 lu:
     loss:13546.979785
     acc:0.885382
 lv:
     loss:16780.388965
     acc:0.182712
 le:
     loss:1448.415210
     acc:0.999205
 encoder:
     loss:0.044268
----------------------------


Epoch: [126/1000]:
train:
----------------------------
 tu:
     loss:20024.904797
     acc:0.495415
 tv:
     loss:20777.076751
     acc:0.341792
 lu:
     loss:14221.465866
     acc:0.886330
 lv:
     loss:17483.839128
     acc:0.211002
 le:
     loss:1519.789501
     acc:0.999262
 encoder:
     loss:0.080126
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19086.372949
     acc:0.493081
 tv:
     loss:19819.836328
     acc:0.335489
 lu:
     loss:13546.763525
     acc:0.885261
 lv:
     loss:16777.290723
     acc:0.182958
 le:
     loss:1448.464600
     acc:0.999184
 encoder:
     loss:0.074970
----------------------------


Epoch: [127/1000]:
train:
----------------------------
 tu:
     loss:20005.501045
     acc:0.500033
 tv:
     loss:20765.835642
     acc:0.344180
 lu:
     loss:14221.710790
     acc:0.886254
 lv:
     loss:17483.663552
     acc:0.211339
 le:
     loss:1519.706743
     acc:0.999273
 encoder:
     loss:0.063960
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19053.030859
     acc:0.500676
 tv:
     loss:19814.038379
     acc:0.336670
 lu:
     loss:13546.528516
     acc:0.885911
 lv:
     loss:16777.865332
     acc:0.183841
 le:
     loss:1449.222479
     acc:0.999016
 encoder:
     loss:0.043297
----------------------------


Epoch: [128/1000]:
train:
----------------------------
 tu:
     loss:19992.002725
     acc:0.502608
 tv:
     loss:20758.631847
     acc:0.345639
 lu:
     loss:14221.255644
     acc:0.886322
 lv:
     loss:17479.732592
     acc:0.212230
 le:
     loss:1519.647098
     acc:0.999270
 encoder:
     loss:0.079856
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19051.019824
     acc:0.500722
 tv:
     loss:19799.422461
     acc:0.340041
 lu:
     loss:13545.179150
     acc:0.885631
 lv:
     loss:16775.830957
     acc:0.183457
 le:
     loss:1448.389447
     acc:0.999208
 encoder:
     loss:0.152390
----------------------------


Epoch: [129/1000]:
train:
----------------------------
 tu:
     loss:19988.367767
     acc:0.503141
 tv:
     loss:20746.524073
     acc:0.348351
 lu:
     loss:14220.529899
     acc:0.886391
 lv:
     loss:17479.594897
     acc:0.211768
 le:
     loss:1519.652470
     acc:0.999272
 encoder:
     loss:0.112639
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19051.118750
     acc:0.500721
 tv:
     loss:19798.394531
     acc:0.339996
 lu:
     loss:13547.161279
     acc:0.885628
 lv:
     loss:16771.852637
     acc:0.184644
 le:
     loss:1448.805579
     acc:0.999080
 encoder:
     loss:0.117035
----------------------------


Epoch: [130/1000]:
train:
----------------------------
 tu:
     loss:19986.955237
     acc:0.503375
 tv:
     loss:20741.813022
     acc:0.349046
 lu:
     loss:14220.804756
     acc:0.886327
 lv:
     loss:17477.222951
     acc:0.212659
 le:
     loss:1519.756972
     acc:0.999247
 encoder:
     loss:0.102262
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19050.006934
     acc:0.501073
 tv:
     loss:19792.406543
     acc:0.340910
 lu:
     loss:13545.351367
     acc:0.885583
 lv:
     loss:16778.016797
     acc:0.183264
 le:
     loss:1448.645380
     acc:0.999146
 encoder:
     loss:0.091440
----------------------------


Epoch: [131/1000]:
train:
----------------------------
 tu:
     loss:19985.774437
     acc:0.503440
 tv:
     loss:20727.013467
     acc:0.352314
 lu:
     loss:14219.327591
     acc:0.886813
 lv:
     loss:17474.990950
     acc:0.212987
 le:
     loss:1519.727956
     acc:0.999247
 encoder:
     loss:0.098093
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19050.271094
     acc:0.501102
 tv:
     loss:19778.635156
     acc:0.344206
 lu:
     loss:13546.442139
     acc:0.885616
 lv:
     loss:16773.851855
     acc:0.183805
 le:
     loss:1448.589148
     acc:0.999164
 encoder:
     loss:0.154416
----------------------------


Epoch: [132/1000]:
train:
----------------------------
 tu:
     loss:19969.268952
     acc:0.507392
 tv:
     loss:20721.094375
     acc:0.353372
 lu:
     loss:14218.149471
     acc:0.886915
 lv:
     loss:17470.767680
     acc:0.214139
 le:
     loss:1519.588128
     acc:0.999289
 encoder:
     loss:0.071752
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19016.731055
     acc:0.508667
 tv:
     loss:19773.104492
     acc:0.345710
 lu:
     loss:13544.918994
     acc:0.885787
 lv:
     loss:16773.618945
     acc:0.184174
 le:
     loss:1448.526312
     acc:0.999184
 encoder:
     loss:0.066642
----------------------------


Epoch: [133/1000]:
train:
----------------------------
 tu:
     loss:19956.116097
     acc:0.510011
 tv:
     loss:20718.725166
     acc:0.353696
 lu:
     loss:14219.013354
     acc:0.886834
 lv:
     loss:17469.026435
     acc:0.214540
 le:
     loss:1519.684732
     acc:0.999281
 encoder:
     loss:0.070180
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:19015.285742
     acc:0.508600
 tv:
     loss:19775.337207
     acc:0.344898
 lu:
     loss:13544.824219
     acc:0.885844
 lv:
     loss:16774.430469
     acc:0.183765
 le:
     loss:1448.398627
     acc:0.999166
 encoder:
     loss:0.062232
----------------------------


Epoch: [134/1000]:
train:
----------------------------
 tu:
     loss:19936.402707
     acc:0.514566
 tv:
     loss:20717.673828
     acc:0.353631
 lu:
     loss:14217.166697
     acc:0.887170
 lv:
     loss:17466.981037
     acc:0.214781
 le:
     loss:1519.646020
     acc:0.999269
 encoder:
     loss:0.056893
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18979.096582
     acc:0.516670
 tv:
     loss:19773.673047
     acc:0.345451
 lu:
     loss:13544.235156
     acc:0.885912
 lv:
     loss:16773.525293
     acc:0.183495
 le:
     loss:1448.432062
     acc:0.999166
 encoder:
     loss:0.047715
----------------------------


Epoch: [135/1000]:
train:
----------------------------
 tu:
     loss:19918.878906
     acc:0.517984
 tv:
     loss:20710.195119
     acc:0.355629
 lu:
     loss:14215.532999
     acc:0.887312
 lv:
     loss:17464.483887
     acc:0.215385
 le:
     loss:1519.573246
     acc:0.999285
 encoder:
     loss:0.059586
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18977.049902
     acc:0.517120
 tv:
     loss:19758.563281
     acc:0.348581
 lu:
     loss:13544.016211
     acc:0.886073
 lv:
     loss:16778.001660
     acc:0.183063
 le:
     loss:1448.368152
     acc:0.999164
 encoder:
     loss:0.059197
----------------------------


Epoch: [136/1000]:
train:
----------------------------
 tu:
     loss:19915.169195
     acc:0.518568
 tv:
     loss:20702.062954
     acc:0.357349
 lu:
     loss:14215.975654
     acc:0.887106
 lv:
     loss:17461.746253
     acc:0.215567
 le:
     loss:1519.609831
     acc:0.999277
 encoder:
     loss:0.060339
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18977.068164
     acc:0.517007
 tv:
     loss:19760.583301
     acc:0.348378
 lu:
     loss:13544.158984
     acc:0.885977
 lv:
     loss:16775.740527
     acc:0.183312
 le:
     loss:1448.353491
     acc:0.999206
 encoder:
     loss:0.048857
----------------------------


Epoch: [137/1000]:
train:
----------------------------
 tu:
     loss:19913.028298
     acc:0.518907
 tv:
     loss:20697.990916
     acc:0.358082
 lu:
     loss:14217.149017
     acc:0.886815
 lv:
     loss:17460.575002
     acc:0.215860
 le:
     loss:1519.599564
     acc:0.999288
 encoder:
     loss:0.054296
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18975.979199
     acc:0.517197
 tv:
     loss:19756.640039
     acc:0.349214
 lu:
     loss:13545.103564
     acc:0.885608
 lv:
     loss:16774.620605
     acc:0.183459
 le:
     loss:1448.793616
     acc:0.999102
 encoder:
     loss:0.050762
----------------------------


Epoch: [138/1000]:
train:
----------------------------
 tu:
     loss:19912.065282
     acc:0.518883
 tv:
     loss:20697.578466
     acc:0.358085
 lu:
     loss:14217.925350
     acc:0.886810
 lv:
     loss:17458.935297
     acc:0.216315
 le:
     loss:1519.556110
     acc:0.999299
 encoder:
     loss:0.062720
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18974.915332
     acc:0.517496
 tv:
     loss:19754.459375
     acc:0.349454
 lu:
     loss:13543.396094
     acc:0.886169
 lv:
     loss:16776.235645
     acc:0.183263
 le:
     loss:1448.362433
     acc:0.999187
 encoder:
     loss:0.064880
----------------------------


Epoch: [139/1000]:
train:
----------------------------
 tu:
     loss:19910.440259
     acc:0.519165
 tv:
     loss:20694.895167
     acc:0.358529
 lu:
     loss:14217.175668
     acc:0.887019
 lv:
     loss:17458.748467
     acc:0.216465
 le:
     loss:1519.651996
     acc:0.999270
 encoder:
     loss:0.067945
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18974.348047
     acc:0.517729
 tv:
     loss:19756.387793
     acc:0.348937
 lu:
     loss:13542.726904
     acc:0.886634
 lv:
     loss:16779.319824
     acc:0.182667
 le:
     loss:1448.361444
     acc:0.999190
 encoder:
     loss:0.082196
----------------------------


Epoch: [140/1000]:
train:
----------------------------
 tu:
     loss:19909.447799
     acc:0.519189
 tv:
     loss:20684.757801
     acc:0.360690
 lu:
     loss:14214.955578
     acc:0.887488
 lv:
     loss:17455.102448
     acc:0.216953
 le:
     loss:1519.470631
     acc:0.999314
 encoder:
     loss:0.070392
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18973.289258
     acc:0.517677
 tv:
     loss:19738.908203
     acc:0.352633
 lu:
     loss:13542.690283
     acc:0.886368
 lv:
     loss:16772.773926
     acc:0.183875
 le:
     loss:1448.443359
     acc:0.999205
 encoder:
     loss:0.049119
----------------------------


Epoch: [141/1000]:
train:
----------------------------
 tu:
     loss:19885.704760
     acc:0.525120
 tv:
     loss:20677.706872
     acc:0.362277
 lu:
     loss:14214.405728
     acc:0.887414
 lv:
     loss:17452.162200
     acc:0.217769
 le:
     loss:1519.405762
     acc:0.999340
 encoder:
     loss:0.053591
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18903.611230
     acc:0.534042
 tv:
     loss:19736.344531
     acc:0.353282
 lu:
     loss:13542.453955
     acc:0.886319
 lv:
     loss:16771.250781
     acc:0.185227
 le:
     loss:1448.614917
     acc:0.999087
 encoder:
     loss:0.056497
----------------------------


Epoch: [142/1000]:
train:
----------------------------
 tu:
     loss:19823.002339
     acc:0.539280
 tv:
     loss:20668.252634
     acc:0.364209
 lu:
     loss:14215.122922
     acc:0.887307
 lv:
     loss:17449.407738
     acc:0.218505
 le:
     loss:1519.573322
     acc:0.999290
 encoder:
     loss:0.087025
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18864.791016
     acc:0.542022
 tv:
     loss:19725.009766
     acc:0.355603
 lu:
     loss:13542.102539
     acc:0.886386
 lv:
     loss:16772.741699
     acc:0.183732
 le:
     loss:1448.702502
     acc:0.999104
 encoder:
     loss:0.070883
----------------------------


Epoch: [143/1000]:
train:
----------------------------
 tu:
     loss:19805.962981
     acc:0.542022
 tv:
     loss:20661.047284
     acc:0.365842
 lu:
     loss:14215.016613
     acc:0.887491
 lv:
     loss:17448.647302
     acc:0.218702
 le:
     loss:1519.489268
     acc:0.999306
 encoder:
     loss:0.057553
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18862.603418
     acc:0.542306
 tv:
     loss:19725.072852
     acc:0.355654
 lu:
     loss:13544.750586
     acc:0.885655
 lv:
     loss:16774.117969
     acc:0.183928
 le:
     loss:1448.278284
     acc:0.999229
 encoder:
     loss:0.089402
----------------------------


Epoch: [144/1000]:
train:
----------------------------
 tu:
     loss:19799.596589
     acc:0.542870
 tv:
     loss:20661.533487
     acc:0.365449
 lu:
     loss:14213.920342
     acc:0.887478
 lv:
     loss:17449.183230
     acc:0.218188
 le:
     loss:1519.417155
     acc:0.999313
 encoder:
     loss:0.084799
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18862.624609
     acc:0.541957
 tv:
     loss:19721.865039
     acc:0.356767
 lu:
     loss:13543.820117
     acc:0.886231
 lv:
     loss:16776.443457
     acc:0.182873
 le:
     loss:1449.084497
     acc:0.999040
 encoder:
     loss:0.058197
----------------------------


Epoch: [145/1000]:
train:
----------------------------
 tu:
     loss:19797.310161
     acc:0.543111
 tv:
     loss:20650.996696
     acc:0.367887
 lu:
     loss:14213.647995
     acc:0.887714
 lv:
     loss:17445.373626
     acc:0.219434
 le:
     loss:1519.468475
     acc:0.999299
 encoder:
     loss:0.087533
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18861.506445
     acc:0.542149
 tv:
     loss:19705.844238
     acc:0.360209
 lu:
     loss:13542.636328
     acc:0.886019
 lv:
     loss:16774.264258
     acc:0.183237
 le:
     loss:1448.579260
     acc:0.999166
 encoder:
     loss:0.082978
----------------------------


Epoch: [146/1000]:
train:
----------------------------
 tu:
     loss:19795.345783
     acc:0.543432
 tv:
     loss:20641.439044
     acc:0.369558
 lu:
     loss:14209.636639
     acc:0.888628
 lv:
     loss:17443.632778
     acc:0.219198
 le:
     loss:1519.391560
     acc:0.999340
 encoder:
     loss:0.146864
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18859.577148
     acc:0.542634
 tv:
     loss:19705.733008
     acc:0.359985
 lu:
     loss:13533.378516
     acc:0.888612
 lv:
     loss:16772.083594
     acc:0.183640
 le:
     loss:1448.564856
     acc:0.999146
 encoder:
     loss:0.099644
----------------------------


Epoch: [147/1000]:
train:
----------------------------
 tu:
     loss:19793.042867
     acc:0.543843
 tv:
     loss:20631.123160
     acc:0.372364
 lu:
     loss:14198.872320
     acc:0.891010
 lv:
     loss:17440.217808
     acc:0.220252
 le:
     loss:1519.320622
     acc:0.999361
 encoder:
     loss:0.123145
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18860.077148
     acc:0.542500
 tv:
     loss:19677.586133
     acc:0.366176
 lu:
     loss:13527.850732
     acc:0.889412
 lv:
     loss:16767.510937
     acc:0.185426
 le:
     loss:1448.417236
     acc:0.999147
 encoder:
     loss:0.089861
----------------------------


Epoch: [148/1000]:
train:
----------------------------
 tu:
     loss:19792.478095
     acc:0.543787
 tv:
     loss:20615.191077
     acc:0.375743
 lu:
     loss:14195.458928
     acc:0.891776
 lv:
     loss:17440.172704
     acc:0.220299
 le:
     loss:1519.369943
     acc:0.999334
 encoder:
     loss:0.151753
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18859.423730
     acc:0.542479
 tv:
     loss:19674.544727
     acc:0.366757
 lu:
     loss:13525.720801
     acc:0.889920
 lv:
     loss:16771.531543
     acc:0.184570
 le:
     loss:1448.596509
     acc:0.999147
 encoder:
     loss:0.211445
----------------------------


Epoch: [149/1000]:
train:
----------------------------
 tu:
     loss:19791.598360
     acc:0.543890
 tv:
     loss:20613.135947
     acc:0.375872
 lu:
     loss:14192.140886
     acc:0.892154
 lv:
     loss:17437.702762
     acc:0.220433
 le:
     loss:1519.347720
     acc:0.999334
 encoder:
     loss:0.120534
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18858.932227
     acc:0.542730
 tv:
     loss:19672.630664
     acc:0.366978
 lu:
     loss:13522.237793
     acc:0.890563
 lv:
     loss:16768.284180
     acc:0.184747
 le:
     loss:1448.644287
     acc:0.999103
 encoder:
     loss:0.069594
----------------------------


Epoch: [150/1000]:
train:
----------------------------
 tu:
     loss:19790.845714
     acc:0.543896
 tv:
     loss:20609.662791
     acc:0.376435
 lu:
     loss:14184.921864
     acc:0.894262
 lv:
     loss:17435.938511
     acc:0.221372
 le:
     loss:1519.401956
     acc:0.999330
 encoder:
     loss:0.081832
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18858.756055
     acc:0.542844
 tv:
     loss:19675.129102
     acc:0.366551
 lu:
     loss:13514.067529
     acc:0.892681
 lv:
     loss:16771.066504
     acc:0.184281
 le:
     loss:1448.550293
     acc:0.999107
 encoder:
     loss:0.141268
----------------------------


Epoch: [151/1000]:
train:
----------------------------
 tu:
     loss:19789.695358
     acc:0.544229
 tv:
     loss:20606.132336
     acc:0.377281
 lu:
     loss:14177.780750
     acc:0.895692
 lv:
     loss:17433.005303
     acc:0.221501
 le:
     loss:1519.419649
     acc:0.999327
 encoder:
     loss:0.115570
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18858.997852
     acc:0.542799
 tv:
     loss:19672.024414
     acc:0.367338
 lu:
     loss:13510.450342
     acc:0.893308
 lv:
     loss:16772.415820
     acc:0.183468
 le:
     loss:1448.442603
     acc:0.999167
 encoder:
     loss:0.155279
----------------------------


Epoch: [152/1000]:
train:
----------------------------
 tu:
     loss:19789.827160
     acc:0.544044
 tv:
     loss:20604.001238
     acc:0.377595
 lu:
     loss:14175.708360
     acc:0.895953
 lv:
     loss:17430.090457
     acc:0.222582
 le:
     loss:1519.501942
     acc:0.999291
 encoder:
     loss:0.088571
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18858.805371
     acc:0.542819
 tv:
     loss:19661.228516
     acc:0.369886
 lu:
     loss:13509.221240
     acc:0.893766
 lv:
     loss:16771.587109
     acc:0.183907
 le:
     loss:1448.671368
     acc:0.999126
 encoder:
     loss:0.058584
----------------------------


Epoch: [153/1000]:
train:
----------------------------
 tu:
     loss:19773.677155
     acc:0.548052
 tv:
     loss:20594.896973
     acc:0.379588
 lu:
     loss:14174.441440
     acc:0.896060
 lv:
     loss:17429.869686
     acc:0.222137
 le:
     loss:1519.414315
     acc:0.999314
 encoder:
     loss:0.070886
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18826.446680
     acc:0.550076
 tv:
     loss:19659.011133
     acc:0.370098
 lu:
     loss:13507.842969
     acc:0.893790
 lv:
     loss:16771.896582
     acc:0.184057
 le:
     loss:1448.578571
     acc:0.999143
 encoder:
     loss:0.073669
----------------------------


Epoch: [154/1000]:
train:
----------------------------
 tu:
     loss:19758.138172
     acc:0.551151
 tv:
     loss:20591.371423
     acc:0.380266
 lu:
     loss:14172.475983
     acc:0.896517
 lv:
     loss:17426.693621
     acc:0.222912
 le:
     loss:1519.338886
     acc:0.999338
 encoder:
     loss:0.071175
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18824.955371
     acc:0.550052
 tv:
     loss:19647.598633
     acc:0.373302
 lu:
     loss:13507.139307
     acc:0.893730
 lv:
     loss:16773.760254
     acc:0.183336
 le:
     loss:1448.756256
     acc:0.999087
 encoder:
     loss:0.056739
----------------------------


Epoch: [155/1000]:
train:
----------------------------
 tu:
     loss:19755.177609
     acc:0.551560
 tv:
     loss:20580.440407
     acc:0.382862
 lu:
     loss:14171.896859
     acc:0.896695
 lv:
     loss:17424.635640
     acc:0.223697
 le:
     loss:1519.423760
     acc:0.999319
 encoder:
     loss:0.078982
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18823.901465
     acc:0.550424
 tv:
     loss:19641.984082
     acc:0.373712
 lu:
     loss:13507.376172
     acc:0.894061
 lv:
     loss:16769.788184
     acc:0.185266
 le:
     loss:1448.564307
     acc:0.999125
 encoder:
     loss:0.060197
----------------------------


Epoch: [156/1000]:
train:
----------------------------
 tu:
     loss:19754.048340
     acc:0.551646
 tv:
     loss:20573.562091
     acc:0.383816
 lu:
     loss:14169.397563
     acc:0.897056
 lv:
     loss:17424.637264
     acc:0.223092
 le:
     loss:1519.287935
     acc:0.999345
 encoder:
     loss:0.094175
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18823.408984
     acc:0.550427
 tv:
     loss:19642.055957
     acc:0.374182
 lu:
     loss:13506.479736
     acc:0.893800
 lv:
     loss:16767.694434
     acc:0.185123
 le:
     loss:1448.460480
     acc:0.999187
 encoder:
     loss:0.086063
----------------------------


Epoch: [157/1000]:
train:
----------------------------
 tu:
     loss:19752.955987
     acc:0.551791
 tv:
     loss:20571.913926
     acc:0.384348
 lu:
     loss:14169.861748
     acc:0.897008
 lv:
     loss:17422.925202
     acc:0.223709
 le:
     loss:1519.383518
     acc:0.999335
 encoder:
     loss:0.085772
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18823.232129
     acc:0.550469
 tv:
     loss:19640.547461
     acc:0.374176
 lu:
     loss:13504.835449
     acc:0.894193
 lv:
     loss:16770.121094
     acc:0.184139
 le:
     loss:1448.589142
     acc:0.999105
 encoder:
     loss:0.071116
----------------------------


Epoch: [158/1000]:
train:
----------------------------
 tu:
     loss:19751.063159
     acc:0.552127
 tv:
     loss:20569.648097
     acc:0.384809
 lu:
     loss:14169.313636
     acc:0.896997
 lv:
     loss:17420.201274
     acc:0.224245
 le:
     loss:1519.244135
     acc:0.999360
 encoder:
     loss:0.126207
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18823.449219
     acc:0.550460
 tv:
     loss:19638.979785
     acc:0.374777
 lu:
     loss:13505.845898
     acc:0.894276
 lv:
     loss:16768.852734
     acc:0.184849
 le:
     loss:1448.566901
     acc:0.999186
 encoder:
     loss:0.156035
----------------------------


Epoch: [159/1000]:
train:
----------------------------
 tu:
     loss:19750.577262
     acc:0.552295
 tv:
     loss:20559.791969
     acc:0.387209
 lu:
     loss:14168.149550
     acc:0.897209
 lv:
     loss:17417.858955
     acc:0.224811
 le:
     loss:1519.386900
     acc:0.999327
 encoder:
     loss:0.133931
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18822.518652
     acc:0.550583
 tv:
     loss:19619.864648
     acc:0.379073
 lu:
     loss:13507.147949
     acc:0.893716
 lv:
     loss:16765.173438
     acc:0.185251
 le:
     loss:1448.562524
     acc:0.999146
 encoder:
     loss:0.058268
----------------------------


Epoch: [160/1000]:
train:
----------------------------
 tu:
     loss:19750.556629
     acc:0.552150
 tv:
     loss:20546.432844
     acc:0.390293
 lu:
     loss:14167.240791
     acc:0.897275
 lv:
     loss:17417.297806
     acc:0.224888
 le:
     loss:1519.267859
     acc:0.999361
 encoder:
     loss:0.065117
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18821.994824
     acc:0.550775
 tv:
     loss:19608.162988
     acc:0.381827
 lu:
     loss:13505.207422
     acc:0.894286
 lv:
     loss:16767.854687
     acc:0.185367
 le:
     loss:1449.004626
     acc:0.999042
 encoder:
     loss:0.082823
----------------------------


Epoch: [161/1000]:
train:
----------------------------
 tu:
     loss:19749.269259
     acc:0.552317
 tv:
     loss:20529.968137
     acc:0.394276
 lu:
     loss:14167.925577
     acc:0.897274
 lv:
     loss:17415.638422
     acc:0.225235
 le:
     loss:1519.341497
     acc:0.999320
 encoder:
     loss:0.151846
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18822.700098
     acc:0.550774
 tv:
     loss:19588.328223
     acc:0.385873
 lu:
     loss:13503.930273
     acc:0.894492
 lv:
     loss:16767.577539
     acc:0.184602
 le:
     loss:1448.684412
     acc:0.999125
 encoder:
     loss:0.156593
----------------------------


Epoch: [162/1000]:
train:
----------------------------
 tu:
     loss:19749.997979
     acc:0.552230
 tv:
     loss:20513.735317
     acc:0.397410
 lu:
     loss:14166.742880
     acc:0.897438
 lv:
     loss:17413.423419
     acc:0.225979
 le:
     loss:1519.263621
     acc:0.999354
 encoder:
     loss:0.097513
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18822.781055
     acc:0.550347
 tv:
     loss:19583.447949
     acc:0.386677
 lu:
     loss:13504.370312
     acc:0.894360
 lv:
     loss:16767.637305
     acc:0.184811
 le:
     loss:1448.715295
     acc:0.999087
 encoder:
     loss:0.089658
----------------------------


Epoch: [163/1000]:
train:
----------------------------
 tu:
     loss:19749.094931
     acc:0.552310
 tv:
     loss:20510.020848
     acc:0.397971
 lu:
     loss:14166.870038
     acc:0.897388
 lv:
     loss:17412.150856
     acc:0.225955
 le:
     loss:1519.295356
     acc:0.999341
 encoder:
     loss:0.078527
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18822.837012
     acc:0.550552
 tv:
     loss:19582.571973
     acc:0.386672
 lu:
     loss:13504.152051
     acc:0.894544
 lv:
     loss:16766.880957
     acc:0.184620
 le:
     loss:1448.497540
     acc:0.999149
 encoder:
     loss:0.054253
----------------------------


Epoch: [164/1000]:
train:
----------------------------
 tu:
     loss:19748.596759
     acc:0.552270
 tv:
     loss:20507.992131
     acc:0.398007
 lu:
     loss:14166.047988
     acc:0.897627
 lv:
     loss:17413.175724
     acc:0.225919
 le:
     loss:1519.283798
     acc:0.999355
 encoder:
     loss:0.107344
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18822.198242
     acc:0.550528
 tv:
     loss:19579.048438
     acc:0.387775
 lu:
     loss:13504.950391
     acc:0.894357
 lv:
     loss:16767.737207
     acc:0.184625
 le:
     loss:1448.828296
     acc:0.999047
 encoder:
     loss:0.120656
----------------------------


Epoch: [165/1000]:
train:
----------------------------
 tu:
     loss:19748.364076
     acc:0.552444
 tv:
     loss:20503.737634
     acc:0.398909
 lu:
     loss:14164.059718
     acc:0.897992
 lv:
     loss:17408.656046
     acc:0.226565
 le:
     loss:1519.317212
     acc:0.999331
 encoder:
     loss:0.121245
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18822.539062
     acc:0.550511
 tv:
     loss:19578.634766
     acc:0.387645
 lu:
     loss:13503.990137
     acc:0.894751
 lv:
     loss:16765.717676
     acc:0.185532
 le:
     loss:1448.685571
     acc:0.999125
 encoder:
     loss:0.067535
----------------------------


Epoch: [166/1000]:
train:
----------------------------
 tu:
     loss:19748.841581
     acc:0.552256
 tv:
     loss:20499.877816
     acc:0.399475
 lu:
     loss:14165.154138
     acc:0.897783
 lv:
     loss:17403.661757
     acc:0.228032
 le:
     loss:1519.221762
     acc:0.999369
 encoder:
     loss:0.077958
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18821.375000
     acc:0.550861
 tv:
     loss:19576.610547
     acc:0.387941
 lu:
     loss:13504.653271
     acc:0.894275
 lv:
     loss:16761.698828
     acc:0.186390
 le:
     loss:1448.560699
     acc:0.999129
 encoder:
     loss:0.231228
----------------------------


Epoch: [167/1000]:
train:
----------------------------
 tu:
     loss:19748.434037
     acc:0.552330
 tv:
     loss:20498.615564
     acc:0.399557
 lu:
     loss:14163.757688
     acc:0.898024
 lv:
     loss:17406.261265
     acc:0.227210
 le:
     loss:1519.247388
     acc:0.999363
 encoder:
     loss:0.164735
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18822.340723
     acc:0.550594
 tv:
     loss:19575.029492
     acc:0.388601
 lu:
     loss:13503.776074
     acc:0.894308
 lv:
     loss:16761.857422
     acc:0.186480
 le:
     loss:1448.577057
     acc:0.999107
 encoder:
     loss:0.085579
----------------------------


Epoch: [168/1000]:
train:
----------------------------
 tu:
     loss:19747.462732
     acc:0.552523
 tv:
     loss:20496.596589
     acc:0.400153
 lu:
     loss:14162.310070
     acc:0.898075
 lv:
     loss:17402.587130
     acc:0.228191
 le:
     loss:1519.176675
     acc:0.999381
 encoder:
     loss:0.149301
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18821.801758
     acc:0.550681
 tv:
     loss:19566.212695
     acc:0.390265
 lu:
     loss:13502.943701
     acc:0.894861
 lv:
     loss:16761.087891
     acc:0.186554
 le:
     loss:1449.238684
     acc:0.998980
 encoder:
     loss:0.208543
----------------------------


Epoch: [169/1000]:
train:
----------------------------
 tu:
     loss:19748.210847
     acc:0.552286
 tv:
     loss:20482.418911
     acc:0.403065
 lu:
     loss:14162.351528
     acc:0.898270
 lv:
     loss:17401.732547
     acc:0.228259
 le:
     loss:1519.177279
     acc:0.999370
 encoder:
     loss:0.660019
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18821.477734
     acc:0.550781
 tv:
     loss:19555.710840
     acc:0.392104
 lu:
     loss:13505.185840
     acc:0.894144
 lv:
     loss:16763.861133
     acc:0.186068
 le:
     loss:1448.715674
     acc:0.999108
 encoder:
     loss:0.548075
----------------------------


Epoch: [170/1000]:
train:
----------------------------
 tu:
     loss:19746.984716
     acc:0.552590
 tv:
     loss:20475.267203
     acc:0.404641
 lu:
     loss:14163.087766
     acc:0.897975
 lv:
     loss:17396.549055
     acc:0.228992
 le:
     loss:1519.174426
     acc:0.999396
 encoder:
     loss:0.304014
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18821.213867
     acc:0.550841
 tv:
     loss:19542.833008
     acc:0.395727
 lu:
     loss:13503.403467
     acc:0.894698
 lv:
     loss:16766.236621
     acc:0.185159
 le:
     loss:1448.736462
     acc:0.999087
 encoder:
     loss:0.100154
----------------------------


Epoch: [171/1000]:
train:
----------------------------
 tu:
     loss:19746.797783
     acc:0.552638
 tv:
     loss:20466.436592
     acc:0.406469
 lu:
     loss:14162.499364
     acc:0.898306
 lv:
     loss:17396.300622
     acc:0.229225
 le:
     loss:1519.240433
     acc:0.999364
 encoder:
     loss:0.075377
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18821.215918
     acc:0.550734
 tv:
     loss:19541.480664
     acc:0.395587
 lu:
     loss:13501.754590
     acc:0.894867
 lv:
     loss:16759.229687
     acc:0.187460
 le:
     loss:1448.594202
     acc:0.999149
 encoder:
     loss:0.079379
----------------------------


Epoch: [172/1000]:
train:
----------------------------
 tu:
     loss:19738.714685
     acc:0.554372
 tv:
     loss:20458.036337
     acc:0.408433
 lu:
     loss:14161.059786
     acc:0.898469
 lv:
     loss:17394.014614
     acc:0.229617
 le:
     loss:1519.198299
     acc:0.999380
 encoder:
     loss:0.128732
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18785.608984
     acc:0.558733
 tv:
     loss:19527.824316
     acc:0.398806
 lu:
     loss:13503.357959
     acc:0.894964
 lv:
     loss:16760.686230
     acc:0.187012
 le:
     loss:1448.779706
     acc:0.999087
 encoder:
     loss:0.119479
----------------------------


Epoch: [173/1000]:
train:
----------------------------
 tu:
     loss:19713.228924
     acc:0.559780
 tv:
     loss:20445.175168
     acc:0.410803
 lu:
     loss:14160.428734
     acc:0.898602
 lv:
     loss:17391.364882
     acc:0.230222
 le:
     loss:1519.177145
     acc:0.999372
 encoder:
     loss:0.110135
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18785.373926
     acc:0.558531
 tv:
     loss:19526.625879
     acc:0.398934
 lu:
     loss:13501.999219
     acc:0.894914
 lv:
     loss:16760.851172
     acc:0.185783
 le:
     loss:1448.512927
     acc:0.999149
 encoder:
     loss:0.080640
----------------------------


Epoch: [174/1000]:
train:
----------------------------
 tu:
     loss:19695.540811
     acc:0.563809
 tv:
     loss:20442.945483
     acc:0.411364
 lu:
     loss:14159.516238
     acc:0.898763
 lv:
     loss:17390.157999
     acc:0.230604
 le:
     loss:1519.081046
     acc:0.999388
 encoder:
     loss:0.076607
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18750.921387
     acc:0.566132
 tv:
     loss:19524.306543
     acc:0.399301
 lu:
     loss:13502.367090
     acc:0.894679
 lv:
     loss:16761.793945
     acc:0.186390
 le:
     loss:1448.869489
     acc:0.999045
 encoder:
     loss:0.061599
----------------------------


Epoch: [175/1000]:
train:
----------------------------
 tu:
     loss:19680.080544
     acc:0.567084
 tv:
     loss:20439.860749
     acc:0.411827
 lu:
     loss:14160.947561
     acc:0.898398
 lv:
     loss:17388.164187
     acc:0.230989
 le:
     loss:1519.152298
     acc:0.999377
 encoder:
     loss:0.091269
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18750.334961
     acc:0.566244
 tv:
     loss:19522.164453
     acc:0.399866
 lu:
     loss:13501.708252
     acc:0.895039
 lv:
     loss:16760.260840
     acc:0.186914
 le:
     loss:1448.773608
     acc:0.999004
 encoder:
     loss:0.070539
----------------------------


Epoch: [176/1000]:
train:
----------------------------
 tu:
     loss:19678.886140
     acc:0.567031
 tv:
     loss:20438.763445
     acc:0.411981
 lu:
     loss:14160.541106
     acc:0.898469
 lv:
     loss:17387.210676
     acc:0.231130
 le:
     loss:1519.170799
     acc:0.999388
 encoder:
     loss:0.078870
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18749.138477
     acc:0.566280
 tv:
     loss:19520.378223
     acc:0.400398
 lu:
     loss:13501.601416
     acc:0.895105
 lv:
     loss:16760.476172
     acc:0.186614
 le:
     loss:1448.741870
     acc:0.999104
 encoder:
     loss:0.089743
----------------------------


Epoch: [177/1000]:
train:
----------------------------
 tu:
     loss:19668.368323
     acc:0.569456
 tv:
     loss:20436.661542
     acc:0.412526
 lu:
     loss:14159.938579
     acc:0.898615
 lv:
     loss:17384.596373
     acc:0.231991
 le:
     loss:1519.097949
     acc:0.999394
 encoder:
     loss:0.087294
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18680.370605
     acc:0.582417
 tv:
     loss:19518.590625
     acc:0.400556
 lu:
     loss:13502.936719
     acc:0.894749
 lv:
     loss:16761.246777
     acc:0.186382
 le:
     loss:1448.428461
     acc:0.999209
 encoder:
     loss:0.058632
----------------------------


Epoch: [178/1000]:
train:
----------------------------
 tu:
     loss:19613.593239
     acc:0.581713
 tv:
     loss:20428.671682
     acc:0.414110
 lu:
     loss:14160.595567
     acc:0.898635
 lv:
     loss:17380.598179
     acc:0.232707
 le:
     loss:1519.165127
     acc:0.999369
 encoder:
     loss:0.064076
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18669.320508
     acc:0.583705
 tv:
     loss:19504.749219
     acc:0.403924
 lu:
     loss:13500.702783
     acc:0.895191
 lv:
     loss:16759.092090
     acc:0.186003
 le:
     loss:1448.747101
     acc:0.999103
 encoder:
     loss:0.072016
----------------------------


Epoch: [179/1000]:
train:
----------------------------
 tu:
     loss:19602.609375
     acc:0.583545
 tv:
     loss:20422.587209
     acc:0.415427
 lu:
     loss:14159.248603
     acc:0.898935
 lv:
     loss:17381.177700
     acc:0.232555
 le:
     loss:1519.026005
     acc:0.999401
 encoder:
     loss:0.068704
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18668.316699
     acc:0.584006
 tv:
     loss:19503.654297
     acc:0.403817
 lu:
     loss:13500.494531
     acc:0.895377
 lv:
     loss:16758.453613
     acc:0.187066
 le:
     loss:1448.781055
     acc:0.999045
 encoder:
     loss:0.101260
----------------------------


Epoch: [180/1000]:
train:
----------------------------
 tu:
     loss:19598.544059
     acc:0.584021
 tv:
     loss:20420.869072
     acc:0.415716
 lu:
     loss:14158.871616
     acc:0.899122
 lv:
     loss:17379.019270
     acc:0.233199
 le:
     loss:1519.132453
     acc:0.999386
 encoder:
     loss:0.072291
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18667.407520
     acc:0.584071
 tv:
     loss:19506.644531
     acc:0.403483
 lu:
     loss:13501.900732
     acc:0.895004
 lv:
     loss:16753.756152
     acc:0.188352
 le:
     loss:1448.973596
     acc:0.999041
 encoder:
     loss:0.088518
----------------------------


Epoch: [181/1000]:
train:
----------------------------
 tu:
     loss:19597.358955
     acc:0.584368
 tv:
     loss:20418.683230
     acc:0.415763
 lu:
     loss:14157.899130
     acc:0.898899
 lv:
     loss:17378.254122
     acc:0.232941
 le:
     loss:1519.038544
     acc:0.999403
 encoder:
     loss:0.086175
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18667.193555
     acc:0.584116
 tv:
     loss:19503.599023
     acc:0.403645
 lu:
     loss:13501.344531
     acc:0.894871
 lv:
     loss:16761.293945
     acc:0.186260
 le:
     loss:1448.613849
     acc:0.999129
 encoder:
     loss:0.061044
----------------------------


Epoch: [182/1000]:
train:
----------------------------
 tu:
     loss:19580.484818
     acc:0.588241
 tv:
     loss:20418.236919
     acc:0.416123
 lu:
     loss:14158.522699
     acc:0.898688
 lv:
     loss:17377.180925
     acc:0.233162
 le:
     loss:1518.964719
     acc:0.999422
 encoder:
     loss:0.064639
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18626.464746
     acc:0.593087
 tv:
     loss:19504.292383
     acc:0.403594
 lu:
     loss:13501.700879
     acc:0.894912
 lv:
     loss:16756.594629
     acc:0.187762
 le:
     loss:1448.556488
     acc:0.999166
 encoder:
     loss:0.055268
----------------------------


Epoch: [183/1000]:
train:
----------------------------
 tu:
     loss:19559.868221
     acc:0.592499
 tv:
     loss:20416.979719
     acc:0.416203
 lu:
     loss:14157.989985
     acc:0.898963
 lv:
     loss:17373.330407
     acc:0.233968
 le:
     loss:1519.108492
     acc:0.999394
 encoder:
     loss:0.075813
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18625.238086
     acc:0.593210
 tv:
     loss:19506.025488
     acc:0.403363
 lu:
     loss:13501.179688
     acc:0.895018
 lv:
     loss:16758.061523
     acc:0.186726
 le:
     loss:1448.622205
     acc:0.999144
 encoder:
     loss:0.076198
----------------------------


Epoch: [184/1000]:
train:
----------------------------
 tu:
     loss:19548.672193
     acc:0.594681
 tv:
     loss:20413.453602
     acc:0.416806
 lu:
     loss:14157.974382
     acc:0.899090
 lv:
     loss:17370.945846
     acc:0.234737
 le:
     loss:1518.995894
     acc:0.999403
 encoder:
     loss:0.160490
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18589.200781
     acc:0.601138
 tv:
     loss:19501.640234
     acc:0.404242
 lu:
     loss:13500.311963
     acc:0.895265
 lv:
     loss:16757.767480
     acc:0.187111
 le:
     loss:1448.545972
     acc:0.999167
 encoder:
     loss:0.076359
----------------------------


Epoch: [185/1000]:
train:
----------------------------
 tu:
     loss:19520.183764
     acc:0.600467
 tv:
     loss:20413.569154
     acc:0.416781
 lu:
     loss:14157.041595
     acc:0.899273
 lv:
     loss:17371.800202
     acc:0.234351
 le:
     loss:1518.942591
     acc:0.999432
 encoder:
     loss:0.077103
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18587.317480
     acc:0.601357
 tv:
     loss:19502.607520
     acc:0.403871
 lu:
     loss:13497.767627
     acc:0.895879
 lv:
     loss:16756.620801
     acc:0.186653
 le:
     loss:1448.645044
     acc:0.999146
 encoder:
     loss:0.060683
----------------------------


Epoch: [186/1000]:
train:
----------------------------
 tu:
     loss:19518.499523
     acc:0.600731
 tv:
     loss:20412.851415
     acc:0.417088
 lu:
     loss:14153.484102
     acc:0.900068
 lv:
     loss:17370.879031
     acc:0.234460
 le:
     loss:1519.130625
     acc:0.999389
 encoder:
     loss:0.075576
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18587.632715
     acc:0.601403
 tv:
     loss:19501.202148
     acc:0.404338
 lu:
     loss:13485.887354
     acc:0.898485
 lv:
     loss:16756.000293
     acc:0.187554
 le:
     loss:1448.881702
     acc:0.999045
 encoder:
     loss:0.081888
----------------------------


Epoch: [187/1000]:
train:
----------------------------
 tu:
     loss:19517.088084
     acc:0.600916
 tv:
     loss:20410.238929
     acc:0.417562
 lu:
     loss:14140.539585
     acc:0.902989
 lv:
     loss:17370.926156
     acc:0.234412
 le:
     loss:1518.949856
     acc:0.999425
 encoder:
     loss:0.092116
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18586.572461
     acc:0.601673
 tv:
     loss:19499.970313
     acc:0.404374
 lu:
     loss:13481.698389
     acc:0.899470
 lv:
     loss:16757.206055
     acc:0.186240
 le:
     loss:1448.618823
     acc:0.999105
 encoder:
     loss:0.129937
----------------------------


Epoch: [188/1000]:
train:
----------------------------
 tu:
     loss:19515.141420
     acc:0.601360
 tv:
     loss:20410.224167
     acc:0.417326
 lu:
     loss:14136.112078
     acc:0.903759
 lv:
     loss:17367.201808
     acc:0.235252
 le:
     loss:1519.052066
     acc:0.999398
 encoder:
     loss:0.100645
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18587.162207
     acc:0.601358
 tv:
     loss:19503.216602
     acc:0.403864
 lu:
     loss:13478.749268
     acc:0.900243
 lv:
     loss:16754.978711
     acc:0.187979
 le:
     loss:1448.943158
     acc:0.999040
 encoder:
     loss:0.070270
----------------------------


Epoch: [189/1000]:
train:
----------------------------
 tu:
     loss:19514.741597
     acc:0.601278
 tv:
     loss:20410.441213
     acc:0.417335
 lu:
     loss:14131.617006
     acc:0.904881
 lv:
     loss:17363.442837
     acc:0.236212
 le:
     loss:1518.941254
     acc:0.999417
 encoder:
     loss:0.119436
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18586.355859
     acc:0.601445
 tv:
     loss:19501.096484
     acc:0.404342
 lu:
     loss:13477.609961
     acc:0.900169
 lv:
     loss:16755.276270
     acc:0.187484
 le:
     loss:1448.894568
     acc:0.999044
 encoder:
     loss:0.164788
----------------------------


Epoch: [190/1000]:
train:
----------------------------
 tu:
     loss:19510.292094
     acc:0.602237
 tv:
     loss:20406.619118
     acc:0.418026
 lu:
     loss:14131.549680
     acc:0.904689
 lv:
     loss:17363.431357
     acc:0.236211
 le:
     loss:1519.058737
     acc:0.999386
 encoder:
     loss:0.128310
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18554.519336
     acc:0.608821
 tv:
     loss:19504.860547
     acc:0.403338
 lu:
     loss:13474.778516
     acc:0.900805
 lv:
     loss:16756.132910
     acc:0.187399
 le:
     loss:1448.697864
     acc:0.999088
 encoder:
     loss:0.067871
----------------------------


Epoch: [191/1000]:
train:
----------------------------
 tu:
     loss:19480.544967
     acc:0.608576
 tv:
     loss:20407.222815
     acc:0.418066
 lu:
     loss:14129.752044
     acc:0.905275
 lv:
     loss:17359.968330
     acc:0.236926
 le:
     loss:1519.108779
     acc:0.999381
 encoder:
     loss:0.079196
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18551.033301
     acc:0.609371
 tv:
     loss:19501.936816
     acc:0.404182
 lu:
     loss:13471.468262
     acc:0.901858
 lv:
     loss:16759.108105
     acc:0.186402
 le:
     loss:1448.722968
     acc:0.999125
 encoder:
     loss:0.077857
----------------------------


Epoch: [192/1000]:
train:
----------------------------
 tu:
     loss:19477.879792
     acc:0.609034
 tv:
     loss:20406.467149
     acc:0.418067
 lu:
     loss:14128.208099
     acc:0.905303
 lv:
     loss:17359.218886
     acc:0.236812
 le:
     loss:1519.055670
     acc:0.999400
 encoder:
     loss:0.092495
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18551.220313
     acc:0.609258
 tv:
     loss:19503.986816
     acc:0.403776
 lu:
     loss:13472.041162
     acc:0.901563
 lv:
     loss:16755.763281
     acc:0.186845
 le:
     loss:1449.007166
     acc:0.999022
 encoder:
     loss:0.113527
----------------------------


Epoch: [193/1000]:
train:
----------------------------
 tu:
     loss:19477.233342
     acc:0.608950
 tv:
     loss:20405.656795
     acc:0.418260
 lu:
     loss:14127.481093
     acc:0.905522
 lv:
     loss:17356.429619
     acc:0.237635
 le:
     loss:1518.952063
     acc:0.999418
 encoder:
     loss:0.085376
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18550.373730
     acc:0.609455
 tv:
     loss:19494.859082
     acc:0.405571
 lu:
     loss:13472.687012
     acc:0.901399
 lv:
     loss:16755.015430
     acc:0.187500
 le:
     loss:1449.194104
     acc:0.998962
 encoder:
     loss:0.080256
----------------------------


Epoch: [194/1000]:
train:
----------------------------
 tu:
     loss:19477.185172
     acc:0.609035
 tv:
     loss:20390.597588
     acc:0.421358
 lu:
     loss:14126.958348
     acc:0.905446
 lv:
     loss:17357.537280
     acc:0.237267
 le:
     loss:1519.081229
     acc:0.999389
 encoder:
     loss:0.073079
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18550.533496
     acc:0.609326
 tv:
     loss:19487.217676
     acc:0.407305
 lu:
     loss:13470.874902
     acc:0.901840
 lv:
     loss:16757.152539
     acc:0.187269
 le:
     loss:1448.631665
     acc:0.999108
 encoder:
     loss:0.078056
----------------------------


Epoch: [195/1000]:
train:
----------------------------
 tu:
     loss:19476.656659
     acc:0.609121
 tv:
     loss:20387.507994
     acc:0.422269
 lu:
     loss:14125.332281
     acc:0.905452
 lv:
     loss:17353.149346
     acc:0.238127
 le:
     loss:1519.048514
     acc:0.999397
 encoder:
     loss:0.119196
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18550.737793
     acc:0.609428
 tv:
     loss:19488.541406
     acc:0.406781
 lu:
     loss:13473.807910
     acc:0.901188
 lv:
     loss:16758.275488
     acc:0.186615
 le:
     loss:1449.054108
     acc:0.999026
 encoder:
     loss:0.114527
----------------------------


Epoch: [196/1000]:
train:
----------------------------
 tu:
     loss:19459.441895
     acc:0.612710
 tv:
     loss:20387.395258
     acc:0.422042
 lu:
     loss:14125.740802
     acc:0.905824
 lv:
     loss:17355.291413
     acc:0.237938
 le:
     loss:1519.047989
     acc:0.999399
 encoder:
     loss:0.113414
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18513.956836
     acc:0.617515
 tv:
     loss:19485.379297
     acc:0.407569
 lu:
     loss:13471.015430
     acc:0.901778
 lv:
     loss:16757.724805
     acc:0.186854
 le:
     loss:1448.754810
     acc:0.999086
 encoder:
     loss:0.117414
----------------------------


Epoch: [197/1000]:
train:
----------------------------
 tu:
     loss:19440.733921
     acc:0.616519
 tv:
     loss:20386.370799
     acc:0.422402
 lu:
     loss:14124.690305
     acc:0.905945
 lv:
     loss:17351.419808
     acc:0.238783
 le:
     loss:1518.907827
     acc:0.999433
 encoder:
     loss:0.088007
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18514.267090
     acc:0.617305
 tv:
     loss:19486.146973
     acc:0.407021
 lu:
     loss:13469.993164
     acc:0.901921
 lv:
     loss:16755.846777
     acc:0.187579
 le:
     loss:1448.949390
     acc:0.999045
 encoder:
     loss:0.068964
----------------------------


Epoch: [198/1000]:
train:
----------------------------
 tu:
     loss:19419.419740
     acc:0.621167
 tv:
     loss:20374.354912
     acc:0.424933
 lu:
     loss:14122.651333
     acc:0.906422
 lv:
     loss:17350.986589
     acc:0.238607
 le:
     loss:1518.922857
     acc:0.999425
 encoder:
     loss:0.087502
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18478.449414
     acc:0.625192
 tv:
     loss:19456.400000
     acc:0.414097
 lu:
     loss:13468.018506
     acc:0.902233
 lv:
     loss:16755.888477
     acc:0.187743
 le:
     loss:1448.932434
     acc:0.999025
 encoder:
     loss:0.088628
----------------------------


Epoch: [199/1000]:
train:
----------------------------
 tu:
     loss:19405.165016
     acc:0.623956
 tv:
     loss:20359.063897
     acc:0.428193
 lu:
     loss:14122.827455
     acc:0.906434
 lv:
     loss:17348.155682
     acc:0.239643
 le:
     loss:1518.965514
     acc:0.999428
 encoder:
     loss:0.076193
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18478.539844
     acc:0.625073
 tv:
     loss:19456.332715
     acc:0.413962
 lu:
     loss:13469.636523
     acc:0.901902
 lv:
     loss:16758.270020
     acc:0.187521
 le:
     loss:1448.665564
     acc:0.999107
 encoder:
     loss:0.067070
----------------------------


Epoch: [200/1000]:
train:
----------------------------
 tu:
     loss:19402.765966
     acc:0.624473
 tv:
     loss:20353.125329
     acc:0.429430
 lu:
     loss:14120.897972
     acc:0.906817
 lv:
     loss:17346.822061
     acc:0.239848
 le:
     loss:1518.906673
     acc:0.999415
 encoder:
     loss:0.075072
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18477.995508
     acc:0.625168
 tv:
     loss:19455.460254
     acc:0.414294
 lu:
     loss:13467.591162
     acc:0.902428
 lv:
     loss:16755.308594
     acc:0.187800
 le:
     loss:1448.563208
     acc:0.999129
 encoder:
     loss:0.069016
----------------------------


Epoch: [201/1000]:
train:
----------------------------
 tu:
     loss:19401.921409
     acc:0.624479
 tv:
     loss:20350.572447
     acc:0.430056
 lu:
     loss:14122.664948
     acc:0.906574
 lv:
     loss:17347.416458
     acc:0.239773
 le:
     loss:1518.876060
     acc:0.999436
 encoder:
     loss:0.075981
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18478.131055
     acc:0.625213
 tv:
     loss:19453.799707
     acc:0.414023
 lu:
     loss:13466.592578
     acc:0.902605
 lv:
     loss:16754.091211
     acc:0.188498
 le:
     loss:1448.484967
     acc:0.999167
 encoder:
     loss:0.108371
----------------------------


Epoch: [202/1000]:
train:
----------------------------
 tu:
     loss:19401.215480
     acc:0.624739
 tv:
     loss:20342.128009
     acc:0.431634
 lu:
     loss:14121.463538
     acc:0.906447
 lv:
     loss:17343.773108
     acc:0.240066
 le:
     loss:1519.005373
     acc:0.999401
 encoder:
     loss:0.083473
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18465.859668
     acc:0.627951
 tv:
     loss:19434.348047
     acc:0.418995
 lu:
     loss:13466.472021
     acc:0.902578
 lv:
     loss:16753.137207
     acc:0.187935
 le:
     loss:1448.640588
     acc:0.999105
 encoder:
     loss:0.065258
----------------------------


Epoch: [203/1000]:
train:
----------------------------
 tu:
     loss:19372.519077
     acc:0.630809
 tv:
     loss:20336.139274
     acc:0.433001
 lu:
     loss:14120.594647
     acc:0.906692
 lv:
     loss:17343.286258
     acc:0.240173
 le:
     loss:1518.909253
     acc:0.999425
 encoder:
     loss:0.074952
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18447.240918
     acc:0.631955
 tv:
     loss:19436.102637
     acc:0.418173
 lu:
     loss:13467.064502
     acc:0.902469
 lv:
     loss:16753.772461
     acc:0.188053
 le:
     loss:1449.138867
     acc:0.999004
 encoder:
     loss:0.076052
----------------------------


Epoch: [204/1000]:
train:
----------------------------
 tu:
     loss:19368.245730
     acc:0.631534
 tv:
     loss:20332.127748
     acc:0.433546
 lu:
     loss:14119.080055
     acc:0.907040
 lv:
     loss:17344.476381
     acc:0.239768
 le:
     loss:1518.978675
     acc:0.999421
 encoder:
     loss:0.102912
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18446.161816
     acc:0.632223
 tv:
     loss:19428.172363
     acc:0.419950
 lu:
     loss:13466.424219
     acc:0.902465
 lv:
     loss:16757.040430
     acc:0.187029
 le:
     loss:1448.520642
     acc:0.999166
 encoder:
     loss:0.125176
----------------------------


Epoch: [205/1000]:
train:
----------------------------
 tu:
     loss:19367.777560
     acc:0.631558
 tv:
     loss:20322.165516
     acc:0.435905
 lu:
     loss:14119.587868
     acc:0.906859
 lv:
     loss:17338.294524
     acc:0.241379
 le:
     loss:1518.849863
     acc:0.999441
 encoder:
     loss:0.166804
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18447.027344
     acc:0.631923
 tv:
     loss:19423.251074
     acc:0.421104
 lu:
     loss:13467.188281
     acc:0.902448
 lv:
     loss:16755.281445
     acc:0.187976
 le:
     loss:1448.587976
     acc:0.999145
 encoder:
     loss:0.101903
----------------------------


Epoch: [206/1000]:
train:
----------------------------
 tu:
     loss:19366.768225
     acc:0.631761
 tv:
     loss:20318.144679
     acc:0.436829
 lu:
     loss:14118.398914
     acc:0.907310
 lv:
     loss:17340.978027
     acc:0.240598
 le:
     loss:1518.983275
     acc:0.999412
 encoder:
     loss:0.094317
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18446.636426
     acc:0.631886
 tv:
     loss:19421.665430
     acc:0.421352
 lu:
     loss:13466.286670
     acc:0.902459
 lv:
     loss:16757.066406
     acc:0.186860
 le:
     loss:1448.474561
     acc:0.999128
 encoder:
     loss:0.084359
----------------------------


Epoch: [207/1000]:
train:
----------------------------
 tu:
     loss:19351.723519
     acc:0.635358
 tv:
     loss:20310.490484
     acc:0.438375
 lu:
     loss:14118.134039
     acc:0.907157
 lv:
     loss:17337.282817
     acc:0.241584
 le:
     loss:1518.944154
     acc:0.999428
 encoder:
     loss:0.078710
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18405.597266
     acc:0.641316
 tv:
     loss:19406.381055
     acc:0.424984
 lu:
     loss:13465.334277
     acc:0.902806
 lv:
     loss:16752.914160
     acc:0.188227
 le:
     loss:1448.820245
     acc:0.999044
 encoder:
     loss:0.067036
----------------------------


Epoch: [208/1000]:
train:
----------------------------
 tu:
     loss:19307.653309
     acc:0.645121
 tv:
     loss:20301.356309
     acc:0.440298
 lu:
     loss:14117.927428
     acc:0.907152
 lv:
     loss:17339.464492
     acc:0.240963
 le:
     loss:1518.860259
     acc:0.999436
 encoder:
     loss:0.083063
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18373.603223
     acc:0.648080
 tv:
     loss:19407.720898
     acc:0.424552
 lu:
     loss:13465.106836
     acc:0.902833
 lv:
     loss:16756.624805
     acc:0.187809
 le:
     loss:1449.064691
     acc:0.998962
 encoder:
     loss:0.125482
----------------------------


Epoch: [209/1000]:
train:
----------------------------
 tu:
     loss:19297.415005
     acc:0.646896
 tv:
     loss:20299.689010
     acc:0.440665
 lu:
     loss:14117.810899
     acc:0.907304
 lv:
     loss:17335.510356
     acc:0.241757
 le:
     loss:1518.849616
     acc:0.999434
 encoder:
     loss:0.095748
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18372.540625
     acc:0.648343
 tv:
     loss:19404.565039
     acc:0.424858
 lu:
     loss:13466.651953
     acc:0.902520
 lv:
     loss:16753.102930
     acc:0.188730
 le:
     loss:1448.730402
     acc:0.999085
 encoder:
     loss:0.087241
----------------------------


Epoch: [210/1000]:
train:
----------------------------
 tu:
     loss:19295.017760
     acc:0.647201
 tv:
     loss:20298.146984
     acc:0.440847
 lu:
     loss:14117.971453
     acc:0.907222
 lv:
     loss:17334.517056
     acc:0.242104
 le:
     loss:1518.879201
     acc:0.999429
 encoder:
     loss:0.095804
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18371.862695
     acc:0.648327
 tv:
     loss:19409.734570
     acc:0.423710
 lu:
     loss:13466.261426
     acc:0.902577
 lv:
     loss:16754.900879
     acc:0.187073
 le:
     loss:1449.306879
     acc:0.998962
 encoder:
     loss:0.099930
----------------------------


Epoch: [211/1000]:
train:
----------------------------
 tu:
     loss:19293.594045
     acc:0.647402
 tv:
     loss:20296.097088
     acc:0.441233
 lu:
     loss:14115.302189
     acc:0.907763
 lv:
     loss:17334.222259
     acc:0.242181
 le:
     loss:1518.861771
     acc:0.999434
 encoder:
     loss:0.135154
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18371.009180
     acc:0.648638
 tv:
     loss:19405.368359
     acc:0.424854
 lu:
     loss:13464.910742
     acc:0.902601
 lv:
     loss:16757.608203
     acc:0.186887
 le:
     loss:1449.169019
     acc:0.999004
 encoder:
     loss:0.196693
----------------------------


Epoch: [212/1000]:
train:
----------------------------
 tu:
     loss:19292.680301
     acc:0.647306
 tv:
     loss:20296.064135
     acc:0.441219
 lu:
     loss:14116.124114
     acc:0.907670
 lv:
     loss:17331.931334
     acc:0.242928
 le:
     loss:1518.840054
     acc:0.999435
 encoder:
     loss:0.098154
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18370.933789
     acc:0.648671
 tv:
     loss:19402.705859
     acc:0.425717
 lu:
     loss:13465.204053
     acc:0.902711
 lv:
     loss:16752.091992
     acc:0.188248
 le:
     loss:1449.128101
     acc:0.999004
 encoder:
     loss:0.054563
----------------------------


Epoch: [213/1000]:
train:
----------------------------
 tu:
     loss:19290.985272
     acc:0.647566
 tv:
     loss:20289.957531
     acc:0.442853
 lu:
     loss:14115.547113
     acc:0.907763
 lv:
     loss:17329.871185
     acc:0.243255
 le:
     loss:1518.786415
     acc:0.999454
 encoder:
     loss:0.076336
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18370.003711
     acc:0.648898
 tv:
     loss:19392.944629
     acc:0.427715
 lu:
     loss:13465.575391
     acc:0.902647
 lv:
     loss:16752.028223
     acc:0.188105
 le:
     loss:1448.911511
     acc:0.999046
 encoder:
     loss:0.099433
----------------------------


Epoch: [214/1000]:
train:
----------------------------
 tu:
     loss:19290.457111
     acc:0.647746
 tv:
     loss:20277.790425
     acc:0.445344
 lu:
     loss:14114.064782
     acc:0.908024
 lv:
     loss:17329.555233
     acc:0.243178
 le:
     loss:1518.785998
     acc:0.999455
 encoder:
     loss:0.138002
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18370.431641
     acc:0.648727
 tv:
     loss:19374.853223
     acc:0.431542
 lu:
     loss:13462.125830
     acc:0.903437
 lv:
     loss:16755.150098
     acc:0.187824
 le:
     loss:1448.586835
     acc:0.999127
 encoder:
     loss:0.106896
----------------------------


Epoch: [215/1000]:
train:
----------------------------
 tu:
     loss:19289.373740
     acc:0.647911
 tv:
     loss:20265.012968
     acc:0.448047
 lu:
     loss:14113.920864
     acc:0.907931
 lv:
     loss:17328.003884
     acc:0.243775
 le:
     loss:1518.869159
     acc:0.999437
 encoder:
     loss:0.101106
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18352.403516
     acc:0.652726
 tv:
     loss:19374.738770
     acc:0.431650
 lu:
     loss:13460.989355
     acc:0.903698
 lv:
     loss:16752.641895
     acc:0.188577
 le:
     loss:1448.883276
     acc:0.999065
 encoder:
     loss:0.122715
----------------------------


Epoch: [216/1000]:
train:
----------------------------
 tu:
     loss:19251.503895
     acc:0.656222
 tv:
     loss:20261.677871
     acc:0.448617
 lu:
     loss:14109.761367
     acc:0.908937
 lv:
     loss:17325.885947
     acc:0.244022
 le:
     loss:1518.874756
     acc:0.999427
 encoder:
     loss:0.150271
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18322.043848
     acc:0.659257
 tv:
     loss:19372.558301
     acc:0.431950
 lu:
     loss:13460.132227
     acc:0.904078
 lv:
     loss:16755.022656
     acc:0.187773
 le:
     loss:1449.151434
     acc:0.999045
 encoder:
     loss:0.131452
----------------------------


Epoch: [217/1000]:
train:
----------------------------
 tu:
     loss:19243.882699
     acc:0.657411
 tv:
     loss:20249.933355
     acc:0.451414
 lu:
     loss:14108.353130
     acc:0.909287
 lv:
     loss:17325.678393
     acc:0.244039
 le:
     loss:1518.924326
     acc:0.999421
 encoder:
     loss:0.090533
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18321.503906
     acc:0.659214
 tv:
     loss:19338.894629
     acc:0.439610
 lu:
     loss:13456.891553
     acc:0.904622
 lv:
     loss:16753.578809
     acc:0.188069
 le:
     loss:1448.882770
     acc:0.999068
 encoder:
     loss:0.079890
----------------------------


Epoch: [218/1000]:
train:
----------------------------
 tu:
     loss:19243.482547
     acc:0.657554
 tv:
     loss:20233.303643
     acc:0.454945
 lu:
     loss:14108.466422
     acc:0.909196
 lv:
     loss:17325.007154
     acc:0.243917
 le:
     loss:1518.874273
     acc:0.999422
 encoder:
     loss:0.094266
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18322.049609
     acc:0.659016
 tv:
     loss:19339.571777
     acc:0.439423
 lu:
     loss:13458.510645
     acc:0.904184
 lv:
     loss:16752.602930
     acc:0.188241
 le:
     loss:1448.766528
     acc:0.999087
 encoder:
     loss:0.127867
----------------------------


Epoch: [219/1000]:
train:
----------------------------
 tu:
     loss:19241.456463
     acc:0.657854
 tv:
     loss:20228.773188
     acc:0.455718
 lu:
     loss:14105.372752
     acc:0.909921
 lv:
     loss:17325.475734
     acc:0.244224
 le:
     loss:1518.946679
     acc:0.999418
 encoder:
     loss:0.099971
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18320.937793
     acc:0.659320
 tv:
     loss:19336.761816
     acc:0.440082
 lu:
     loss:13455.323340
     acc:0.904886
 lv:
     loss:16756.246289
     acc:0.187020
 le:
     loss:1448.517798
     acc:0.999147
 encoder:
     loss:0.100109
----------------------------


Epoch: [220/1000]:
train:
----------------------------
 tu:
     loss:19241.761662
     acc:0.657697
 tv:
     loss:20228.019395
     acc:0.455590
 lu:
     loss:14106.120197
     acc:0.909656
 lv:
     loss:17323.064078
     acc:0.244459
 le:
     loss:1518.677766
     acc:0.999481
 encoder:
     loss:0.100973
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18321.792871
     acc:0.659219
 tv:
     loss:19335.086230
     acc:0.440050
 lu:
     loss:13455.630469
     acc:0.905069
 lv:
     loss:16754.674121
     acc:0.187571
 le:
     loss:1449.017126
     acc:0.999004
 encoder:
     loss:0.084325
----------------------------


Epoch: [221/1000]:
train:
----------------------------
 tu:
     loss:19241.386764
     acc:0.657741
 tv:
     loss:20223.589549
     acc:0.456576
 lu:
     loss:14104.106968
     acc:0.910128
 lv:
     loss:17319.072947
     acc:0.245323
 le:
     loss:1518.786833
     acc:0.999456
 encoder:
     loss:0.111866
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18322.020508
     acc:0.659214
 tv:
     loss:19336.192578
     acc:0.440052
 lu:
     loss:13453.043213
     acc:0.905770
 lv:
     loss:16756.357715
     acc:0.186922
 le:
     loss:1448.605884
     acc:0.999128
 encoder:
     loss:0.274273
----------------------------


Epoch: [222/1000]:
train:
----------------------------
 tu:
     loss:19240.754622
     acc:0.657815
 tv:
     loss:20223.300452
     acc:0.456561
 lu:
     loss:14103.102187
     acc:0.910294
 lv:
     loss:17318.691815
     acc:0.245176
 le:
     loss:1518.728050
     acc:0.999461
 encoder:
     loss:0.220371
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18321.093848
     acc:0.659336
 tv:
     loss:19336.441406
     acc:0.439739
 lu:
     loss:13455.512402
     acc:0.904949
 lv:
     loss:16755.638184
     acc:0.187690
 le:
     loss:1448.745892
     acc:0.999088
 encoder:
     loss:0.218435
----------------------------


Epoch: [223/1000]:
train:
----------------------------
 tu:
     loss:19230.011480
     acc:0.660164
 tv:
     loss:20221.599098
     acc:0.457041
 lu:
     loss:14104.291288
     acc:0.910254
 lv:
     loss:17319.259697
     acc:0.245601
 le:
     loss:1518.840802
     acc:0.999438
 encoder:
     loss:0.102327
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18286.245117
     acc:0.666777
 tv:
     loss:19335.144141
     acc:0.440310
 lu:
     loss:13453.773047
     acc:0.905468
 lv:
     loss:16748.478223
     acc:0.189085
 le:
     loss:1448.810284
     acc:0.999085
 encoder:
     loss:0.070246
----------------------------


Epoch: [224/1000]:
train:
----------------------------
 tu:
     loss:19205.942837
     acc:0.665134
 tv:
     loss:20212.321119
     acc:0.458746
 lu:
     loss:14103.360692
     acc:0.910289
 lv:
     loss:17316.443632
     acc:0.245967
 le:
     loss:1518.852716
     acc:0.999440
 encoder:
     loss:0.080139
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18286.914844
     acc:0.666567
 tv:
     loss:19319.527539
     acc:0.443893
 lu:
     loss:13454.515967
     acc:0.905424
 lv:
     loss:16753.253320
     acc:0.188261
 le:
     loss:1448.755676
     acc:0.999087
 encoder:
     loss:0.084366
----------------------------


Epoch: [225/1000]:
train:
----------------------------
 tu:
     loss:19204.680108
     acc:0.665369
 tv:
     loss:20204.089912
     acc:0.460418
 lu:
     loss:14102.360295
     acc:0.910392
 lv:
     loss:17314.723167
     acc:0.246256
 le:
     loss:1518.805810
     acc:0.999447
 encoder:
     loss:0.077200
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18286.300293
     acc:0.666693
 tv:
     loss:19322.540527
     acc:0.443210
 lu:
     loss:13452.726562
     acc:0.905644
 lv:
     loss:16753.140625
     acc:0.187567
 le:
     loss:1448.638818
     acc:0.999107
 encoder:
     loss:0.068335
----------------------------


Epoch: [226/1000]:
train:
----------------------------
 tu:
     loss:19203.977675
     acc:0.665448
 tv:
     loss:20201.164562
     acc:0.461116
 lu:
     loss:14101.975370
     acc:0.910562
 lv:
     loss:17317.588254
     acc:0.245938
 le:
     loss:1518.686826
     acc:0.999466
 encoder:
     loss:0.087324
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18286.250879
     acc:0.666814
 tv:
     loss:19304.432617
     acc:0.447178
 lu:
     loss:13454.903418
     acc:0.905346
 lv:
     loss:16752.140625
     acc:0.188277
 le:
     loss:1448.625018
     acc:0.999129
 encoder:
     loss:0.080014
----------------------------


Epoch: [227/1000]:
train:
----------------------------
 tu:
     loss:19203.215003
     acc:0.665462
 tv:
     loss:20185.882381
     acc:0.464680
 lu:
     loss:14102.116427
     acc:0.910465
 lv:
     loss:17312.552871
     acc:0.246921
 le:
     loss:1518.634045
     acc:0.999492
 encoder:
     loss:0.095221
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18286.499609
     acc:0.666634
 tv:
     loss:19303.387402
     acc:0.447164
 lu:
     loss:13451.964062
     acc:0.905969
 lv:
     loss:16753.027344
     acc:0.188038
 le:
     loss:1448.875519
     acc:0.999045
 encoder:
     loss:0.083015
----------------------------


Epoch: [228/1000]:
train:
----------------------------
 tu:
     loss:19202.772257
     acc:0.665511
 tv:
     loss:20182.281136
     acc:0.465262
 lu:
     loss:14101.588969
     acc:0.910481
 lv:
     loss:17310.801054
     acc:0.247154
 le:
     loss:1518.632245
     acc:0.999496
 encoder:
     loss:0.303632
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18285.953906
     acc:0.666864
 tv:
     loss:19304.167969
     acc:0.446596
 lu:
     loss:13456.096924
     acc:0.904833
 lv:
     loss:16752.565430
     acc:0.188148
 le:
     loss:1448.980859
     acc:0.999003
 encoder:
     loss:0.252749
----------------------------


Epoch: [229/1000]:
train:
----------------------------
 tu:
     loss:19202.677303
     acc:0.665668
 tv:
     loss:20181.994731
     acc:0.465134
 lu:
     loss:14100.951467
     acc:0.910617
 lv:
     loss:17312.876351
     acc:0.246352
 le:
     loss:1518.692648
     acc:0.999478
 encoder:
     loss:0.142039
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18285.442871
     acc:0.666926
 tv:
     loss:19303.301465
     acc:0.446871
 lu:
     loss:13454.117578
     acc:0.905213
 lv:
     loss:16752.651367
     acc:0.187538
 le:
     loss:1448.840808
     acc:0.999087
 encoder:
     loss:0.095001
----------------------------


Epoch: [230/1000]:
train:
----------------------------
 tu:
     loss:19202.185433
     acc:0.665603
 tv:
     loss:20181.317451
     acc:0.465427
 lu:
     loss:14100.866620
     acc:0.910819
 lv:
     loss:17310.835415
     acc:0.247391
 le:
     loss:1518.703661
     acc:0.999461
 encoder:
     loss:0.148776
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18285.717773
     acc:0.666982
 tv:
     loss:19305.622949
     acc:0.446808
 lu:
     loss:13456.116943
     acc:0.904991
 lv:
     loss:16757.392383
     acc:0.186732
 le:
     loss:1449.058643
     acc:0.999022
 encoder:
     loss:0.108362
----------------------------


Epoch: [231/1000]:
train:
----------------------------
 tu:
     loss:19193.663279
     acc:0.667616
 tv:
     loss:20180.305948
     acc:0.465438
 lu:
     loss:14100.906136
     acc:0.910666
 lv:
     loss:17310.834064
     acc:0.247059
 le:
     loss:1518.785341
     acc:0.999449
 encoder:
     loss:0.189248
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18246.225977
     acc:0.675544
 tv:
     loss:19303.316211
     acc:0.447277
 lu:
     loss:13453.213770
     acc:0.905554
 lv:
     loss:16755.681348
     acc:0.187487
 le:
     loss:1449.005237
     acc:0.999037
 encoder:
     loss:0.373364
----------------------------


Epoch: [232/1000]:
train:
----------------------------
 tu:
     loss:19140.479140
     acc:0.679169
 tv:
     loss:20178.400322
     acc:0.465665
 lu:
     loss:14100.334484
     acc:0.910563
 lv:
     loss:17307.399369
     acc:0.247944
 le:
     loss:1518.751830
     acc:0.999448
 encoder:
     loss:0.235628
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18216.552832
     acc:0.682041
 tv:
     loss:19305.332813
     acc:0.446506
 lu:
     loss:13452.196729
     acc:0.905708
 lv:
     loss:16754.524316
     acc:0.187951
 le:
     loss:1448.604578
     acc:0.999148
 encoder:
     loss:0.103584
----------------------------


Epoch: [233/1000]:
train:
----------------------------
 tu:
     loss:19132.099473
     acc:0.680687
 tv:
     loss:20176.116677
     acc:0.466294
 lu:
     loss:14100.647427
     acc:0.910732
 lv:
     loss:17305.703466
     acc:0.248224
 le:
     loss:1518.886615
     acc:0.999434
 encoder:
     loss:0.119856
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18217.455566
     acc:0.681728
 tv:
     loss:19291.423145
     acc:0.449853
 lu:
     loss:13452.681445
     acc:0.905678
 lv:
     loss:16751.809277
     acc:0.188012
 le:
     loss:1448.937463
     acc:0.999024
 encoder:
     loss:0.116666
----------------------------


Epoch: [234/1000]:
train:
----------------------------
 tu:
     loss:19130.483580
     acc:0.680784
 tv:
     loss:20163.907874
     acc:0.468904
 lu:
     loss:14099.900890
     acc:0.910783
 lv:
     loss:17305.252203
     acc:0.248353
 le:
     loss:1518.637173
     acc:0.999479
 encoder:
     loss:0.089650
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18216.914453
     acc:0.682050
 tv:
     loss:19285.964062
     acc:0.451198
 lu:
     loss:13453.217969
     acc:0.905460
 lv:
     loss:16749.815527
     acc:0.188510
 le:
     loss:1448.706738
     acc:0.999066
 encoder:
     loss:0.067162
----------------------------


Epoch: [235/1000]:
train:
----------------------------
 tu:
     loss:19129.699741
     acc:0.680966
 tv:
     loss:20159.143135
     acc:0.469666
 lu:
     loss:14098.720544
     acc:0.910972
 lv:
     loss:17302.704908
     acc:0.248883
 le:
     loss:1518.718107
     acc:0.999467
 encoder:
     loss:0.075613
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18215.672949
     acc:0.682262
 tv:
     loss:19289.478027
     acc:0.449684
 lu:
     loss:13453.071924
     acc:0.905567
 lv:
     loss:16753.872461
     acc:0.188343
 le:
     loss:1448.697070
     acc:0.999107
 encoder:
     loss:0.074912
----------------------------


Epoch: [236/1000]:
train:
----------------------------
 tu:
     loss:19128.900288
     acc:0.681144
 tv:
     loss:20159.931436
     acc:0.469451
 lu:
     loss:14098.830510
     acc:0.911053
 lv:
     loss:17301.288245
     acc:0.248689
 le:
     loss:1518.618234
     acc:0.999491
 encoder:
     loss:0.093350
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18216.504980
     acc:0.681956
 tv:
     loss:19282.725293
     acc:0.452101
 lu:
     loss:13453.643896
     acc:0.905560
 lv:
     loss:16751.145703
     acc:0.188949
 le:
     loss:1448.888898
     acc:0.999066
 encoder:
     loss:0.073234
----------------------------


Epoch: [237/1000]:
train:
----------------------------
 tu:
     loss:19125.972827
     acc:0.681478
 tv:
     loss:20150.795342
     acc:0.471848
 lu:
     loss:14098.769066
     acc:0.911075
 lv:
     loss:17300.517907
     acc:0.249121
 le:
     loss:1518.639688
     acc:0.999489
 encoder:
     loss:0.136740
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18184.648438
     acc:0.688949
 tv:
     loss:19279.441016
     acc:0.452536
 lu:
     loss:13453.503418
     acc:0.905110
 lv:
     loss:16754.115234
     acc:0.187990
 le:
     loss:1449.391150
     acc:0.998903
 encoder:
     loss:0.140677
----------------------------


Epoch: [238/1000]:
train:
----------------------------
 tu:
     loss:19071.225166
     acc:0.693426
 tv:
     loss:20146.645417
     acc:0.472683
 lu:
     loss:14098.585506
     acc:0.911071
 lv:
     loss:17301.084075
     acc:0.249481
 le:
     loss:1518.809624
     acc:0.999445
 encoder:
     loss:0.108159
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18149.172461
     acc:0.696533
 tv:
     loss:19275.070898
     acc:0.453197
 lu:
     loss:13454.255469
     acc:0.905030
 lv:
     loss:16752.509961
     acc:0.188374
 le:
     loss:1448.784222
     acc:0.999099
 encoder:
     loss:0.086648
----------------------------


Epoch: [239/1000]:
train:
----------------------------
 tu:
     loss:19057.668934
     acc:0.696035
 tv:
     loss:20143.568041
     acc:0.473020
 lu:
     loss:14097.150039
     acc:0.911269
 lv:
     loss:17299.886969
     acc:0.249291
 le:
     loss:1518.757161
     acc:0.999462
 encoder:
     loss:0.156970
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18148.329297
     acc:0.696813
 tv:
     loss:19277.096387
     acc:0.453076
 lu:
     loss:13453.250488
     acc:0.905396
 lv:
     loss:16752.580371
     acc:0.188199
 le:
     loss:1448.662982
     acc:0.999161
 encoder:
     loss:0.135064
----------------------------


Epoch: [240/1000]:
train:
----------------------------
 tu:
     loss:19055.374398
     acc:0.696431
 tv:
     loss:20140.089753
     acc:0.473869
 lu:
     loss:14096.757404
     acc:0.911487
 lv:
     loss:17296.579420
     acc:0.249995
 le:
     loss:1518.612009
     acc:0.999483
 encoder:
     loss:0.093939
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18148.148633
     acc:0.696859
 tv:
     loss:19275.406934
     acc:0.453192
 lu:
     loss:13452.070752
     acc:0.905695
 lv:
     loss:16749.976758
     acc:0.189195
 le:
     loss:1449.373987
     acc:0.998975
 encoder:
     loss:0.088856
----------------------------


Epoch: [241/1000]:
train:
----------------------------
 tu:
     loss:19055.496128
     acc:0.696355
 tv:
     loss:20141.247979
     acc:0.473240
 lu:
     loss:14098.446936
     acc:0.911104
 lv:
     loss:17300.081759
     acc:0.249552
 le:
     loss:1518.699077
     acc:0.999470
 encoder:
     loss:0.127341
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18148.060059
     acc:0.696800
 tv:
     loss:19273.135254
     acc:0.453764
 lu:
     loss:13452.454395
     acc:0.905873
 lv:
     loss:16754.349023
     acc:0.187396
 le:
     loss:1448.639832
     acc:0.999128
 encoder:
     loss:0.130845
----------------------------


Epoch: [242/1000]:
train:
----------------------------
 tu:
     loss:19053.914142
     acc:0.696615
 tv:
     loss:20142.625602
     acc:0.473342
 lu:
     loss:14098.372752
     acc:0.911174
 lv:
     loss:17295.783351
     acc:0.250256
 le:
     loss:1518.816261
     acc:0.999449
 encoder:
     loss:0.178284
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18147.849414
     acc:0.697020
 tv:
     loss:19272.256934
     acc:0.453719
 lu:
     loss:13452.498145
     acc:0.905714
 lv:
     loss:16749.865332
     acc:0.188751
 le:
     loss:1448.681171
     acc:0.999102
 encoder:
     loss:0.129978
----------------------------


Epoch: [243/1000]:
train:
----------------------------
 tu:
     loss:19049.325650
     acc:0.697649
 tv:
     loss:20129.852800
     acc:0.475918
 lu:
     loss:14097.203909
     acc:0.911336
 lv:
     loss:17294.833950
     acc:0.250192
 le:
     loss:1518.710767
     acc:0.999472
 encoder:
     loss:0.112432
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18115.206348
     acc:0.704315
 tv:
     loss:19240.958105
     acc:0.460593
 lu:
     loss:13451.205078
     acc:0.906037
 lv:
     loss:16752.114648
     acc:0.188283
 le:
     loss:1448.754932
     acc:0.999086
 encoder:
     loss:0.124612
----------------------------


Epoch: [244/1000]:
train:
----------------------------
 tu:
     loss:19013.476756
     acc:0.705764
 tv:
     loss:20108.847599
     acc:0.480509
 lu:
     loss:14096.852789
     acc:0.911409
 lv:
     loss:17296.894304
     acc:0.249849
 le:
     loss:1518.720678
     acc:0.999459
 encoder:
     loss:0.093201
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18066.040723
     acc:0.714882
 tv:
     loss:19234.727832
     acc:0.462026
 lu:
     loss:13452.296387
     acc:0.905881
 lv:
     loss:16749.141895
     acc:0.188702
 le:
     loss:1449.315564
     acc:0.998980
 encoder:
     loss:0.116218
----------------------------


Epoch: [245/1000]:
train:
----------------------------
 tu:
     loss:18946.369549
     acc:0.719834
 tv:
     loss:20104.008062
     acc:0.481386
 lu:
     loss:14096.155887
     acc:0.911508
 lv:
     loss:17294.149823
     acc:0.250497
 le:
     loss:1518.648483
     acc:0.999480
 encoder:
     loss:0.100580
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18025.222168
     acc:0.723390
 tv:
     loss:19237.568652
     acc:0.461285
 lu:
     loss:13452.167578
     acc:0.906038
 lv:
     loss:16750.958691
     acc:0.189245
 le:
     loss:1448.725909
     acc:0.999105
 encoder:
     loss:0.091470
----------------------------


Epoch: [246/1000]:
train:
----------------------------
 tu:
     loss:18931.970056
     acc:0.722490
 tv:
     loss:20104.675577
     acc:0.481212
 lu:
     loss:14098.015057
     acc:0.911030
 lv:
     loss:17291.544206
     acc:0.251371
 le:
     loss:1518.704170
     acc:0.999460
 encoder:
     loss:0.092964
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18024.224219
     acc:0.723618
 tv:
     loss:19241.795703
     acc:0.460411
 lu:
     loss:13453.043359
     acc:0.905531
 lv:
     loss:16752.647852
     acc:0.187639
 le:
     loss:1449.396753
     acc:0.998957
 encoder:
     loss:0.089739
----------------------------


Epoch: [247/1000]:
train:
----------------------------
 tu:
     loss:18929.185990
     acc:0.722880
 tv:
     loss:20102.013649
     acc:0.481414
 lu:
     loss:14096.373433
     acc:0.911416
 lv:
     loss:17291.250738
     acc:0.250990
 le:
     loss:1518.712201
     acc:0.999467
 encoder:
     loss:0.096223
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18023.934863
     acc:0.723614
 tv:
     loss:19238.303711
     acc:0.461302
 lu:
     loss:13450.947070
     acc:0.906032
 lv:
     loss:16756.091016
     acc:0.187584
 le:
     loss:1448.937256
     acc:0.999047
 encoder:
     loss:0.096941
----------------------------


Epoch: [248/1000]:
train:
----------------------------
 tu:
     loss:18928.113599
     acc:0.722819
 tv:
     loss:20101.320619
     acc:0.481758
 lu:
     loss:14095.769111
     acc:0.911532
 lv:
     loss:17288.823254
     acc:0.251529
 le:
     loss:1518.617554
     acc:0.999475
 encoder:
     loss:0.123568
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18023.437207
     acc:0.723803
 tv:
     loss:19233.081152
     acc:0.462633
 lu:
     loss:13450.130322
     acc:0.906375
 lv:
     loss:16749.286914
     acc:0.188968
 le:
     loss:1449.337793
     acc:0.998918
 encoder:
     loss:0.119450
----------------------------


Epoch: [249/1000]:
train:
----------------------------
 tu:
     loss:18927.661473
     acc:0.723108
 tv:
     loss:20098.958712
     acc:0.482306
 lu:
     loss:14095.052314
     acc:0.911757
 lv:
     loss:17286.760799
     acc:0.252248
 le:
     loss:1518.581427
     acc:0.999504
 encoder:
     loss:0.099638
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18023.079492
     acc:0.723826
 tv:
     loss:19238.856543
     acc:0.460912
 lu:
     loss:13443.735254
     acc:0.907772
 lv:
     loss:16747.578809
     acc:0.189567
 le:
     loss:1448.681604
     acc:0.999108
 encoder:
     loss:0.082101
----------------------------


Epoch: [250/1000]:
train:
----------------------------
 tu:
     loss:18927.356627
     acc:0.723007
 tv:
     loss:20100.572788
     acc:0.481613
 lu:
     loss:14090.857047
     acc:0.912780
 lv:
     loss:17286.148120
     acc:0.252301
 le:
     loss:1518.700600
     acc:0.999474
 encoder:
     loss:0.107787
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18023.860352
     acc:0.723472
 tv:
     loss:19238.706641
     acc:0.460814
 lu:
     loss:13439.220117
     acc:0.908684
 lv:
     loss:16754.520898
     acc:0.187567
 le:
     loss:1449.100500
     acc:0.999005
 encoder:
     loss:0.104555
----------------------------


Epoch: [251/1000]:
train:
----------------------------
 tu:
     loss:18926.067178
     acc:0.723237
 tv:
     loss:20098.814748
     acc:0.482103
 lu:
     loss:14087.319745
     acc:0.913874
 lv:
     loss:17286.776004
     acc:0.251803
 le:
     loss:1518.567891
     acc:0.999500
 encoder:
     loss:0.098399
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18022.399902
     acc:0.723950
 tv:
     loss:19238.205957
     acc:0.461052
 lu:
     loss:13436.420020
     acc:0.909258
 lv:
     loss:16752.934766
     acc:0.187574
 le:
     loss:1448.916284
     acc:0.999026
 encoder:
     loss:0.110706
----------------------------


Epoch: [252/1000]:
train:
----------------------------
 tu:
     loss:18926.292991
     acc:0.723201
 tv:
     loss:20093.448254
     acc:0.483427
 lu:
     loss:14084.662666
     acc:0.914163
 lv:
     loss:17285.835131
     acc:0.251850
 le:
     loss:1518.637460
     acc:0.999481
 encoder:
     loss:0.132089
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:18023.295801
     acc:0.723639
 tv:
     loss:19226.637012
     acc:0.463924
 lu:
     loss:13435.325928
     acc:0.909582
 lv:
     loss:16750.476172
     acc:0.188082
 le:
     loss:1448.374927
     acc:0.999170
 encoder:
     loss:0.166407
----------------------------


Epoch: [253/1000]:
train:
----------------------------
 tu:
     loss:18897.894032
     acc:0.729386
 tv:
     loss:20084.998115
     acc:0.485125
 lu:
     loss:14082.251919
     acc:0.914793
 lv:
     loss:17282.488236
     acc:0.252978
 le:
     loss:1518.710854
     acc:0.999471
 encoder:
     loss:0.169215
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17950.389648
     acc:0.740006
 tv:
     loss:19224.447559
     acc:0.464396
 lu:
     loss:13433.939355
     acc:0.910228
 lv:
     loss:16743.609863
     acc:0.190049
 le:
     loss:1448.811127
     acc:0.999088
 encoder:
     loss:0.122555
----------------------------


Epoch: [254/1000]:
train:
----------------------------
 tu:
     loss:18855.015000
     acc:0.738768
 tv:
     loss:20082.949457
     acc:0.485515
 lu:
     loss:14081.522938
     acc:0.914744
 lv:
     loss:17284.281534
     acc:0.252557
 le:
     loss:1518.560419
     acc:0.999490
 encoder:
     loss:0.114268
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17946.217090
     acc:0.740686
 tv:
     loss:19221.618750
     acc:0.464705
 lu:
     loss:13433.585791
     acc:0.910042
 lv:
     loss:16756.138379
     acc:0.188074
 le:
     loss:1449.224048
     acc:0.998940
 encoder:
     loss:0.114506
----------------------------


Epoch: [255/1000]:
train:
----------------------------
 tu:
     loss:18850.163563
     acc:0.739309
 tv:
     loss:20080.670081
     acc:0.485818
 lu:
     loss:14078.723485
     acc:0.915514
 lv:
     loss:17282.313931
     acc:0.252870
 le:
     loss:1518.546236
     acc:0.999507
 encoder:
     loss:0.163350
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17945.696973
     acc:0.740644
 tv:
     loss:19221.728320
     acc:0.464914
 lu:
     loss:13431.029785
     acc:0.910670
 lv:
     loss:16754.161914
     acc:0.187486
 le:
     loss:1449.169214
     acc:0.998981
 encoder:
     loss:0.097291
----------------------------


Epoch: [256/1000]:
train:
----------------------------
 tu:
     loss:18849.628338
     acc:0.739375
 tv:
     loss:20079.807981
     acc:0.486059
 lu:
     loss:14078.402242
     acc:0.915428
 lv:
     loss:17280.970749
     acc:0.253457
 le:
     loss:1518.588941
     acc:0.999484
 encoder:
     loss:0.270724
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17945.259180
     acc:0.740866
 tv:
     loss:19221.336426
     acc:0.465030
 lu:
     loss:13430.537402
     acc:0.910590
 lv:
     loss:16747.245605
     acc:0.189524
 le:
     loss:1449.158545
     acc:0.998984
 encoder:
     loss:0.152287
----------------------------


Epoch: [257/1000]:
train:
----------------------------
 tu:
     loss:18847.762639
     acc:0.739577
 tv:
     loss:20080.180528
     acc:0.485983
 lu:
     loss:14078.284021
     acc:0.915531
 lv:
     loss:17282.043786
     acc:0.253152
 le:
     loss:1518.476423
     acc:0.999519
 encoder:
     loss:0.104739
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17944.713867
     acc:0.740852
 tv:
     loss:19221.524902
     acc:0.465031
 lu:
     loss:13431.415527
     acc:0.910328
 lv:
     loss:16749.108496
     acc:0.188589
 le:
     loss:1449.126025
     acc:0.998981
 encoder:
     loss:0.084436
----------------------------


Epoch: [258/1000]:
train:
----------------------------
 tu:
     loss:18821.170251
     acc:0.745356
 tv:
     loss:20076.070142
     acc:0.486704
 lu:
     loss:14077.014592
     acc:0.915676
 lv:
     loss:17280.885538
     acc:0.253108
 le:
     loss:1518.678360
     acc:0.999475
 encoder:
     loss:0.096079
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17908.773926
     acc:0.748671
 tv:
     loss:19222.432324
     acc:0.464683
 lu:
     loss:13431.859424
     acc:0.910182
 lv:
     loss:16754.793945
     acc:0.187679
 le:
     loss:1448.576965
     acc:0.999107
 encoder:
     loss:0.117633
----------------------------


Epoch: [259/1000]:
train:
----------------------------
 tu:
     loss:18809.953148
     acc:0.747492
 tv:
     loss:20076.447822
     acc:0.486945
 lu:
     loss:14076.561149
     acc:0.915837
 lv:
     loss:17279.738815
     acc:0.253962
 le:
     loss:1518.542053
     acc:0.999507
 encoder:
     loss:0.161750
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17908.931641
     acc:0.748561
 tv:
     loss:19221.008301
     acc:0.465159
 lu:
     loss:13429.995801
     acc:0.910730
 lv:
     loss:16744.708984
     acc:0.190098
 le:
     loss:1448.564142
     acc:0.999128
 encoder:
     loss:0.296163
----------------------------


Epoch: [260/1000]:
train:
----------------------------
 tu:
     loss:18808.096964
     acc:0.747730
 tv:
     loss:20067.656432
     acc:0.488836
 lu:
     loss:14075.336312
     acc:0.915907
 lv:
     loss:17278.664891
     acc:0.253597
 le:
     loss:1518.589652
     acc:0.999487
 encoder:
     loss:0.278271
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17908.524512
     acc:0.748655
 tv:
     loss:19206.747754
     acc:0.468144
 lu:
     loss:13429.906689
     acc:0.910474
 lv:
     loss:16746.991699
     acc:0.188934
 le:
     loss:1448.675641
     acc:0.999129
 encoder:
     loss:0.220110
----------------------------


Epoch: [261/1000]:
train:
----------------------------
 tu:
     loss:18784.739882
     acc:0.752954
 tv:
     loss:20062.764637
     acc:0.489777
 lu:
     loss:14075.992630
     acc:0.915707
 lv:
     loss:17277.199900
     acc:0.253851
 le:
     loss:1518.591794
     acc:0.999481
 encoder:
     loss:0.139253
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17878.616895
     acc:0.755329
 tv:
     loss:19207.145410
     acc:0.468147
 lu:
     loss:13430.517236
     acc:0.910518
 lv:
     loss:16744.690137
     acc:0.189508
 le:
     loss:1448.925183
     acc:0.999046
 encoder:
     loss:0.150196
----------------------------


Epoch: [262/1000]:
train:
----------------------------
 tu:
     loss:18773.254554
     acc:0.755139
 tv:
     loss:20054.075298
     acc:0.491637
 lu:
     loss:14075.351971
     acc:0.916053
 lv:
     loss:17273.848122
     acc:0.254838
 le:
     loss:1518.532723
     acc:0.999516
 encoder:
     loss:0.306523
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17878.110547
     acc:0.755372
 tv:
     loss:19189.388867
     acc:0.472503
 lu:
     loss:13428.873535
     acc:0.910902
 lv:
     loss:16748.373828
     acc:0.188551
 le:
     loss:1448.739014
     acc:0.999066
 encoder:
     loss:0.254013
----------------------------


Epoch: [263/1000]:
train:
----------------------------
 tu:
     loss:18772.413279
     acc:0.755206
 tv:
     loss:20043.905966
     acc:0.494164
 lu:
     loss:14075.165050
     acc:0.916055
 lv:
     loss:17277.306130
     acc:0.254063
 le:
     loss:1518.580329
     acc:0.999495
 encoder:
     loss:0.182369
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17877.974902
     acc:0.755499
 tv:
     loss:19191.394629
     acc:0.471208
 lu:
     loss:13430.722363
     acc:0.910623
 lv:
     loss:16751.797656
     acc:0.188111
 le:
     loss:1449.475885
     acc:0.998916
 encoder:
     loss:0.109268
----------------------------


Epoch: [264/1000]:
train:
----------------------------
 tu:
     loss:18772.297511
     acc:0.755175
 tv:
     loss:20043.463595
     acc:0.493717
 lu:
     loss:14075.251147
     acc:0.916021
 lv:
     loss:17275.956498
     acc:0.254219
 le:
     loss:1518.607331
     acc:0.999476
 encoder:
     loss:0.099001
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17877.334180
     acc:0.755700
 tv:
     loss:19192.992871
     acc:0.471307
 lu:
     loss:13427.752100
     acc:0.911245
 lv:
     loss:16747.865918
     acc:0.188901
 le:
     loss:1448.722778
     acc:0.999128
 encoder:
     loss:0.084170
----------------------------


Epoch: [265/1000]:
train:
----------------------------
 tu:
     loss:18771.089231
     acc:0.755342
 tv:
     loss:20040.883437
     acc:0.494345
 lu:
     loss:14073.361464
     acc:0.916466
 lv:
     loss:17274.626624
     acc:0.254373
 le:
     loss:1518.534838
     acc:0.999510
 encoder:
     loss:0.135786
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17877.723438
     acc:0.755476
 tv:
     loss:19187.962305
     acc:0.471964
 lu:
     loss:13429.713135
     acc:0.910835
 lv:
     loss:16747.647266
     acc:0.189232
 le:
     loss:1449.187671
     acc:0.998986
 encoder:
     loss:0.112918
----------------------------


Epoch: [266/1000]:
train:
----------------------------
 tu:
     loss:18767.731320
     acc:0.755913
 tv:
     loss:20039.271337
     acc:0.494783
 lu:
     loss:14074.358580
     acc:0.916122
 lv:
     loss:17271.596884
     acc:0.255500
 le:
     loss:1518.505726
     acc:0.999510
 encoder:
     loss:0.121822
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17857.613184
     acc:0.759969
 tv:
     loss:19189.729785
     acc:0.471743
 lu:
     loss:13430.750146
     acc:0.910486
 lv:
     loss:16744.624805
     acc:0.190016
 le:
     loss:1448.811053
     acc:0.999125
 encoder:
     loss:0.105699
----------------------------


Epoch: [267/1000]:
train:
----------------------------
 tu:
     loss:18741.269191
     acc:0.762060
 tv:
     loss:20041.087811
     acc:0.493927
 lu:
     loss:14073.823015
     acc:0.916145
 lv:
     loss:17273.316361
     acc:0.254808
 le:
     loss:1518.467275
     acc:0.999522
 encoder:
     loss:0.105622
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17840.815039
     acc:0.763602
 tv:
     loss:19189.305176
     acc:0.471587
 lu:
     loss:13430.407324
     acc:0.910349
 lv:
     loss:16750.740039
     acc:0.188224
 le:
     loss:1448.910370
     acc:0.999045
 encoder:
     loss:0.140644
----------------------------


Epoch: [268/1000]:
train:
----------------------------
 tu:
     loss:18734.474325
     acc:0.763293
 tv:
     loss:20038.893384
     acc:0.494488
 lu:
     loss:14073.401390
     acc:0.916336
 lv:
     loss:17270.628418
     acc:0.255689
 le:
     loss:1518.637873
     acc:0.999473
 encoder:
     loss:0.147960
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17840.278613
     acc:0.763585
 tv:
     loss:19186.161035
     acc:0.472719
 lu:
     loss:13428.387842
     acc:0.910851
 lv:
     loss:16740.569824
     acc:0.190395
 le:
     loss:1449.186224
     acc:0.999002
 encoder:
     loss:0.148908
----------------------------


Epoch: [269/1000]:
train:
----------------------------
 tu:
     loss:18733.943655
     acc:0.763133
 tv:
     loss:20036.819029
     acc:0.494864
 lu:
     loss:14071.449332
     acc:0.916727
 lv:
     loss:17272.958314
     acc:0.254629
 le:
     loss:1518.534887
     acc:0.999505
 encoder:
     loss:0.158985
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17840.413477
     acc:0.763581
 tv:
     loss:19188.329785
     acc:0.471837
 lu:
     loss:13428.820459
     acc:0.910833
 lv:
     loss:16746.965430
     acc:0.188835
 le:
     loss:1448.735101
     acc:0.999125
 encoder:
     loss:0.147124
----------------------------


Epoch: [270/1000]:
train:
----------------------------
 tu:
     loss:18731.681731
     acc:0.763503
 tv:
     loss:20039.225382
     acc:0.494423
 lu:
     loss:14071.740109
     acc:0.916730
 lv:
     loss:17267.375273
     acc:0.256056
 le:
     loss:1518.639323
     acc:0.999470
 encoder:
     loss:0.146227
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17839.634961
     acc:0.763744
 tv:
     loss:19189.535742
     acc:0.471910
 lu:
     loss:13426.980029
     acc:0.911407
 lv:
     loss:16748.778711
     acc:0.188232
 le:
     loss:1448.518774
     acc:0.999129
 encoder:
     loss:0.135958
----------------------------


Epoch: [271/1000]:
train:
----------------------------
 tu:
     loss:18731.507869
     acc:0.763665
 tv:
     loss:20034.924157
     acc:0.495122
 lu:
     loss:14072.014603
     acc:0.916501
 lv:
     loss:17267.985181
     acc:0.255704
 le:
     loss:1518.710082
     acc:0.999461
 encoder:
     loss:0.136821
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17839.794824
     acc:0.763721
 tv:
     loss:19187.593066
     acc:0.472007
 lu:
     loss:13428.988916
     acc:0.911058
 lv:
     loss:16751.419531
     acc:0.187821
 le:
     loss:1448.856238
     acc:0.999067
 encoder:
     loss:0.104287
----------------------------


Epoch: [272/1000]:
train:
----------------------------
 tu:
     loss:18731.134800
     acc:0.763701
 tv:
     loss:20033.167935
     acc:0.495585
 lu:
     loss:14071.464333
     acc:0.916704
 lv:
     loss:17265.385027
     acc:0.256685
 le:
     loss:1518.547254
     acc:0.999497
 encoder:
     loss:0.116239
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17839.946582
     acc:0.763744
 tv:
     loss:19180.296094
     acc:0.474175
 lu:
     loss:13428.992480
     acc:0.910852
 lv:
     loss:16749.418457
     acc:0.189096
 le:
     loss:1448.895264
     acc:0.999042
 encoder:
     loss:0.128064
----------------------------


Epoch: [273/1000]:
train:
----------------------------
 tu:
     loss:18730.946323
     acc:0.763521
 tv:
     loss:20022.862850
     acc:0.498125
 lu:
     loss:14072.381382
     acc:0.916544
 lv:
     loss:17264.678631
     acc:0.256674
 le:
     loss:1518.437053
     acc:0.999523
 encoder:
     loss:0.258105
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17840.072852
     acc:0.763662
 tv:
     loss:19175.486914
     acc:0.474442
 lu:
     loss:13426.785791
     acc:0.911257
 lv:
     loss:16747.096094
     acc:0.188739
 le:
     loss:1448.662640
     acc:0.999149
 encoder:
     loss:0.260254
----------------------------


Epoch: [274/1000]:
train:
----------------------------
 tu:
     loss:18730.774096
     acc:0.763369
 tv:
     loss:20020.372229
     acc:0.498403
 lu:
     loss:14070.483171
     acc:0.916810
 lv:
     loss:17265.116177
     acc:0.256476
 le:
     loss:1518.481228
     acc:0.999528
 encoder:
     loss:0.153924
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17839.869824
     acc:0.763704
 tv:
     loss:19171.289355
     acc:0.475475
 lu:
     loss:13427.820898
     acc:0.911231
 lv:
     loss:16746.058008
     acc:0.189383
 le:
     loss:1448.964868
     acc:0.999025
 encoder:
     loss:0.085981
----------------------------


Epoch: [275/1000]:
train:
----------------------------
 tu:
     loss:18729.101869
     acc:0.763868
 tv:
     loss:20017.858171
     acc:0.498956
 lu:
     loss:14071.461505
     acc:0.916758
 lv:
     loss:17264.888649
     acc:0.256453
 le:
     loss:1518.727961
     acc:0.999460
 encoder:
     loss:0.132757
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17839.482617
     acc:0.763744
 tv:
     loss:19173.423242
     acc:0.475100
 lu:
     loss:13427.400586
     acc:0.911026
 lv:
     loss:16747.796289
     acc:0.189277
 le:
     loss:1448.966949
     acc:0.999026
 encoder:
     loss:0.112699
----------------------------


Epoch: [276/1000]:
train:
----------------------------
 tu:
     loss:18728.760856
     acc:0.764067
 tv:
     loss:20018.336380
     acc:0.498645
 lu:
     loss:14070.552893
     acc:0.916738
 lv:
     loss:17262.852914
     acc:0.256920
 le:
     loss:1518.438484
     acc:0.999530
 encoder:
     loss:0.156767
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17839.712793
     acc:0.763560
 tv:
     loss:19172.059473
     acc:0.475538
 lu:
     loss:13427.074121
     acc:0.911195
 lv:
     loss:16750.795020
     acc:0.188106
 le:
     loss:1449.057233
     acc:0.999025
 encoder:
     loss:0.105882
----------------------------


Epoch: [277/1000]:
train:
----------------------------
 tu:
     loss:18728.664040
     acc:0.763939
 tv:
     loss:20014.994038
     acc:0.499394
 lu:
     loss:14070.316929
     acc:0.916882
 lv:
     loss:17259.963833
     acc:0.257801
 le:
     loss:1518.512119
     acc:0.999511
 encoder:
     loss:0.132271
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17824.796484
     acc:0.767089
 tv:
     loss:19174.466602
     acc:0.475096
 lu:
     loss:13429.346680
     acc:0.910544
 lv:
     loss:16750.817480
     acc:0.188621
 le:
     loss:1448.664514
     acc:0.999108
 encoder:
     loss:0.094819
----------------------------


Epoch: [278/1000]:
train:
----------------------------
 tu:
     loss:18697.563431
     acc:0.770833
 tv:
     loss:20009.218727
     acc:0.500765
 lu:
     loss:14071.305448
     acc:0.916532
 lv:
     loss:17259.366938
     acc:0.257738
 le:
     loss:1518.507487
     acc:0.999507
 encoder:
     loss:0.124854
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17803.663672
     acc:0.771563
 tv:
     loss:19156.394531
     acc:0.478908
 lu:
     loss:13427.275732
     acc:0.911256
 lv:
     loss:16747.178613
     acc:0.189323
 le:
     loss:1448.601129
     acc:0.999106
 encoder:
     loss:0.127952
----------------------------


Epoch: [279/1000]:
train:
----------------------------
 tu:
     loss:18692.346021
     acc:0.771748
 tv:
     loss:19996.541038
     acc:0.503416
 lu:
     loss:14070.380462
     acc:0.916783
 lv:
     loss:17259.496014
     acc:0.257487
 le:
     loss:1518.518917
     acc:0.999513
 encoder:
     loss:0.092855
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17803.006543
     acc:0.771704
 tv:
     loss:19162.694336
     acc:0.476863
 lu:
     loss:13428.659912
     acc:0.910806
 lv:
     loss:16750.915625
     acc:0.188401
 le:
     loss:1448.816809
     acc:0.999066
 encoder:
     loss:0.080452
----------------------------


Epoch: [280/1000]:
train:
----------------------------
 tu:
     loss:18691.639410
     acc:0.771801
 tv:
     loss:19998.661133
     acc:0.502964
 lu:
     loss:14070.263854
     acc:0.916861
 lv:
     loss:17260.108342
     acc:0.257464
 le:
     loss:1518.454234
     acc:0.999516
 encoder:
     loss:0.115840
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17803.793555
     acc:0.771392
 tv:
     loss:19156.624219
     acc:0.478620
 lu:
     loss:13428.828418
     acc:0.910785
 lv:
     loss:16749.407617
     acc:0.188134
 le:
     loss:1449.251184
     acc:0.998962
 encoder:
     loss:0.101081
----------------------------


Epoch: [281/1000]:
train:
----------------------------
 tu:
     loss:18678.884187
     acc:0.774520
 tv:
     loss:19995.520712
     acc:0.503407
 lu:
     loss:14069.337754
     acc:0.916987
 lv:
     loss:17258.244356
     acc:0.257922
 le:
     loss:1518.435002
     acc:0.999543
 encoder:
     loss:0.118448
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17769.454395
     acc:0.779024
 tv:
     loss:19160.436816
     acc:0.477846
 lu:
     loss:13427.930615
     acc:0.911057
 lv:
     loss:16746.354102
     acc:0.189229
 le:
     loss:1448.673792
     acc:0.999129
 encoder:
     loss:0.101460
----------------------------


Epoch: [282/1000]:
train:
----------------------------
 tu:
     loss:18654.781477
     acc:0.779341
 tv:
     loss:19992.992676
     acc:0.503893
 lu:
     loss:14068.225132
     acc:0.917289
 lv:
     loss:17257.534168
     acc:0.258182
 le:
     loss:1518.521989
     acc:0.999507
 encoder:
     loss:0.100836
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17769.013867
     acc:0.779170
 tv:
     loss:19155.710254
     acc:0.478650
 lu:
     loss:13426.595410
     acc:0.911670
 lv:
     loss:16744.342383
     acc:0.190033
 le:
     loss:1448.450372
     acc:0.999170
 encoder:
     loss:0.097564
----------------------------


Epoch: [283/1000]:
train:
----------------------------
 tu:
     loss:18653.715514
     acc:0.779511
 tv:
     loss:19993.903423
     acc:0.503961
 lu:
     loss:14070.127135
     acc:0.916927
 lv:
     loss:17259.124580
     acc:0.257909
 le:
     loss:1518.477663
     acc:0.999518
 encoder:
     loss:0.144143
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17768.570605
     acc:0.779199
 tv:
     loss:19154.707520
     acc:0.478973
 lu:
     loss:13426.499219
     acc:0.911183
 lv:
     loss:16740.240039
     acc:0.191023
 le:
     loss:1448.934973
     acc:0.999104
 encoder:
     loss:0.110728
----------------------------


Epoch: [284/1000]:
train:
----------------------------
 tu:
     loss:18653.063760
     acc:0.779729
 tv:
     loss:19990.602494
     acc:0.504315
 lu:
     loss:14069.364019
     acc:0.917028
 lv:
     loss:17254.891988
     acc:0.258411
 le:
     loss:1518.463182
     acc:0.999509
 encoder:
     loss:0.259868
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17769.173730
     acc:0.778969
 tv:
     loss:19154.485938
     acc:0.479342
 lu:
     loss:13426.161328
     acc:0.911223
 lv:
     loss:16748.606445
     acc:0.188607
 le:
     loss:1449.270978
     acc:0.998998
 encoder:
     loss:0.152657
----------------------------


Epoch: [285/1000]:
train:
----------------------------
 tu:
     loss:18652.127123
     acc:0.779850
 tv:
     loss:19992.568064
     acc:0.503837
 lu:
     loss:14068.949911
     acc:0.917136
 lv:
     loss:17253.217705
     acc:0.259163
 le:
     loss:1518.544634
     acc:0.999500
 encoder:
     loss:0.310990
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17769.093945
     acc:0.779006
 tv:
     loss:19157.095508
     acc:0.478464
 lu:
     loss:13427.125049
     acc:0.911055
 lv:
     loss:16752.034082
     acc:0.188817
 le:
     loss:1448.864197
     acc:0.999004
 encoder:
     loss:0.278229
----------------------------


Epoch: [286/1000]:
train:
----------------------------
 tu:
     loss:18652.184479
     acc:0.779700
 tv:
     loss:19990.536019
     acc:0.504204
 lu:
     loss:14068.062500
     acc:0.917258
 lv:
     loss:17252.572822
     acc:0.259266
 le:
     loss:1518.452112
     acc:0.999526
 encoder:
     loss:0.173274
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17768.840332
     acc:0.778965
 tv:
     loss:19155.884961
     acc:0.479183
 lu:
     loss:13425.444238
     acc:0.911299
 lv:
     loss:16748.063477
     acc:0.189279
 le:
     loss:1449.302466
     acc:0.999002
 encoder:
     loss:0.121327
----------------------------


Epoch: [287/1000]:
train:
----------------------------
 tu:
     loss:18652.016738
     acc:0.779718
 tv:
     loss:19988.783612
     acc:0.504753
 lu:
     loss:14068.079533
     acc:0.917342
 lv:
     loss:17250.441338
     acc:0.259693
 le:
     loss:1518.408284
     acc:0.999528
 encoder:
     loss:0.132788
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17769.229102
     acc:0.778925
 tv:
     loss:19158.650781
     acc:0.478183
 lu:
     loss:13425.461377
     acc:0.911411
 lv:
     loss:16744.449219
     acc:0.190059
 le:
     loss:1448.911951
     acc:0.999062
 encoder:
     loss:0.166276
----------------------------


Epoch: [288/1000]:
train:
----------------------------
 tu:
     loss:18651.770372
     acc:0.779759
 tv:
     loss:19990.189112
     acc:0.504348
 lu:
     loss:14068.381802
     acc:0.917194
 lv:
     loss:17249.436228
     acc:0.259980
 le:
     loss:1518.507387
     acc:0.999505
 encoder:
     loss:0.152491
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17768.763672
     acc:0.778969
 tv:
     loss:19157.174707
     acc:0.478421
 lu:
     loss:13427.678174
     acc:0.911074
 lv:
     loss:16745.349316
     acc:0.189316
 le:
     loss:1449.125635
     acc:0.999024
 encoder:
     loss:0.190208
----------------------------


Epoch: [289/1000]:
train:
----------------------------
 tu:
     loss:18651.285542
     acc:0.779781
 tv:
     loss:19989.273528
     acc:0.504594
 lu:
     loss:14068.169127
     acc:0.917066
 lv:
     loss:17248.507245
     acc:0.260081
 le:
     loss:1518.399440
     acc:0.999536
 encoder:
     loss:0.187573
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17768.443555
     acc:0.779216
 tv:
     loss:19158.581836
     acc:0.478264
 lu:
     loss:13425.220068
     acc:0.911737
 lv:
     loss:16753.451562
     acc:0.187770
 le:
     loss:1449.315387
     acc:0.998920
 encoder:
     loss:0.152621
----------------------------


Epoch: [290/1000]:
train:
----------------------------
 tu:
     loss:18651.559196
     acc:0.779821
 tv:
     loss:19987.730389
     acc:0.505067
 lu:
     loss:14067.954817
     acc:0.917053
 lv:
     loss:17247.669729
     acc:0.260352
 le:
     loss:1518.412734
     acc:0.999545
 encoder:
     loss:0.150103
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17768.809570
     acc:0.778988
 tv:
     loss:19158.420605
     acc:0.478336
 lu:
     loss:13425.330127
     acc:0.911435
 lv:
     loss:16748.725391
     acc:0.188490
 le:
     loss:1448.978009
     acc:0.999067
 encoder:
     loss:0.171169
----------------------------


Epoch: [291/1000]:
train:
----------------------------
 tu:
     loss:18650.866608
     acc:0.779947
 tv:
     loss:19987.046466
     acc:0.505185
 lu:
     loss:14067.251635
     acc:0.917402
 lv:
     loss:17248.312330
     acc:0.259903
 le:
     loss:1518.517553
     acc:0.999514
 encoder:
     loss:0.140372
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17768.648730
     acc:0.778967
 tv:
     loss:19153.817578
     acc:0.479494
 lu:
     loss:13426.833984
     acc:0.911071
 lv:
     loss:16748.379395
     acc:0.188143
 le:
     loss:1448.543622
     acc:0.999149
 encoder:
     loss:0.094286
----------------------------


Epoch: [292/1000]:
train:
----------------------------
 tu:
     loss:18650.161224
     acc:0.780101
 tv:
     loss:19987.176690
     acc:0.505001
 lu:
     loss:14067.971566
     acc:0.917005
 lv:
     loss:17244.128543
     acc:0.260995
 le:
     loss:1518.428602
     acc:0.999529
 encoder:
     loss:0.094743
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17762.962207
     acc:0.780175
 tv:
     loss:19157.980176
     acc:0.478385
 lu:
     loss:13424.145752
     acc:0.911839
 lv:
     loss:16748.656641
     acc:0.188762
 le:
     loss:1448.721851
     acc:0.999106
 encoder:
     loss:0.088217
----------------------------


Epoch: [293/1000]:
train:
----------------------------
 tu:
     loss:18613.329544
     acc:0.788210
 tv:
     loss:19984.032840
     acc:0.505764
 lu:
     loss:14067.034906
     acc:0.917430
 lv:
     loss:17245.804540
     acc:0.260684
 le:
     loss:1518.456116
     acc:0.999523
 encoder:
     loss:0.107790
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17697.614355
     acc:0.794810
 tv:
     loss:19157.507617
     acc:0.478576
 lu:
     loss:13427.829834
     acc:0.910953
 lv:
     loss:16744.919141
     acc:0.189645
 le:
     loss:1448.728229
     acc:0.999108
 encoder:
     loss:0.096551
----------------------------


Epoch: [294/1000]:
train:
----------------------------
 tu:
     loss:18579.897143
     acc:0.795306
 tv:
     loss:19985.562511
     acc:0.505141
 lu:
     loss:14068.399130
     acc:0.916988
 lv:
     loss:17244.568575
     acc:0.260662
 le:
     loss:1518.473765
     acc:0.999513
 encoder:
     loss:0.085835
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17696.980469
     acc:0.794753
 tv:
     loss:19157.242285
     acc:0.478530
 lu:
     loss:13426.597021
     acc:0.911253
 lv:
     loss:16742.784082
     acc:0.189832
 le:
     loss:1448.806616
     acc:0.999066
 encoder:
     loss:0.098743
----------------------------


Epoch: [295/1000]:
train:
----------------------------
 tu:
     loss:18577.157329
     acc:0.795523
 tv:
     loss:19984.335551
     acc:0.505544
 lu:
     loss:14067.328477
     acc:0.917180
 lv:
     loss:17241.304517
     acc:0.261238
 le:
     loss:1518.415219
     acc:0.999541
 encoder:
     loss:0.105989
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17696.970020
     acc:0.794770
 tv:
     loss:19161.048047
     acc:0.477564
 lu:
     loss:13427.442139
     acc:0.910986
 lv:
     loss:16748.007715
     acc:0.189149
 le:
     loss:1448.363934
     acc:0.999208
 encoder:
     loss:0.096340
----------------------------


Epoch: [296/1000]:
train:
----------------------------
 tu:
     loss:18575.570540
     acc:0.795819
 tv:
     loss:19982.609046
     acc:0.505898
 lu:
     loss:14067.615813
     acc:0.917149
 lv:
     loss:17243.180335
     acc:0.261430
 le:
     loss:1518.308800
     acc:0.999550
 encoder:
     loss:0.102973
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17696.966797
     acc:0.794691
 tv:
     loss:19155.366992
     acc:0.479007
 lu:
     loss:13427.432617
     acc:0.911077
 lv:
     loss:16740.925391
     acc:0.190800
 le:
     loss:1448.698553
     acc:0.999124
 encoder:
     loss:0.130093
----------------------------


Epoch: [297/1000]:
train:
----------------------------
 tu:
     loss:18575.204215
     acc:0.795831
 tv:
     loss:19981.606389
     acc:0.506044
 lu:
     loss:14066.598701
     acc:0.917409
 lv:
     loss:17240.139978
     acc:0.261602
 le:
     loss:1518.409043
     acc:0.999525
 encoder:
     loss:0.116215
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17696.686523
     acc:0.794877
 tv:
     loss:19155.276074
     acc:0.479156
 lu:
     loss:13425.604004
     acc:0.911643
 lv:
     loss:16747.710352
     acc:0.188945
 le:
     loss:1449.312640
     acc:0.998982
 encoder:
     loss:0.090025
----------------------------


Epoch: [298/1000]:
train:
----------------------------
 tu:
     loss:18574.300123
     acc:0.795919
 tv:
     loss:19981.037768
     acc:0.506123
 lu:
     loss:14066.788676
     acc:0.917509
 lv:
     loss:17241.706554
     acc:0.261530
 le:
     loss:1518.348375
     acc:0.999552
 encoder:
     loss:0.107097
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17696.555566
     acc:0.794790
 tv:
     loss:19156.746680
     acc:0.478671
 lu:
     loss:13425.196924
     acc:0.911571
 lv:
     loss:16749.440820
     acc:0.188590
 le:
     loss:1448.594019
     acc:0.999108
 encoder:
     loss:0.146148
----------------------------


Epoch: [299/1000]:
train:
----------------------------
 tu:
     loss:18567.190452
     acc:0.797501
 tv:
     loss:19982.500681
     acc:0.505778
 lu:
     loss:14066.816372
     acc:0.917481
 lv:
     loss:17241.389308
     acc:0.261391
 le:
     loss:1518.425949
     acc:0.999530
 encoder:
     loss:0.139634
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17661.633496
     acc:0.802648
 tv:
     loss:19161.588086
     acc:0.477261
 lu:
     loss:13424.459912
     acc:0.911773
 lv:
     loss:16744.851367
     acc:0.189861
 le:
     loss:1448.742566
     acc:0.999087
 encoder:
     loss:0.106349
----------------------------


Epoch: [300/1000]:
train:
----------------------------
 tu:
     loss:18539.226971
     acc:0.803288
 tv:
     loss:19981.594318
     acc:0.505986
 lu:
     loss:14065.327353
     acc:0.917708
 lv:
     loss:17239.274516
     acc:0.261886
 le:
     loss:1518.414303
     acc:0.999519
 encoder:
     loss:0.123787
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17661.250977
     acc:0.802586
 tv:
     loss:19160.658105
     acc:0.477545
 lu:
     loss:13426.110156
     acc:0.911367
 lv:
     loss:16750.635840
     acc:0.188321
 le:
     loss:1448.979321
     acc:0.999024
 encoder:
     loss:0.106317
----------------------------


Epoch: [301/1000]:
train:
----------------------------
 tu:
     loss:18537.983239
     acc:0.803486
 tv:
     loss:19982.377952
     acc:0.505772
 lu:
     loss:14066.545138
     acc:0.917517
 lv:
     loss:17240.594534
     acc:0.261727
 le:
     loss:1518.471528
     acc:0.999520
 encoder:
     loss:0.137648
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17660.923535
     acc:0.802649
 tv:
     loss:19154.545312
     acc:0.479921
 lu:
     loss:13427.267578
     acc:0.910783
 lv:
     loss:16747.885547
     acc:0.189126
 le:
     loss:1448.700995
     acc:0.999087
 encoder:
     loss:0.271176
----------------------------


Epoch: [302/1000]:
train:
----------------------------
 tu:
     loss:18536.802121
     acc:0.803675
 tv:
     loss:19978.872922
     acc:0.506580
 lu:
     loss:14066.408362
     acc:0.917526
 lv:
     loss:17236.236646
     acc:0.262557
 le:
     loss:1518.383468
     acc:0.999545
 encoder:
     loss:0.175725
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17661.264648
     acc:0.802583
 tv:
     loss:19154.362988
     acc:0.479088
 lu:
     loss:13424.178955
     acc:0.911810
 lv:
     loss:16746.859766
     acc:0.189389
 le:
     loss:1448.745599
     acc:0.999108
 encoder:
     loss:0.103700
----------------------------


Epoch: [303/1000]:
train:
----------------------------
 tu:
     loss:18536.346157
     acc:0.803778
 tv:
     loss:19978.747547
     acc:0.506685
 lu:
     loss:14065.483648
     acc:0.917633
 lv:
     loss:17237.133585
     acc:0.262443
 le:
     loss:1518.447087
     acc:0.999518
 encoder:
     loss:0.294915
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17660.356348
     acc:0.802714
 tv:
     loss:19155.463867
     acc:0.478644
 lu:
     loss:13424.565674
     acc:0.911656
 lv:
     loss:16746.066895
     acc:0.189774
 le:
     loss:1448.890039
     acc:0.999027
 encoder:
     loss:0.246642
----------------------------


Epoch: [304/1000]:
train:
----------------------------
 tu:
     loss:18536.657931
     acc:0.803586
 tv:
     loss:19977.446936
     acc:0.506760
 lu:
     loss:14064.721646
     acc:0.917847
 lv:
     loss:17234.109046
     acc:0.263049
 le:
     loss:1518.459848
     acc:0.999516
 encoder:
     loss:0.160511
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17660.966895
     acc:0.802627
 tv:
     loss:19155.722266
     acc:0.478998
 lu:
     loss:13427.425244
     acc:0.910903
 lv:
     loss:16749.803906
     acc:0.187968
 le:
     loss:1448.699548
     acc:0.999108
 encoder:
     loss:0.139648
----------------------------


Epoch: [305/1000]:
train:
----------------------------
 tu:
     loss:18536.065475
     acc:0.803811
 tv:
     loss:19977.873615
     acc:0.506988
 lu:
     loss:14064.077750
     acc:0.918027
 lv:
     loss:17233.785758
     acc:0.263281
 le:
     loss:1518.389592
     acc:0.999540
 encoder:
     loss:0.110587
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17660.974609
     acc:0.802586
 tv:
     loss:19153.974414
     acc:0.479362
 lu:
     loss:13426.294629
     acc:0.911106
 lv:
     loss:16744.773730
     acc:0.188789
 le:
     loss:1449.015564
     acc:0.999047
 encoder:
     loss:0.095185
----------------------------


Epoch: [306/1000]:
train:
----------------------------
 tu:
     loss:18535.853277
     acc:0.803811
 tv:
     loss:19976.534441
     acc:0.506964
 lu:
     loss:14064.919661
     acc:0.917686
 lv:
     loss:17234.498592
     acc:0.262833
 le:
     loss:1518.421689
     acc:0.999533
 encoder:
     loss:0.092600
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17661.030371
     acc:0.802544
 tv:
     loss:19156.173730
     acc:0.478514
 lu:
     loss:13423.976562
     acc:0.911818
 lv:
     loss:16745.081934
     acc:0.189303
 le:
     loss:1448.966345
     acc:0.999046
 encoder:
     loss:0.094417
----------------------------


Epoch: [307/1000]:
train:
----------------------------
 tu:
     loss:18535.024051
     acc:0.803981
 tv:
     loss:19975.122627
     acc:0.507199
 lu:
     loss:14063.293911
     acc:0.918083
 lv:
     loss:17230.170410
     acc:0.263530
 le:
     loss:1518.351117
     acc:0.999543
 encoder:
     loss:0.121097
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17661.097168
     acc:0.802590
 tv:
     loss:19156.286230
     acc:0.478482
 lu:
     loss:13424.708545
     acc:0.911468
 lv:
     loss:16748.206445
     acc:0.188308
 le:
     loss:1449.166290
     acc:0.999004
 encoder:
     loss:0.091244
----------------------------


Epoch: [308/1000]:
train:
----------------------------
 tu:
     loss:18535.324662
     acc:0.803893
 tv:
     loss:19974.960960
     acc:0.507282
 lu:
     loss:14064.191792
     acc:0.918067
 lv:
     loss:17229.880201
     acc:0.263822
 le:
     loss:1518.353896
     acc:0.999547
 encoder:
     loss:0.099434
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17660.712500
     acc:0.802668
 tv:
     loss:19155.173340
     acc:0.479255
 lu:
     loss:13423.737012
     acc:0.911880
 lv:
     loss:16747.562598
     acc:0.189028
 le:
     loss:1448.683429
     acc:0.999128
 encoder:
     loss:0.197085
----------------------------


Epoch: [309/1000]:
train:
----------------------------
 tu:
     loss:18535.053586
     acc:0.803903
 tv:
     loss:19975.043321
     acc:0.507358
 lu:
     loss:14064.611294
     acc:0.917775
 lv:
     loss:17230.104992
     acc:0.263921
 le:
     loss:1518.359775
     acc:0.999545
 encoder:
     loss:0.174980
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17660.983594
     acc:0.802470
 tv:
     loss:19160.325488
     acc:0.477566
 lu:
     loss:13425.385986
     acc:0.911572
 lv:
     loss:16746.962500
     acc:0.189266
 le:
     loss:1448.863983
     acc:0.999067
 encoder:
     loss:0.148567
----------------------------


Epoch: [310/1000]:
train:
----------------------------
 tu:
     loss:18534.881030
     acc:0.803921
 tv:
     loss:19974.879474
     acc:0.507101
 lu:
     loss:14062.752805
     acc:0.918126
 lv:
     loss:17229.811660
     acc:0.263811
 le:
     loss:1518.369439
     acc:0.999547
 encoder:
     loss:0.130780
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17660.536523
     acc:0.802672
 tv:
     loss:19157.883301
     acc:0.478458
 lu:
     loss:13423.927832
     acc:0.911818
 lv:
     loss:16743.469336
     acc:0.189818
 le:
     loss:1448.543787
     acc:0.999150
 encoder:
     loss:0.084256
----------------------------


Epoch: [311/1000]:
train:
----------------------------
 tu:
     loss:18534.826308
     acc:0.803781
 tv:
     loss:19974.316781
     acc:0.507382
 lu:
     loss:14062.019770
     acc:0.918378
 lv:
     loss:17228.898506
     acc:0.264225
 le:
     loss:1518.413911
     acc:0.999527
 encoder:
     loss:0.098675
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17660.440527
     acc:0.802589
 tv:
     loss:19156.506934
     acc:0.478990
 lu:
     loss:13426.048975
     acc:0.911174
 lv:
     loss:16748.214844
     acc:0.188576
 le:
     loss:1448.673804
     acc:0.999109
 encoder:
     loss:0.111576
----------------------------


Epoch: [312/1000]:
train:
----------------------------
 tu:
     loss:18534.846475
     acc:0.803891
 tv:
     loss:19972.406988
     acc:0.507782
 lu:
     loss:14062.219238
     acc:0.918313
 lv:
     loss:17227.299078
     acc:0.264304
 le:
     loss:1518.357615
     acc:0.999538
 encoder:
     loss:0.160827
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17660.751074
     acc:0.802563
 tv:
     loss:19157.385254
     acc:0.478504
 lu:
     loss:13425.698340
     acc:0.911580
 lv:
     loss:16745.519141
     acc:0.189768
 le:
     loss:1448.562268
     acc:0.999127
 encoder:
     loss:0.188172
----------------------------


Epoch: [313/1000]:
train:
----------------------------
 tu:
     loss:18534.284475
     acc:0.803946
 tv:
     loss:19973.088027
     acc:0.507527
 lu:
     loss:14063.689964
     acc:0.917883
 lv:
     loss:17229.100234
     acc:0.263816
 le:
     loss:1518.456014
     acc:0.999516
 encoder:
     loss:0.175470
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17660.619531
     acc:0.802567
 tv:
     loss:19159.601367
     acc:0.477643
 lu:
     loss:13425.995068
     acc:0.911264
 lv:
     loss:16747.911523
     acc:0.189006
 le:
     loss:1449.016223
     acc:0.999027
 encoder:
     loss:0.110539
----------------------------


Epoch: [314/1000]:
train:
----------------------------
 tu:
     loss:18533.947254
     acc:0.803967
 tv:
     loss:19971.470851
     acc:0.508043
 lu:
     loss:14061.908748
     acc:0.918368
 lv:
     loss:17225.491756
     acc:0.264776
 le:
     loss:1518.377617
     acc:0.999535
 encoder:
     loss:0.122420
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17660.850586
     acc:0.802582
 tv:
     loss:19157.780664
     acc:0.478611
 lu:
     loss:13424.098584
     acc:0.912066
 lv:
     loss:16744.781543
     acc:0.189630
 le:
     loss:1448.980255
     acc:0.999026
 encoder:
     loss:0.153208
----------------------------


Epoch: [315/1000]:
train:
----------------------------
 tu:
     loss:18533.840014
     acc:0.804002
 tv:
     loss:19972.079385
     acc:0.507761
 lu:
     loss:14063.156852
     acc:0.918120
 lv:
     loss:17225.921364
     acc:0.264673
 le:
     loss:1518.405570
     acc:0.999529
 encoder:
     loss:0.112360
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17660.604687
     acc:0.802588
 tv:
     loss:19157.162598
     acc:0.478879
 lu:
     loss:13426.491357
     acc:0.911209
 lv:
     loss:16751.420312
     acc:0.187800
 le:
     loss:1448.753754
     acc:0.999108
 encoder:
     loss:0.114169
----------------------------


Epoch: [316/1000]:
train:
----------------------------
 tu:
     loss:18533.800702
     acc:0.803918
 tv:
     loss:19971.236896
     acc:0.508076
 lu:
     loss:14062.639274
     acc:0.918352
 lv:
     loss:17223.656636
     acc:0.265424
 le:
     loss:1518.335155
     acc:0.999551
 encoder:
     loss:0.197836
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17660.681836
     acc:0.802528
 tv:
     loss:19157.908594
     acc:0.478336
 lu:
     loss:13425.186816
     acc:0.911471
 lv:
     loss:16742.948828
     acc:0.190780
 le:
     loss:1448.932349
     acc:0.999045
 encoder:
     loss:0.242477
----------------------------


Epoch: [317/1000]:
train:
----------------------------
 tu:
     loss:18534.399687
     acc:0.803927
 tv:
     loss:19968.609829
     acc:0.508601
 lu:
     loss:14061.985772
     acc:0.918290
 lv:
     loss:17225.287927
     acc:0.264661
 le:
     loss:1518.265626
     acc:0.999569
 encoder:
     loss:0.160465
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17660.745703
     acc:0.802707
 tv:
     loss:19156.408203
     acc:0.478868
 lu:
     loss:13425.977148
     acc:0.911312
 lv:
     loss:16748.285645
     acc:0.189063
 le:
     loss:1448.842700
     acc:0.999025
 encoder:
     loss:0.145698
----------------------------


Epoch: [318/1000]:
train:
----------------------------
 tu:
     loss:18533.094386
     acc:0.804154
 tv:
     loss:19970.884266
     acc:0.507896
 lu:
     loss:14061.735794
     acc:0.918277
 lv:
     loss:17225.577273
     acc:0.264607
 le:
     loss:1518.371879
     acc:0.999548
 encoder:
     loss:0.148129
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17653.563770
     acc:0.804096
 tv:
     loss:19158.900000
     acc:0.478435
 lu:
     loss:13425.077441
     acc:0.911380
 lv:
     loss:16751.189453
     acc:0.188251
 le:
     loss:1448.749072
     acc:0.999088
 encoder:
     loss:0.143129
----------------------------


Epoch: [319/1000]:
train:
----------------------------
 tu:
     loss:18498.313636
     acc:0.811720
 tv:
     loss:19970.636923
     acc:0.507956
 lu:
     loss:14061.732649
     acc:0.918406
 lv:
     loss:17225.177689
     acc:0.264692
 le:
     loss:1518.281889
     acc:0.999563
 encoder:
     loss:0.159005
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17619.425684
     acc:0.811592
 tv:
     loss:19159.518262
     acc:0.477857
 lu:
     loss:13427.298486
     acc:0.910997
 lv:
     loss:16751.175586
     acc:0.188524
 le:
     loss:1449.166376
     acc:0.998984
 encoder:
     loss:0.115014
----------------------------


Epoch: [320/1000]:
train:
----------------------------
 tu:
     loss:18492.248649
     acc:0.812868
 tv:
     loss:19968.527412
     acc:0.508417
 lu:
     loss:14062.041118
     acc:0.918339
 lv:
     loss:17221.429903
     acc:0.265505
 le:
     loss:1518.369108
     acc:0.999536
 encoder:
     loss:0.130179
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17619.270898
     acc:0.811656
 tv:
     loss:19154.980371
     acc:0.479317
 lu:
     loss:13425.668018
     acc:0.911241
 lv:
     loss:16746.243848
     acc:0.188928
 le:
     loss:1449.440295
     acc:0.998941
 encoder:
     loss:0.089583
----------------------------


Epoch: [321/1000]:
train:
----------------------------
 tu:
     loss:18490.724371
     acc:0.813078
 tv:
     loss:19966.820676
     acc:0.508971
 lu:
     loss:14062.047738
     acc:0.918281
 lv:
     loss:17220.342546
     acc:0.265907
 le:
     loss:1518.306182
     acc:0.999561
 encoder:
     loss:0.126498
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17619.158984
     acc:0.811551
 tv:
     loss:19157.673730
     acc:0.478667
 lu:
     loss:13424.868164
     acc:0.911325
 lv:
     loss:16746.819727
     acc:0.189636
 le:
     loss:1448.964124
     acc:0.999005
 encoder:
     loss:0.136190
----------------------------


Epoch: [322/1000]:
train:
----------------------------
 tu:
     loss:18490.101858
     acc:0.813201
 tv:
     loss:19970.650311
     acc:0.507936
 lu:
     loss:14061.418593
     acc:0.918408
 lv:
     loss:17218.931538
     acc:0.266199
 le:
     loss:1518.363347
     acc:0.999538
 encoder:
     loss:0.235970
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17619.232129
     acc:0.811616
 tv:
     loss:19158.360742
     acc:0.478261
 lu:
     loss:13425.952148
     acc:0.911377
 lv:
     loss:16746.025781
     acc:0.189136
 le:
     loss:1449.515466
     acc:0.998982
 encoder:
     loss:0.176545
----------------------------


Epoch: [323/1000]:
train:
----------------------------
 tu:
     loss:18489.535417
     acc:0.813293
 tv:
     loss:19966.891999
     acc:0.508556
 lu:
     loss:14061.803722
     acc:0.918388
 lv:
     loss:17217.328182
     acc:0.266498
 le:
     loss:1518.415970
     acc:0.999529
 encoder:
     loss:0.139517
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17618.741992
     acc:0.811612
 tv:
     loss:19156.355664
     acc:0.478617
 lu:
     loss:13423.504492
     acc:0.911899
 lv:
     loss:16749.774219
     acc:0.189063
 le:
     loss:1448.641248
     acc:0.999108
 encoder:
     loss:0.145290
----------------------------


Epoch: [324/1000]:
train:
----------------------------
 tu:
     loss:18489.272268
     acc:0.813184
 tv:
     loss:19966.311841
     acc:0.509071
 lu:
     loss:14061.820721
     acc:0.918481
 lv:
     loss:17217.485374
     acc:0.266656
 le:
     loss:1518.357359
     acc:0.999548
 encoder:
     loss:0.206245
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17618.498633
     acc:0.811777
 tv:
     loss:19160.665918
     acc:0.477868
 lu:
     loss:13424.528418
     acc:0.911449
 lv:
     loss:16744.434473
     acc:0.189975
 le:
     loss:1448.688916
     acc:0.999145
 encoder:
     loss:0.263347
----------------------------


Epoch: [325/1000]:
train:
----------------------------
 tu:
     loss:18490.292492
     acc:0.812948
 tv:
     loss:19965.710665
     acc:0.509155
 lu:
     loss:14060.548783
     acc:0.918565
 lv:
     loss:17216.577058
     acc:0.266729
 le:
     loss:1518.353122
     acc:0.999548
 encoder:
     loss:0.304585
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17618.440527
     acc:0.811717
 tv:
     loss:19156.159375
     acc:0.478821
 lu:
     loss:13424.147998
     acc:0.911684
 lv:
     loss:16745.427539
     acc:0.189858
 le:
     loss:1448.793634
     acc:0.999088
 encoder:
     loss:0.183832
----------------------------


Epoch: [326/1000]:
train:
----------------------------
 tu:
     loss:18489.270076
     acc:0.813194
 tv:
     loss:19963.380848
     acc:0.509629
 lu:
     loss:14061.052485
     acc:0.918600
 lv:
     loss:17213.827444
     acc:0.267058
 le:
     loss:1518.413188
     acc:0.999537
 encoder:
     loss:0.140188
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17618.996289
     acc:0.811756
 tv:
     loss:19157.216504
     acc:0.478644
 lu:
     loss:13424.566113
     acc:0.911851
 lv:
     loss:16746.777832
     acc:0.189238
 le:
     loss:1448.513574
     acc:0.999167
 encoder:
     loss:0.086035
----------------------------


Epoch: [327/1000]:
train:
----------------------------
 tu:
     loss:18488.647052
     acc:0.813319
 tv:
     loss:19966.197822
     acc:0.508660
 lu:
     loss:14060.556993
     acc:0.918479
 lv:
     loss:17215.470737
     acc:0.266650
 le:
     loss:1518.307627
     acc:0.999554
 encoder:
     loss:0.128171
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17618.617480
     acc:0.811672
 tv:
     loss:19154.636523
     acc:0.478911
 lu:
     loss:13426.132520
     acc:0.911296
 lv:
     loss:16749.018262
     acc:0.188650
 le:
     loss:1448.617566
     acc:0.999108
 encoder:
     loss:0.188512
----------------------------


Epoch: [328/1000]:
train:
----------------------------
 tu:
     loss:18488.569109
     acc:0.813313
 tv:
     loss:19961.833178
     acc:0.509987
 lu:
     loss:14060.831577
     acc:0.918566
 lv:
     loss:17213.863611
     acc:0.267199
 le:
     loss:1518.332413
     acc:0.999550
 encoder:
     loss:0.130217
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17618.712207
     acc:0.811809
 tv:
     loss:19158.622168
     acc:0.478450
 lu:
     loss:13425.508643
     acc:0.911546
 lv:
     loss:16745.470508
     acc:0.189860
 le:
     loss:1448.963574
     acc:0.999006
 encoder:
     loss:0.090668
----------------------------


Epoch: [329/1000]:
train:
----------------------------
 tu:
     loss:18488.800883
     acc:0.813376
 tv:
     loss:19963.040164
     acc:0.509454
 lu:
     loss:14060.611794
     acc:0.918563
 lv:
     loss:17213.600018
     acc:0.267085
 le:
     loss:1518.434746
     acc:0.999522
 encoder:
     loss:0.100576
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17618.577441
     acc:0.811797
 tv:
     loss:19160.484277
     acc:0.478261
 lu:
     loss:13422.908838
     acc:0.912135
 lv:
     loss:16751.804004
     acc:0.187834
 le:
     loss:1448.778680
     acc:0.999064
 encoder:
     loss:0.093286
----------------------------


Epoch: [330/1000]:
train:
----------------------------
 tu:
     loss:18488.600359
     acc:0.813269
 tv:
     loss:19963.313499
     acc:0.509289
 lu:
     loss:14061.431323
     acc:0.918362
 lv:
     loss:17210.531500
     acc:0.268027
 le:
     loss:1518.309119
     acc:0.999561
 encoder:
     loss:0.101148
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17619.380957
     acc:0.811618
 tv:
     loss:19160.220996
     acc:0.477928
 lu:
     loss:13424.778320
     acc:0.911634
 lv:
     loss:16743.009863
     acc:0.190412
 le:
     loss:1449.553058
     acc:0.998917
 encoder:
     loss:0.111049
----------------------------


Epoch: [331/1000]:
train:
----------------------------
 tu:
     loss:18488.539926
     acc:0.813364
 tv:
     loss:19964.342512
     acc:0.509334
 lu:
     loss:14060.136299
     acc:0.918703
 lv:
     loss:17209.370049
     acc:0.268115
 le:
     loss:1518.374378
     acc:0.999539
 encoder:
     loss:0.138502
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17619.054492
     acc:0.811614
 tv:
     loss:19161.654102
     acc:0.477502
 lu:
     loss:13423.688525
     acc:0.912116
 lv:
     loss:16745.536133
     acc:0.189748
 le:
     loss:1448.975513
     acc:0.999025
 encoder:
     loss:0.113550
----------------------------


Epoch: [332/1000]:
train:
----------------------------
 tu:
     loss:18488.982763
     acc:0.813213
 tv:
     loss:19963.666595
     acc:0.509617
 lu:
     loss:14059.418264
     acc:0.918867
 lv:
     loss:17212.483069
     acc:0.267325
 le:
     loss:1518.431812
     acc:0.999522
 encoder:
     loss:0.122750
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17619.116504
     acc:0.811592
 tv:
     loss:19162.674219
     acc:0.477035
 lu:
     loss:13423.253418
     acc:0.911913
 lv:
     loss:16745.003809
     acc:0.189762
 le:
     loss:1448.492798
     acc:0.999167
 encoder:
     loss:0.140980
----------------------------


Epoch: [333/1000]:
train:
----------------------------
 tu:
     loss:18487.850064
     acc:0.813432
 tv:
     loss:19964.932594
     acc:0.509094
 lu:
     loss:14060.111339
     acc:0.918655
 lv:
     loss:17210.320176
     acc:0.268167
 le:
     loss:1518.270821
     acc:0.999566
 encoder:
     loss:0.397171
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17618.432812
     acc:0.811856
 tv:
     loss:19160.655469
     acc:0.478118
 lu:
     loss:13424.287158
     acc:0.911903
 lv:
     loss:16747.889355
     acc:0.189280
 le:
     loss:1449.020190
     acc:0.999024
 encoder:
     loss:0.247758
----------------------------


Epoch: [334/1000]:
train:
----------------------------
 tu:
     loss:18488.251737
     acc:0.813280
 tv:
     loss:19964.017056
     acc:0.509375
 lu:
     loss:14061.019588
     acc:0.918393
 lv:
     loss:17210.458019
     acc:0.267930
 le:
     loss:1518.459339
     acc:0.999509
 encoder:
     loss:0.201607
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17618.131348
     acc:0.811935
 tv:
     loss:19157.472168
     acc:0.478658
 lu:
     loss:13424.433252
     acc:0.911647
 lv:
     loss:16745.784277
     acc:0.189510
 le:
     loss:1448.506549
     acc:0.999150
 encoder:
     loss:0.176223
----------------------------


Epoch: [335/1000]:
train:
----------------------------
 tu:
     loss:18488.788869
     acc:0.813277
 tv:
     loss:19960.197686
     acc:0.510117
 lu:
     loss:14058.918854
     acc:0.918956
 lv:
     loss:17207.106706
     acc:0.268330
 le:
     loss:1518.492832
     acc:0.999508
 encoder:
     loss:0.138387
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17618.573535
     acc:0.811771
 tv:
     loss:19159.870898
     acc:0.478165
 lu:
     loss:13425.846826
     acc:0.911323
 lv:
     loss:16750.388770
     acc:0.187872
 le:
     loss:1448.999933
     acc:0.999005
 encoder:
     loss:0.152570
----------------------------


Epoch: [336/1000]:
train:
----------------------------
 tu:
     loss:18487.938636
     acc:0.813378
 tv:
     loss:19961.532613
     acc:0.509855
 lu:
     loss:14059.348519
     acc:0.918852
 lv:
     loss:17208.428643
     acc:0.268199
 le:
     loss:1518.340627
     acc:0.999548
 encoder:
     loss:0.240111
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17618.855176
     acc:0.811739
 tv:
     loss:19162.795215
     acc:0.477486
 lu:
     loss:13423.806104
     acc:0.911888
 lv:
     loss:16748.106934
     acc:0.189063
 le:
     loss:1448.697473
     acc:0.999124
 encoder:
     loss:0.234199
----------------------------


Epoch: [337/1000]:
train:
----------------------------
 tu:
     loss:18487.534430
     acc:0.813428
 tv:
     loss:19960.507847
     acc:0.510029
 lu:
     loss:14060.004871
     acc:0.918593
 lv:
     loss:17208.375784
     acc:0.268433
 le:
     loss:1518.509465
     acc:0.999499
 encoder:
     loss:0.285454
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17619.259668
     acc:0.811668
 tv:
     loss:19160.796484
     acc:0.477676
 lu:
     loss:13425.019141
     acc:0.911804
 lv:
     loss:16746.628516
     acc:0.189728
 le:
     loss:1448.969244
     acc:0.999028
 encoder:
     loss:0.397159
----------------------------


Epoch: [338/1000]:
train:
----------------------------
 tu:
     loss:18488.744833
     acc:0.813142
 tv:
     loss:19961.116075
     acc:0.509831
 lu:
     loss:14058.004940
     acc:0.919204
 lv:
     loss:17206.642181
     acc:0.268618
 le:
     loss:1518.409085
     acc:0.999534
 encoder:
     loss:0.277011
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17618.669141
     acc:0.811739
 tv:
     loss:19161.579199
     acc:0.477649
 lu:
     loss:13422.997363
     acc:0.912182
 lv:
     loss:16746.191113
     acc:0.190364
 le:
     loss:1448.389215
     acc:0.999169
 encoder:
     loss:0.179112
----------------------------


Epoch: [339/1000]:
train:
----------------------------
 tu:
     loss:18486.787745
     acc:0.813539
 tv:
     loss:19959.892998
     acc:0.510122
 lu:
     loss:14058.368936
     acc:0.919157
 lv:
     loss:17205.713368
     acc:0.268986
 le:
     loss:1518.432084
     acc:0.999520
 encoder:
     loss:0.133513
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17618.880176
     acc:0.811672
 tv:
     loss:19156.838086
     acc:0.478817
 lu:
     loss:13423.330029
     acc:0.912040
 lv:
     loss:16746.614648
     acc:0.189146
 le:
     loss:1448.767529
     acc:0.999085
 encoder:
     loss:0.106621
----------------------------


Epoch: [340/1000]:
train:
----------------------------
 tu:
     loss:18487.836176
     acc:0.813317
 tv:
     loss:19959.314056
     acc:0.510264
 lu:
     loss:14058.428359
     acc:0.919017
 lv:
     loss:17202.331089
     acc:0.270086
 le:
     loss:1518.324816
     acc:0.999539
 encoder:
     loss:0.113715
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17618.033301
     acc:0.811876
 tv:
     loss:19161.619043
     acc:0.477770
 lu:
     loss:13422.346143
     acc:0.912153
 lv:
     loss:16744.006055
     acc:0.190457
 le:
     loss:1449.350500
     acc:0.998943
 encoder:
     loss:0.108071
----------------------------


Epoch: [341/1000]:
train:
----------------------------
 tu:
     loss:18466.064226
     acc:0.818173
 tv:
     loss:19958.210279
     acc:0.510536
 lu:
     loss:14058.205089
     acc:0.919005
 lv:
     loss:17204.511003
     acc:0.268832
 le:
     loss:1518.292896
     acc:0.999556
 encoder:
     loss:0.100220
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17582.814355
     acc:0.819483
 tv:
     loss:19160.533008
     acc:0.477928
 lu:
     loss:13421.458105
     acc:0.912562
 lv:
     loss:16746.312109
     acc:0.188982
 le:
     loss:1449.072736
     acc:0.999023
 encoder:
     loss:0.092534
----------------------------


Epoch: [342/1000]:
train:
----------------------------
 tu:
     loss:18452.913268
     acc:0.820830
 tv:
     loss:19959.190759
     acc:0.510214
 lu:
     loss:14057.194654
     acc:0.919279
 lv:
     loss:17204.293060
     acc:0.269091
 le:
     loss:1518.285889
     acc:0.999560
 encoder:
     loss:0.253924
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17582.744238
     acc:0.819431
 tv:
     loss:19163.038867
     acc:0.477554
 lu:
     loss:13422.632275
     acc:0.912214
 lv:
     loss:16743.412695
     acc:0.190082
 le:
     loss:1448.533923
     acc:0.999105
 encoder:
     loss:0.291870
----------------------------


Epoch: [343/1000]:
train:
----------------------------
 tu:
     loss:18453.163245
     acc:0.820728
 tv:
     loss:19958.587686
     acc:0.510417
 lu:
     loss:14057.250011
     acc:0.919338
 lv:
     loss:17201.965184
     acc:0.269800
 le:
     loss:1518.237997
     acc:0.999567
 encoder:
     loss:0.163071
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17582.746680
     acc:0.819476
 tv:
     loss:19161.682129
     acc:0.477705
 lu:
     loss:13421.087842
     acc:0.912207
 lv:
     loss:16749.911914
     acc:0.188027
 le:
     loss:1448.986646
     acc:0.999024
 encoder:
     loss:0.156112
----------------------------


Epoch: [344/1000]:
train:
----------------------------
 tu:
     loss:18451.761832
     acc:0.820967
 tv:
     loss:19957.125432
     acc:0.510756
 lu:
     loss:14057.740461
     acc:0.919170
 lv:
     loss:17202.039278
     acc:0.269814
 le:
     loss:1518.271609
     acc:0.999560
 encoder:
     loss:0.125893
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17582.517480
     acc:0.819596
 tv:
     loss:19154.846387
     acc:0.479251
 lu:
     loss:13423.030957
     acc:0.911992
 lv:
     loss:16748.152051
     acc:0.188875
 le:
     loss:1448.712225
     acc:0.999108
 encoder:
     loss:0.156567
----------------------------


Epoch: [345/1000]:
train:
----------------------------
 tu:
     loss:18451.975881
     acc:0.820826
 tv:
     loss:19957.128940
     acc:0.510580
 lu:
     loss:14056.614405
     acc:0.919344
 lv:
     loss:17200.686353
     acc:0.269920
 le:
     loss:1518.271013
     acc:0.999566
 encoder:
     loss:0.158452
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17582.248437
     acc:0.819660
 tv:
     loss:19158.748047
     acc:0.478291
 lu:
     loss:13421.698926
     acc:0.912356
 lv:
     loss:16745.253516
     acc:0.189440
 le:
     loss:1448.633014
     acc:0.999107
 encoder:
     loss:0.114705
----------------------------


Epoch: [346/1000]:
train:
----------------------------
 tu:
     loss:18450.940327
     acc:0.821027
 tv:
     loss:19956.440714
     acc:0.510795
 lu:
     loss:14056.428041
     acc:0.919375
 lv:
     loss:17201.478845
     acc:0.269431
 le:
     loss:1518.297456
     acc:0.999556
 encoder:
     loss:0.108577
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17583.075391
     acc:0.819426
 tv:
     loss:19163.185156
     acc:0.477126
 lu:
     loss:13420.299951
     acc:0.912575
 lv:
     loss:16747.819922
     acc:0.189075
 le:
     loss:1448.660962
     acc:0.999108
 encoder:
     loss:0.088274
----------------------------


Epoch: [347/1000]:
train:
----------------------------
 tu:
     loss:18451.081940
     acc:0.820998
 tv:
     loss:19956.989508
     acc:0.510643
 lu:
     loss:14056.025288
     acc:0.919486
 lv:
     loss:17198.773438
     acc:0.270351
 le:
     loss:1518.212239
     acc:0.999576
 encoder:
     loss:0.140314
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17583.202832
     acc:0.819446
 tv:
     loss:19162.684473
     acc:0.477282
 lu:
     loss:13422.382812
     acc:0.912138
 lv:
     loss:16749.904785
     acc:0.189055
 le:
     loss:1448.631683
     acc:0.999086
 encoder:
     loss:0.143655
----------------------------


Epoch: [348/1000]:
train:
----------------------------
 tu:
     loss:18450.916958
     acc:0.821055
 tv:
     loss:19955.548601
     acc:0.510928
 lu:
     loss:14056.466365
     acc:0.919269
 lv:
     loss:17196.536133
     acc:0.270637
 le:
     loss:1518.254891
     acc:0.999562
 encoder:
     loss:0.244523
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17582.186230
     acc:0.819579
 tv:
     loss:19160.824121
     acc:0.477749
 lu:
     loss:13420.142773
     acc:0.912524
 lv:
     loss:16746.912402
     acc:0.189464
 le:
     loss:1448.519281
     acc:0.999129
 encoder:
     loss:0.298510
----------------------------


Epoch: [349/1000]:
train:
----------------------------
 tu:
     loss:18450.866847
     acc:0.821045
 tv:
     loss:19953.642317
     acc:0.511276
 lu:
     loss:14056.277026
     acc:0.919297
 lv:
     loss:17198.577557
     acc:0.270199
 le:
     loss:1518.319476
     acc:0.999553
 encoder:
     loss:0.241939
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17582.319922
     acc:0.819719
 tv:
     loss:19160.802344
     acc:0.477719
 lu:
     loss:13419.304395
     acc:0.912806
 lv:
     loss:16749.719043
     acc:0.189048
 le:
     loss:1448.993396
     acc:0.999006
 encoder:
     loss:0.125216
----------------------------


Epoch: [350/1000]:
train:
----------------------------
 tu:
     loss:18450.675441
     acc:0.821043
 tv:
     loss:19953.853504
     acc:0.511105
 lu:
     loss:14056.081475
     acc:0.919412
 lv:
     loss:17197.649982
     acc:0.270451
 le:
     loss:1518.366062
     acc:0.999540
 encoder:
     loss:0.135419
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17582.398047
     acc:0.819536
 tv:
     loss:19158.174512
     acc:0.478131
 lu:
     loss:13420.293408
     acc:0.912716
 lv:
     loss:16747.753809
     acc:0.188732
 le:
     loss:1448.668866
     acc:0.999109
 encoder:
     loss:0.116651
----------------------------


Epoch: [351/1000]:
train:
----------------------------
 tu:
     loss:18450.402707
     acc:0.821107
 tv:
     loss:19956.518543
     acc:0.510844
 lu:
     loss:14056.166776
     acc:0.919004
 lv:
     loss:17195.866904
     acc:0.270793
 le:
     loss:1518.249015
     acc:0.999568
 encoder:
     loss:0.181581
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17583.209473
     acc:0.819404
 tv:
     loss:19162.775586
     acc:0.477279
 lu:
     loss:13422.625928
     acc:0.912219
 lv:
     loss:16746.153809
     acc:0.189432
 le:
     loss:1448.865173
     acc:0.999068
 encoder:
     loss:0.309223
----------------------------


Epoch: [352/1000]:
train:
----------------------------
 tu:
     loss:18450.463356
     acc:0.821133
 tv:
     loss:19955.013093
     acc:0.511083
 lu:
     loss:14056.014501
     acc:0.919469
 lv:
     loss:17196.972134
     acc:0.270979
 le:
     loss:1518.205669
     acc:0.999583
 encoder:
     loss:0.261729
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17583.411523
     acc:0.819264
 tv:
     loss:19160.290625
     acc:0.478074
 lu:
     loss:13422.086426
     acc:0.912299
 lv:
     loss:16745.558887
     acc:0.190083
 le:
     loss:1448.859668
     acc:0.999123
 encoder:
     loss:0.142842
----------------------------


Epoch: [353/1000]:
train:
----------------------------
 tu:
     loss:18450.058367
     acc:0.821192
 tv:
     loss:19953.817383
     acc:0.511188
 lu:
     loss:14054.057810
     acc:0.920075
 lv:
     loss:17193.677348
     acc:0.271599
 le:
     loss:1518.333124
     acc:0.999546
 encoder:
     loss:0.145113
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17583.398438
     acc:0.819409
 tv:
     loss:19158.914160
     acc:0.478027
 lu:
     loss:13421.008203
     acc:0.912661
 lv:
     loss:16743.973633
     acc:0.189802
 le:
     loss:1449.095581
     acc:0.998984
 encoder:
     loss:0.138093
----------------------------


Epoch: [354/1000]:
train:
----------------------------
 tu:
     loss:18450.264467
     acc:0.821074
 tv:
     loss:19953.724587
     acc:0.511461
 lu:
     loss:14055.161860
     acc:0.919619
 lv:
     loss:17193.986192
     acc:0.271621
 le:
     loss:1518.225377
     acc:0.999568
 encoder:
     loss:0.129720
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17583.401660
     acc:0.819308
 tv:
     loss:19164.316602
     acc:0.477007
 lu:
     loss:13422.438477
     acc:0.912127
 lv:
     loss:16748.431738
     acc:0.189218
 le:
     loss:1449.453992
     acc:0.998963
 encoder:
     loss:0.114494
----------------------------


Epoch: [355/1000]:
train:
----------------------------
 tu:
     loss:18449.699628
     acc:0.821224
 tv:
     loss:19952.361987
     acc:0.511613
 lu:
     loss:14053.960801
     acc:0.919907
 lv:
     loss:17195.589571
     acc:0.270968
 le:
     loss:1518.253314
     acc:0.999569
 encoder:
     loss:0.149902
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17582.384473
     acc:0.819681
 tv:
     loss:19161.303125
     acc:0.477759
 lu:
     loss:13422.188184
     acc:0.912158
 lv:
     loss:16750.125000
     acc:0.188549
 le:
     loss:1448.555725
     acc:0.999128
 encoder:
     loss:0.114634
----------------------------


Epoch: [356/1000]:
train:
----------------------------
 tu:
     loss:18449.743743
     acc:0.821211
 tv:
     loss:19952.560774
     acc:0.511403
 lu:
     loss:14054.611521
     acc:0.919651
 lv:
     loss:17192.325104
     acc:0.271414
 le:
     loss:1518.287114
     acc:0.999567
 encoder:
     loss:0.111948
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17582.346875
     acc:0.819696
 tv:
     loss:19158.363086
     acc:0.478314
 lu:
     loss:13423.486621
     acc:0.911912
 lv:
     loss:16744.022949
     acc:0.190457
 le:
     loss:1448.759955
     acc:0.999105
 encoder:
     loss:0.105890
----------------------------


Epoch: [357/1000]:
train:
----------------------------
 tu:
     loss:18449.877827
     acc:0.821205
 tv:
     loss:19951.304324
     acc:0.511757
 lu:
     loss:14054.850427
     acc:0.919691
 lv:
     loss:17190.627748
     acc:0.271855
 le:
     loss:1518.199197
     acc:0.999574
 encoder:
     loss:0.126705
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17583.539844
     acc:0.819350
 tv:
     loss:19163.799512
     acc:0.477184
 lu:
     loss:13421.015723
     acc:0.912340
 lv:
     loss:16742.766211
     acc:0.190242
 le:
     loss:1449.050763
     acc:0.999024
 encoder:
     loss:0.140881
----------------------------


Epoch: [358/1000]:
train:
----------------------------
 tu:
     loss:18449.769157
     acc:0.821150
 tv:
     loss:19951.557220
     acc:0.511890
 lu:
     loss:14055.777741
     acc:0.919716
 lv:
     loss:17191.366677
     acc:0.272131
 le:
     loss:1518.187933
     acc:0.999571
 encoder:
     loss:0.112568
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17582.110742
     acc:0.819640
 tv:
     loss:19156.904297
     acc:0.478635
 lu:
     loss:13421.675293
     acc:0.912543
 lv:
     loss:16741.694141
     acc:0.190908
 le:
     loss:1448.886169
     acc:0.999065
 encoder:
     loss:0.096656
----------------------------


Epoch: [359/1000]:
train:
----------------------------
 tu:
     loss:18449.571312
     acc:0.821200
 tv:
     loss:19953.486975
     acc:0.511207
 lu:
     loss:14055.450684
     acc:0.919539
 lv:
     loss:17192.433639
     acc:0.271427
 le:
     loss:1518.109216
     acc:0.999605
 encoder:
     loss:0.094225
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17582.486719
     acc:0.819471
 tv:
     loss:19160.362012
     acc:0.477823
 lu:
     loss:13419.927588
     acc:0.912691
 lv:
     loss:16742.950000
     acc:0.190413
 le:
     loss:1448.903058
     acc:0.999066
 encoder:
     loss:0.091547
----------------------------


Epoch: [360/1000]:
train:
----------------------------
 tu:
     loss:18449.576467
     acc:0.821129
 tv:
     loss:19951.552553
     acc:0.511740
 lu:
     loss:14054.804392
     acc:0.919760
 lv:
     loss:17189.025868
     acc:0.272720
 le:
     loss:1518.261969
     acc:0.999566
 encoder:
     loss:0.145169
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17582.194141
     acc:0.819620
 tv:
     loss:19163.740918
     acc:0.477000
 lu:
     loss:13419.344141
     acc:0.913001
 lv:
     loss:16747.836035
     acc:0.189331
 le:
     loss:1448.600934
     acc:0.999109
 encoder:
     loss:0.214714
----------------------------


Epoch: [361/1000]:
train:
----------------------------
 tu:
     loss:18449.788574
     acc:0.821190
 tv:
     loss:19949.929301
     acc:0.512100
 lu:
     loss:14053.748740
     acc:0.919931
 lv:
     loss:17189.888013
     acc:0.271990
 le:
     loss:1518.292021
     acc:0.999554
 encoder:
     loss:0.215120
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17581.786426
     acc:0.819760
 tv:
     loss:19161.647852
     acc:0.478141
 lu:
     loss:13421.142187
     acc:0.912500
 lv:
     loss:16739.187109
     acc:0.191108
 le:
     loss:1448.854340
     acc:0.999066
 encoder:
     loss:0.183621
----------------------------


Epoch: [362/1000]:
train:
----------------------------
 tu:
     loss:18449.901095
     acc:0.821139
 tv:
     loss:19950.912791
     acc:0.511971
 lu:
     loss:14053.599689
     acc:0.920032
 lv:
     loss:17192.875454
     acc:0.271484
 le:
     loss:1518.279700
     acc:0.999554
 encoder:
     loss:0.138199
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17582.638477
     acc:0.819497
 tv:
     loss:19163.829199
     acc:0.477484
 lu:
     loss:13421.128662
     acc:0.912262
 lv:
     loss:16739.948242
     acc:0.190653
 le:
     loss:1449.216968
     acc:0.999003
 encoder:
     loss:0.137735
----------------------------


Epoch: [363/1000]:
train:
----------------------------
 tu:
     loss:18449.260947
     acc:0.821255
 tv:
     loss:19950.110533
     acc:0.511940
 lu:
     loss:14054.067746
     acc:0.919950
 lv:
     loss:17189.718773
     acc:0.272384
 le:
     loss:1518.276127
     acc:0.999557
 encoder:
     loss:0.134392
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17582.322363
     acc:0.819673
 tv:
     loss:19160.286523
     acc:0.478024
 lu:
     loss:13421.002100
     acc:0.912405
 lv:
     loss:16742.442676
     acc:0.190894
 le:
     loss:1448.603247
     acc:0.999109
 encoder:
     loss:0.147160
----------------------------


Epoch: [364/1000]:
train:
----------------------------
 tu:
     loss:18449.425009
     acc:0.821202
 tv:
     loss:19948.546932
     acc:0.512467
 lu:
     loss:14053.392396
     acc:0.919943
 lv:
     loss:17189.017794
     acc:0.272618
 le:
     loss:1518.284439
     acc:0.999568
 encoder:
     loss:0.157823
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17582.554297
     acc:0.819537
 tv:
     loss:19159.609180
     acc:0.478267
 lu:
     loss:13421.285791
     acc:0.912290
 lv:
     loss:16745.551953
     acc:0.189476
 le:
     loss:1448.669849
     acc:0.999065
 encoder:
     loss:0.164824
----------------------------


Epoch: [365/1000]:
train:
----------------------------
 tu:
     loss:18449.540141
     acc:0.821143
 tv:
     loss:19949.944063
     acc:0.512187
 lu:
     loss:14054.155512
     acc:0.919802
 lv:
     loss:17186.620253
     acc:0.272959
 le:
     loss:1518.174258
     acc:0.999585
 encoder:
     loss:0.179012
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17582.192188
     acc:0.819638
 tv:
     loss:19160.920605
     acc:0.478057
 lu:
     loss:13420.522217
     acc:0.912532
 lv:
     loss:16745.840039
     acc:0.190431
 le:
     loss:1448.690631
     acc:0.999106
 encoder:
     loss:0.271967
----------------------------


Epoch: [366/1000]:
train:
----------------------------
 tu:
     loss:18448.841059
     acc:0.821298
 tv:
     loss:19945.022359
     acc:0.513143
 lu:
     loss:14052.768305
     acc:0.920145
 lv:
     loss:17185.986112
     acc:0.272925
 le:
     loss:1518.269352
     acc:0.999560
 encoder:
     loss:0.282743
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17582.177539
     acc:0.819621
 tv:
     loss:19143.324414
     acc:0.481926
 lu:
     loss:13421.317139
     acc:0.912404
 lv:
     loss:16745.393164
     acc:0.190044
 le:
     loss:1448.860040
     acc:0.999045
 encoder:
     loss:0.175021
----------------------------


Epoch: [367/1000]:
train:
----------------------------
 tu:
     loss:18450.033237
     acc:0.821043
 tv:
     loss:19932.575593
     acc:0.515758
 lu:
     loss:14054.373615
     acc:0.919871
 lv:
     loss:17185.241688
     acc:0.273217
 le:
     loss:1518.224448
     acc:0.999575
 encoder:
     loss:0.166573
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17582.266992
     acc:0.819676
 tv:
     loss:19140.736816
     acc:0.482328
 lu:
     loss:13421.111914
     acc:0.912497
 lv:
     loss:16749.300098
     acc:0.189630
 le:
     loss:1448.803412
     acc:0.999085
 encoder:
     loss:0.111578
----------------------------


Epoch: [368/1000]:
train:
----------------------------
 tu:
     loss:18448.978618
     acc:0.821195
 tv:
     loss:19931.679983
     acc:0.515973
 lu:
     loss:14054.017044
     acc:0.919909
 lv:
     loss:17185.946845
     acc:0.272904
 le:
     loss:1518.277290
     acc:0.999553
 encoder:
     loss:0.126757
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17581.787598
     acc:0.819736
 tv:
     loss:19142.710547
     acc:0.481849
 lu:
     loss:13421.412695
     acc:0.912317
 lv:
     loss:16744.965137
     acc:0.190932
 le:
     loss:1448.839618
     acc:0.999086
 encoder:
     loss:0.146588
----------------------------


Epoch: [369/1000]:
train:
----------------------------
 tu:
     loss:18448.238849
     acc:0.821407
 tv:
     loss:19929.602516
     acc:0.516491
 lu:
     loss:14053.524062
     acc:0.920023
 lv:
     loss:17183.195574
     acc:0.273500
 le:
     loss:1518.234846
     acc:0.999570
 encoder:
     loss:0.164224
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17582.536523
     acc:0.819511
 tv:
     loss:19146.380957
     acc:0.481015
 lu:
     loss:13422.936182
     acc:0.912107
 lv:
     loss:16745.544434
     acc:0.189707
 le:
     loss:1448.676495
     acc:0.999085
 encoder:
     loss:0.124471
----------------------------


Epoch: [370/1000]:
train:
----------------------------
 tu:
     loss:18448.153434
     acc:0.821343
 tv:
     loss:19931.937318
     acc:0.515734
 lu:
     loss:14053.650357
     acc:0.919873
 lv:
     loss:17183.811240
     acc:0.273331
 le:
     loss:1518.153574
     acc:0.999594
 encoder:
     loss:0.119847
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17581.896094
     acc:0.819723
 tv:
     loss:19138.949316
     acc:0.482787
 lu:
     loss:13421.997510
     acc:0.912074
 lv:
     loss:16741.212305
     acc:0.191467
 le:
     loss:1448.747540
     acc:0.999087
 encoder:
     loss:0.094370
----------------------------


Epoch: [371/1000]:
train:
----------------------------
 tu:
     loss:18449.025697
     acc:0.821238
 tv:
     loss:19929.168241
     acc:0.516438
 lu:
     loss:14052.709859
     acc:0.920031
 lv:
     loss:17183.099780
     acc:0.273318
 le:
     loss:1518.355550
     acc:0.999540
 encoder:
     loss:0.112562
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17582.742773
     acc:0.819510
 tv:
     loss:19144.288477
     acc:0.481178
 lu:
     loss:13421.033936
     acc:0.912526
 lv:
     loss:16747.759473
     acc:0.189894
 le:
     loss:1449.069189
     acc:0.999023
 encoder:
     loss:0.108787
----------------------------


Epoch: [372/1000]:
train:
----------------------------
 tu:
     loss:18448.505666
     acc:0.821331
 tv:
     loss:19926.299010
     acc:0.517012
 lu:
     loss:14052.374035
     acc:0.920154
 lv:
     loss:17180.878781
     acc:0.273873
 le:
     loss:1518.251797
     acc:0.999561
 encoder:
     loss:0.142188
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17581.853223
     acc:0.819720
 tv:
     loss:19144.391504
     acc:0.481293
 lu:
     loss:13420.702344
     acc:0.912507
 lv:
     loss:16748.463281
     acc:0.189486
 le:
     loss:1448.309778
     acc:0.999211
 encoder:
     loss:0.236726
----------------------------


Epoch: [373/1000]:
train:
----------------------------
 tu:
     loss:18449.004508
     acc:0.821297
 tv:
     loss:19923.913552
     acc:0.517493
 lu:
     loss:14052.693280
     acc:0.920102
 lv:
     loss:17181.099019
     acc:0.273729
 le:
     loss:1518.276922
     acc:0.999549
 encoder:
     loss:0.227687
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17582.100977
     acc:0.819716
 tv:
     loss:19129.808887
     acc:0.484514
 lu:
     loss:13420.549609
     acc:0.912435
 lv:
     loss:16742.651660
     acc:0.190425
 le:
     loss:1449.035138
     acc:0.998981
 encoder:
     loss:0.225012
----------------------------


Epoch: [374/1000]:
train:
----------------------------
 tu:
     loss:18448.999001
     acc:0.821156
 tv:
     loss:19912.492687
     acc:0.519994
 lu:
     loss:14052.936955
     acc:0.920094
 lv:
     loss:17179.107933
     acc:0.274456
 le:
     loss:1518.247322
     acc:0.999570
 encoder:
     loss:0.288622
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17581.735156
     acc:0.819636
 tv:
     loss:19127.525586
     acc:0.485121
 lu:
     loss:13420.256982
     acc:0.912536
 lv:
     loss:16745.148340
     acc:0.189402
 le:
     loss:1448.389325
     acc:0.999190
 encoder:
     loss:0.887444
----------------------------


Epoch: [375/1000]:
train:
----------------------------
 tu:
     loss:18448.702080
     acc:0.821366
 tv:
     loss:19911.180198
     acc:0.520292
 lu:
     loss:14052.886401
     acc:0.919970
 lv:
     loss:17178.141499
     acc:0.274242
 le:
     loss:1518.538116
     acc:0.999500
 encoder:
     loss:0.413059
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17581.712598
     acc:0.819757
 tv:
     loss:19123.742676
     acc:0.486019
 lu:
     loss:13420.669385
     acc:0.912488
 lv:
     loss:16745.544336
     acc:0.189275
 le:
     loss:1449.172974
     acc:0.998982
 encoder:
     loss:0.193586
----------------------------


Epoch: [376/1000]:
train:
----------------------------
 tu:
     loss:18448.920830
     acc:0.821248
 tv:
     loss:19911.881098
     acc:0.520207
 lu:
     loss:14052.015216
     acc:0.920217
 lv:
     loss:17177.936228
     acc:0.274795
 le:
     loss:1518.108261
     acc:0.999606
 encoder:
     loss:0.155948
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17581.954688
     acc:0.819698
 tv:
     loss:19125.567090
     acc:0.485447
 lu:
     loss:13422.024072
     acc:0.912171
 lv:
     loss:16750.441992
     acc:0.188275
 le:
     loss:1448.537347
     acc:0.999130
 encoder:
     loss:0.137714
----------------------------


Epoch: [377/1000]:
train:
----------------------------
 tu:
     loss:18448.024823
     acc:0.821417
 tv:
     loss:19911.832769
     acc:0.519961
 lu:
     loss:14052.916969
     acc:0.920011
 lv:
     loss:17180.866733
     acc:0.273746
 le:
     loss:1518.282425
     acc:0.999554
 encoder:
     loss:0.166713
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17581.951953
     acc:0.819679
 tv:
     loss:19124.093945
     acc:0.485868
 lu:
     loss:13422.311230
     acc:0.912121
 lv:
     loss:16746.926465
     acc:0.189121
 le:
     loss:1448.246051
     acc:0.999211
 encoder:
     loss:0.109187
----------------------------


Epoch: [378/1000]:
train:
----------------------------
 tu:
     loss:18448.391692
     acc:0.821299
 tv:
     loss:19908.780069
     acc:0.520438
 lu:
     loss:14052.969431
     acc:0.920052
 lv:
     loss:17176.069586
     acc:0.274916
 le:
     loss:1518.362746
     acc:0.999538
 encoder:
     loss:0.137909
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17582.518359
     acc:0.819556
 tv:
     loss:19124.420215
     acc:0.485658
 lu:
     loss:13421.018457
     acc:0.912580
 lv:
     loss:16745.555859
     acc:0.189748
 le:
     loss:1449.151837
     acc:0.998942
 encoder:
     loss:0.103821
----------------------------


Epoch: [379/1000]:
train:
----------------------------
 tu:
     loss:18448.297375
     acc:0.821264
 tv:
     loss:19908.390148
     acc:0.520971
 lu:
     loss:14053.047795
     acc:0.920157
 lv:
     loss:17179.089117
     acc:0.274572
 le:
     loss:1518.127979
     acc:0.999594
 encoder:
     loss:0.148769
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17582.651270
     acc:0.819656
 tv:
     loss:19123.147559
     acc:0.486091
 lu:
     loss:13421.242480
     acc:0.912395
 lv:
     loss:16741.353418
     acc:0.190738
 le:
     loss:1448.561786
     acc:0.999146
 encoder:
     loss:0.173460
----------------------------


Epoch: [380/1000]:
train:
----------------------------
 tu:
     loss:18448.652253
     acc:0.821292
 tv:
     loss:19910.456861
     acc:0.520367
 lu:
     loss:14052.128861
     acc:0.920189
 lv:
     loss:17174.510299
     acc:0.275228
 le:
     loss:1518.203761
     acc:0.999585
 encoder:
     loss:0.169866
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17582.003027
     acc:0.819699
 tv:
     loss:19125.954688
     acc:0.485197
 lu:
     loss:13422.171826
     acc:0.912213
 lv:
     loss:16743.822949
     acc:0.189478
 le:
     loss:1448.924097
     acc:0.999006
 encoder:
     loss:0.143537
----------------------------


Epoch: [381/1000]:
train:
----------------------------
 tu:
     loss:18447.939828
     acc:0.821339
 tv:
     loss:19908.917583
     acc:0.520669
 lu:
     loss:14052.285156
     acc:0.920312
 lv:
     loss:17176.485624
     acc:0.274968
 le:
     loss:1518.191554
     acc:0.999577
 encoder:
     loss:0.125817
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17581.948437
     acc:0.819698
 tv:
     loss:19126.497559
     acc:0.485323
 lu:
     loss:13421.546582
     acc:0.912315
 lv:
     loss:16741.203516
     acc:0.190453
 le:
     loss:1448.793774
     acc:0.999087
 encoder:
     loss:0.148752
----------------------------


Epoch: [382/1000]:
train:
----------------------------
 tu:
     loss:18448.607683
     acc:0.821393
 tv:
     loss:19908.852437
     acc:0.520398
 lu:
     loss:14052.157158
     acc:0.920154
 lv:
     loss:17177.177519
     acc:0.274438
 le:
     loss:1518.210905
     acc:0.999569
 encoder:
     loss:0.140231
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17582.066699
     acc:0.819633
 tv:
     loss:19128.405664
     acc:0.485133
 lu:
     loss:13422.257031
     acc:0.912182
 lv:
     loss:16743.752441
     acc:0.189805
 le:
     loss:1449.459473
     acc:0.998922
 encoder:
     loss:0.127475
----------------------------


Epoch: [383/1000]:
train:
----------------------------
 tu:
     loss:18448.361067
     acc:0.821311
 tv:
     loss:19907.373342
     acc:0.520962
 lu:
     loss:14052.186523
     acc:0.920257
 lv:
     loss:17176.356650
     acc:0.274622
 le:
     loss:1518.210600
     acc:0.999571
 encoder:
     loss:0.123100
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17581.939160
     acc:0.819680
 tv:
     loss:19124.594727
     acc:0.485835
 lu:
     loss:13421.060498
     acc:0.912309
 lv:
     loss:16748.853809
     acc:0.189113
 le:
     loss:1448.502972
     acc:0.999149
 encoder:
     loss:0.160924
----------------------------


Epoch: [384/1000]:
train:
----------------------------
 tu:
     loss:18448.249023
     acc:0.821422
 tv:
     loss:19908.047011
     acc:0.520778
 lu:
     loss:14052.206861
     acc:0.920203
 lv:
     loss:17170.087550
     acc:0.276358
 le:
     loss:1518.194259
     acc:0.999573
 encoder:
     loss:0.138627
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17582.026074
     acc:0.819733
 tv:
     loss:19127.407813
     acc:0.485274
 lu:
     loss:13420.348877
     acc:0.912477
 lv:
     loss:16742.744922
     acc:0.190357
 le:
     loss:1448.850635
     acc:0.999087
 encoder:
     loss:0.127854
----------------------------


Epoch: [385/1000]:
train:
----------------------------
 tu:
     loss:18448.354447
     acc:0.821323
 tv:
     loss:19906.899289
     acc:0.520968
 lu:
     loss:14052.860272
     acc:0.920037
 lv:
     loss:17175.566145
     acc:0.275027
 le:
     loss:1518.272177
     acc:0.999550
 encoder:
     loss:0.159126
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17582.234961
     acc:0.819595
 tv:
     loss:19125.855371
     acc:0.485597
 lu:
     loss:13420.760498
     acc:0.912374
 lv:
     loss:16743.338965
     acc:0.190372
 le:
     loss:1448.810718
     acc:0.999108
 encoder:
     loss:0.255513
----------------------------


Epoch: [386/1000]:
train:
----------------------------
 tu:
     loss:18447.719613
     acc:0.821358
 tv:
     loss:19906.554245
     acc:0.520948
 lu:
     loss:14051.770542
     acc:0.920328
 lv:
     loss:17175.597179
     acc:0.275146
 le:
     loss:1518.249929
     acc:0.999565
 encoder:
     loss:0.152270
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17581.689453
     acc:0.819720
 tv:
     loss:19127.298145
     acc:0.485116
 lu:
     loss:13421.090576
     acc:0.912295
 lv:
     loss:16742.360156
     acc:0.190811
 le:
     loss:1449.001703
     acc:0.999044
 encoder:
     loss:0.101125
----------------------------


Epoch: [387/1000]:
train:
----------------------------
 tu:
     loss:18447.843489
     acc:0.821409
 tv:
     loss:19906.786473
     acc:0.521162
 lu:
     loss:14051.213356
     acc:0.920388
 lv:
     loss:17173.087641
     acc:0.275649
 le:
     loss:1518.253957
     acc:0.999567
 encoder:
     loss:0.119498
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17582.422266
     acc:0.819562
 tv:
     loss:19132.048145
     acc:0.484024
 lu:
     loss:13419.230469
     acc:0.912999
 lv:
     loss:16739.789160
     acc:0.191022
 le:
     loss:1449.132648
     acc:0.999003
 encoder:
     loss:0.159581
----------------------------


Epoch: [388/1000]:
train:
----------------------------
 tu:
     loss:18448.195154
     acc:0.821277
 tv:
     loss:19906.772824
     acc:0.520917
 lu:
     loss:14051.577001
     acc:0.920445
 lv:
     loss:17171.905625
     acc:0.276060
 le:
     loss:1518.166421
     acc:0.999581
 encoder:
     loss:0.252407
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17581.700391
     acc:0.819745
 tv:
     loss:19125.919336
     acc:0.485677
 lu:
     loss:13420.388086
     acc:0.912419
 lv:
     loss:16740.369531
     acc:0.191421
 le:
     loss:1448.828271
     acc:0.999046
 encoder:
     loss:0.247284
----------------------------


Epoch: [389/1000]:
train:
----------------------------
 tu:
     loss:18447.998853
     acc:0.821352
 tv:
     loss:19905.747615
     acc:0.521393
 lu:
     loss:14052.507812
     acc:0.920083
 lv:
     loss:17170.149641
     acc:0.276461
 le:
     loss:1518.240967
     acc:0.999562
 encoder:
     loss:0.156675
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17581.679199
     acc:0.819820
 tv:
     loss:19128.137500
     acc:0.484813
 lu:
     loss:13419.523096
     acc:0.912774
 lv:
     loss:16740.625000
     acc:0.191802
 le:
     loss:1448.516022
     acc:0.999128
 encoder:
     loss:0.124199
----------------------------


Epoch: [390/1000]:
train:
----------------------------
 tu:
     loss:18447.992097
     acc:0.821345
 tv:
     loss:19904.672023
     acc:0.521293
 lu:
     loss:14052.579408
     acc:0.920092
 lv:
     loss:17168.853788
     acc:0.276454
 le:
     loss:1518.219684
     acc:0.999572
 encoder:
     loss:0.110924
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17581.982129
     acc:0.819580
 tv:
     loss:19128.414258
     acc:0.484909
 lu:
     loss:13421.032080
     acc:0.912405
 lv:
     loss:16746.601562
     acc:0.189169
 le:
     loss:1448.713464
     acc:0.999086
 encoder:
     loss:0.105161
----------------------------


Epoch: [391/1000]:
train:
----------------------------
 tu:
     loss:18447.714321
     acc:0.821384
 tv:
     loss:19905.226472
     acc:0.521469
 lu:
     loss:14052.939692
     acc:0.920000
 lv:
     loss:17172.244879
     acc:0.275877
 le:
     loss:1518.234923
     acc:0.999563
 encoder:
     loss:0.120953
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17582.252930
     acc:0.819552
 tv:
     loss:19129.879492
     acc:0.484288
 lu:
     loss:13420.135156
     acc:0.912653
 lv:
     loss:16740.061133
     acc:0.191256
 le:
     loss:1449.134283
     acc:0.999023
 encoder:
     loss:0.115464
----------------------------


Epoch: [392/1000]:
train:
----------------------------
 tu:
     loss:18448.293332
     acc:0.821307
 tv:
     loss:19903.505610
     acc:0.521498
 lu:
     loss:14053.459427
     acc:0.919785
 lv:
     loss:17165.507449
     acc:0.277206
 le:
     loss:1518.199253
     acc:0.999579
 encoder:
     loss:0.149708
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17581.975000
     acc:0.819745
 tv:
     loss:19126.542578
     acc:0.485431
 lu:
     loss:13419.202588
     acc:0.912930
 lv:
     loss:16739.830176
     acc:0.191681
 le:
     loss:1449.145508
     acc:0.999005
 encoder:
     loss:0.324492
----------------------------


Epoch: [393/1000]:
train:
----------------------------
 tu:
     loss:18447.293616
     acc:0.821576
 tv:
     loss:19904.690441
     acc:0.521388
 lu:
     loss:14051.792163
     acc:0.920256
 lv:
     loss:17168.966229
     acc:0.276410
 le:
     loss:1518.266975
     acc:0.999561
 encoder:
     loss:0.245389
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17582.146875
     acc:0.819637
 tv:
     loss:19127.498340
     acc:0.485103
 lu:
     loss:13420.399414
     acc:0.912694
 lv:
     loss:16738.927637
     acc:0.191334
 le:
     loss:1448.743903
     acc:0.999086
 encoder:
     loss:0.181274
----------------------------


Epoch: [394/1000]:
train:
----------------------------
 tu:
     loss:18448.365178
     acc:0.821201
 tv:
     loss:19904.262729
     acc:0.521600
 lu:
     loss:14051.947595
     acc:0.920220
 lv:
     loss:17167.232899
     acc:0.277080
 le:
     loss:1518.175912
     acc:0.999587
 encoder:
     loss:0.142562
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17581.373730
     acc:0.819877
 tv:
     loss:19127.385059
     acc:0.485122
 lu:
     loss:13419.926123
     acc:0.912663
 lv:
     loss:16738.966797
     acc:0.191423
 le:
     loss:1448.733795
     acc:0.999107
 encoder:
     loss:0.110924
----------------------------


Epoch: [395/1000]:
train:
----------------------------
 tu:
     loss:18447.444302
     acc:0.821423
 tv:
     loss:19903.490859
     acc:0.521632
 lu:
     loss:14051.182379
     acc:0.920392
 lv:
     loss:17164.644974
     acc:0.277294
 le:
     loss:1518.257808
     acc:0.999570
 encoder:
     loss:0.153138
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17582.005566
     acc:0.819698
 tv:
     loss:19129.330859
     acc:0.484569
 lu:
     loss:13421.258105
     acc:0.912224
 lv:
     loss:16741.918457
     acc:0.190728
 le:
     loss:1449.039587
     acc:0.998963
 encoder:
     loss:0.115972
----------------------------


Epoch: [396/1000]:
train:
----------------------------
 tu:
     loss:18448.098338
     acc:0.821261
 tv:
     loss:19903.624001
     acc:0.521687
 lu:
     loss:14050.781784
     acc:0.920567
 lv:
     loss:17165.896689
     acc:0.277233
 le:
     loss:1518.282245
     acc:0.999555
 encoder:
     loss:0.155633
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17581.850000
     acc:0.819761
 tv:
     loss:19133.959766
     acc:0.483301
 lu:
     loss:13421.134961
     acc:0.912350
 lv:
     loss:16741.133203
     acc:0.190862
 le:
     loss:1448.701349
     acc:0.999127
 encoder:
     loss:0.443367
----------------------------


Epoch: [397/1000]:
train:
----------------------------
 tu:
     loss:18447.436092
     acc:0.821498
 tv:
     loss:19901.698492
     acc:0.521924
 lu:
     loss:14050.776469
     acc:0.920487
 lv:
     loss:17164.237543
     acc:0.277392
 le:
     loss:1518.337008
     acc:0.999543
 encoder:
     loss:0.542173
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17582.778027
     acc:0.819453
 tv:
     loss:19129.809082
     acc:0.484685
 lu:
     loss:13421.072559
     acc:0.912447
 lv:
     loss:16741.857129
     acc:0.191064
 le:
     loss:1449.031903
     acc:0.999042
 encoder:
     loss:0.253784
----------------------------


Epoch: [398/1000]:
train:
----------------------------
 tu:
     loss:18433.629031
     acc:0.824443
 tv:
     loss:19902.291833
     acc:0.521846
 lu:
     loss:14050.455748
     acc:0.920568
 lv:
     loss:17162.757143
     acc:0.277789
 le:
     loss:1518.428340
     acc:0.999528
 encoder:
     loss:0.352023
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17545.852734
     acc:0.827560
 tv:
     loss:19132.540039
     acc:0.483974
 lu:
     loss:13420.308984
     acc:0.912257
 lv:
     loss:16744.289355
     acc:0.190103
 le:
     loss:1448.561774
     acc:0.999169
 encoder:
     loss:0.192112
----------------------------


Epoch: [399/1000]:
train:
----------------------------
 tu:
     loss:18412.527014
     acc:0.829004
 tv:
     loss:19900.918298
     acc:0.522370
 lu:
     loss:14051.929869
     acc:0.920051
 lv:
     loss:17167.523279
     acc:0.276598
 le:
     loss:1518.235600
     acc:0.999570
 encoder:
     loss:0.209640
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.356348
     acc:0.827494
 tv:
     loss:19127.181836
     acc:0.485274
 lu:
     loss:13421.068750
     acc:0.912238
 lv:
     loss:16744.194727
     acc:0.190016
 le:
     loss:1448.636554
     acc:0.999148
 encoder:
     loss:0.214144
----------------------------


Epoch: [400/1000]:
train:
----------------------------
 tu:
     loss:18411.568041
     acc:0.828916
 tv:
     loss:19901.249523
     acc:0.522153
 lu:
     loss:14051.695029
     acc:0.920333
 lv:
     loss:17165.003520
     acc:0.277265
 le:
     loss:1518.316998
     acc:0.999550
 encoder:
     loss:0.198995
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17545.989453
     acc:0.827597
 tv:
     loss:19129.192285
     acc:0.485037
 lu:
     loss:13421.574609
     acc:0.912389
 lv:
     loss:16742.426758
     acc:0.191029
 le:
     loss:1448.750861
     acc:0.999086
 encoder:
     loss:0.157324
----------------------------


Epoch: [401/1000]:
train:
----------------------------
 tu:
     loss:18412.150232
     acc:0.828906
 tv:
     loss:19902.003021
     acc:0.522057
 lu:
     loss:14051.351812
     acc:0.920314
 lv:
     loss:17165.451637
     acc:0.277278
 le:
     loss:1518.322165
     acc:0.999555
 encoder:
     loss:0.127852
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.122852
     acc:0.827496
 tv:
     loss:19127.975098
     acc:0.485062
 lu:
     loss:13419.909424
     acc:0.912777
 lv:
     loss:16741.243750
     acc:0.191570
 le:
     loss:1448.897589
     acc:0.999065
 encoder:
     loss:0.129681
----------------------------


Epoch: [402/1000]:
train:
----------------------------
 tu:
     loss:18410.836778
     acc:0.829138
 tv:
     loss:19899.592274
     acc:0.522532
 lu:
     loss:14050.237452
     acc:0.920593
 lv:
     loss:17162.837947
     acc:0.277540
 le:
     loss:1518.139401
     acc:0.999597
 encoder:
     loss:0.154277
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.252246
     acc:0.827533
 tv:
     loss:19128.864844
     acc:0.484434
 lu:
     loss:13420.634277
     acc:0.912649
 lv:
     loss:16740.996875
     acc:0.191079
 le:
     loss:1449.117651
     acc:0.999002
 encoder:
     loss:0.118479
----------------------------


Epoch: [403/1000]:
train:
----------------------------
 tu:
     loss:18410.570506
     acc:0.829143
 tv:
     loss:19900.945063
     acc:0.522268
 lu:
     loss:14050.319449
     acc:0.920559
 lv:
     loss:17163.604117
     acc:0.277714
 le:
     loss:1518.281226
     acc:0.999557
 encoder:
     loss:0.158372
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.769238
     acc:0.827310
 tv:
     loss:19131.631934
     acc:0.484237
 lu:
     loss:13420.532568
     acc:0.912599
 lv:
     loss:16742.551074
     acc:0.190007
 le:
     loss:1449.376782
     acc:0.998940
 encoder:
     loss:0.202832
----------------------------


Epoch: [404/1000]:
train:
----------------------------
 tu:
     loss:18410.505984
     acc:0.829141
 tv:
     loss:19898.865041
     acc:0.522426
 lu:
     loss:14051.548896
     acc:0.920253
 lv:
     loss:17160.143430
     acc:0.278148
 le:
     loss:1518.057921
     acc:0.999609
 encoder:
     loss:0.253593
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.710742
     acc:0.827307
 tv:
     loss:19127.754102
     acc:0.484964
 lu:
     loss:13420.191113
     acc:0.912665
 lv:
     loss:16738.817578
     acc:0.191444
 le:
     loss:1449.254236
     acc:0.999002
 encoder:
     loss:0.211565
----------------------------


Epoch: [405/1000]:
train:
----------------------------
 tu:
     loss:18410.286837
     acc:0.829132
 tv:
     loss:19900.145383
     acc:0.522355
 lu:
     loss:14051.457406
     acc:0.920267
 lv:
     loss:17159.638115
     acc:0.278455
 le:
     loss:1518.111821
     acc:0.999596
 encoder:
     loss:0.145298
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.061914
     acc:0.827413
 tv:
     loss:19124.419727
     acc:0.485390
 lu:
     loss:13420.950781
     acc:0.912294
 lv:
     loss:16744.903223
     acc:0.190209
 le:
     loss:1448.821643
     acc:0.999065
 encoder:
     loss:0.107538
----------------------------


Epoch: [406/1000]:
train:
----------------------------
 tu:
     loss:18410.490144
     acc:0.829128
 tv:
     loss:19898.273074
     acc:0.522657
 lu:
     loss:14051.453829
     acc:0.920313
 lv:
     loss:17160.497536
     acc:0.278120
 le:
     loss:1518.178511
     acc:0.999584
 encoder:
     loss:0.130164
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.006055
     acc:0.827470
 tv:
     loss:19130.218359
     acc:0.484390
 lu:
     loss:13419.686475
     acc:0.912713
 lv:
     loss:16738.482031
     acc:0.192294
 le:
     loss:1448.259375
     acc:0.999189
 encoder:
     loss:0.185287
----------------------------


Epoch: [407/1000]:
train:
----------------------------
 tu:
     loss:18411.016170
     acc:0.829106
 tv:
     loss:19899.629974
     acc:0.522456
 lu:
     loss:14051.706804
     acc:0.920295
 lv:
     loss:17159.318484
     acc:0.278290
 le:
     loss:1518.332111
     acc:0.999545
 encoder:
     loss:0.153247
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.839941
     acc:0.827269
 tv:
     loss:19130.891406
     acc:0.484294
 lu:
     loss:13418.982520
     acc:0.912838
 lv:
     loss:16747.492090
     acc:0.189534
 le:
     loss:1448.956012
     acc:0.999028
 encoder:
     loss:0.235364
----------------------------


Epoch: [408/1000]:
train:
----------------------------
 tu:
     loss:18411.066486
     acc:0.829043
 tv:
     loss:19900.392453
     acc:0.522120
 lu:
     loss:14049.636355
     acc:0.920651
 lv:
     loss:17160.046353
     acc:0.278360
 le:
     loss:1518.187149
     acc:0.999588
 encoder:
     loss:0.194444
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.463867
     acc:0.827356
 tv:
     loss:19129.046191
     acc:0.484579
 lu:
     loss:13420.745264
     acc:0.912447
 lv:
     loss:16741.845996
     acc:0.190532
 le:
     loss:1449.147034
     acc:0.999025
 encoder:
     loss:0.164393
----------------------------


Epoch: [409/1000]:
train:
----------------------------
 tu:
     loss:18410.199310
     acc:0.829240
 tv:
     loss:19898.805142
     acc:0.522395
 lu:
     loss:14050.840945
     acc:0.920389
 lv:
     loss:17156.990632
     acc:0.278807
 le:
     loss:1518.103318
     acc:0.999589
 encoder:
     loss:0.156516
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17545.561328
     acc:0.827621
 tv:
     loss:19130.835059
     acc:0.484251
 lu:
     loss:13416.693701
     acc:0.913441
 lv:
     loss:16742.404492
     acc:0.190547
 le:
     loss:1448.899512
     acc:0.999102
 encoder:
     loss:0.258685
----------------------------


Epoch: [410/1000]:
train:
----------------------------
 tu:
     loss:18409.131825
     acc:0.829467
 tv:
     loss:19899.344545
     acc:0.522459
 lu:
     loss:14049.564362
     acc:0.920507
 lv:
     loss:17158.327262
     acc:0.278468
 le:
     loss:1518.126130
     acc:0.999594
 encoder:
     loss:0.168936
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.033301
     acc:0.827532
 tv:
     loss:19130.161230
     acc:0.484230
 lu:
     loss:13418.920752
     acc:0.913044
 lv:
     loss:16744.199219
     acc:0.190247
 le:
     loss:1449.111823
     acc:0.998983
 encoder:
     loss:0.110117
----------------------------


Epoch: [411/1000]:
train:
----------------------------
 tu:
     loss:18409.578670
     acc:0.829248
 tv:
     loss:19898.111067
     acc:0.522763
 lu:
     loss:14049.466751
     acc:0.920773
 lv:
     loss:17154.954124
     acc:0.279504
 le:
     loss:1518.258305
     acc:0.999556
 encoder:
     loss:0.215127
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.065820
     acc:0.827395
 tv:
     loss:19126.230957
     acc:0.485275
 lu:
     loss:13420.270605
     acc:0.912704
 lv:
     loss:16743.668848
     acc:0.191388
 le:
     loss:1448.630261
     acc:0.999108
 encoder:
     loss:0.239409
----------------------------


Epoch: [412/1000]:
train:
----------------------------
 tu:
     loss:18410.396995
     acc:0.829138
 tv:
     loss:19897.928563
     acc:0.522732
 lu:
     loss:14050.749148
     acc:0.920478
 lv:
     loss:17155.368675
     acc:0.279366
 le:
     loss:1518.136419
     acc:0.999582
 encoder:
     loss:0.220651
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.384668
     acc:0.827468
 tv:
     loss:19125.247461
     acc:0.485682
 lu:
     loss:13420.079883
     acc:0.912504
 lv:
     loss:16748.389453
     acc:0.188921
 le:
     loss:1448.919989
     acc:0.999040
 encoder:
     loss:0.175577
----------------------------


Epoch: [413/1000]:
train:
----------------------------
 tu:
     loss:18410.734750
     acc:0.829010
 tv:
     loss:19899.668071
     acc:0.522523
 lu:
     loss:14050.225064
     acc:0.920586
 lv:
     loss:17156.108183
     acc:0.279032
 le:
     loss:1518.123768
     acc:0.999591
 encoder:
     loss:0.182032
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.281348
     acc:0.827407
 tv:
     loss:19128.787988
     acc:0.484859
 lu:
     loss:13421.653223
     acc:0.912157
 lv:
     loss:16747.691699
     acc:0.189852
 le:
     loss:1448.852448
     acc:0.999081
 encoder:
     loss:0.130709
----------------------------


Epoch: [414/1000]:
train:
----------------------------
 tu:
     loss:18409.958178
     acc:0.829150
 tv:
     loss:19898.715832
     acc:0.522491
 lu:
     loss:14049.794752
     acc:0.920640
 lv:
     loss:17154.785588
     acc:0.279305
 le:
     loss:1518.179529
     acc:0.999577
 encoder:
     loss:0.145954
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.394141
     acc:0.827272
 tv:
     loss:19130.062891
     acc:0.484351
 lu:
     loss:13418.815039
     acc:0.913029
 lv:
     loss:16748.664551
     acc:0.188820
 le:
     loss:1449.040900
     acc:0.999022
 encoder:
     loss:0.115755
----------------------------


Epoch: [415/1000]:
train:
----------------------------
 tu:
     loss:18410.309411
     acc:0.829142
 tv:
     loss:19894.504463
     acc:0.523433
 lu:
     loss:14049.633006
     acc:0.920648
 lv:
     loss:17153.800804
     acc:0.279750
 le:
     loss:1518.106837
     acc:0.999602
 encoder:
     loss:0.134518
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17545.835645
     acc:0.827614
 tv:
     loss:19128.044141
     acc:0.484717
 lu:
     loss:13419.815771
     acc:0.912724
 lv:
     loss:16744.827539
     acc:0.190066
 le:
     loss:1449.155811
     acc:0.999002
 encoder:
     loss:0.176502
----------------------------


Epoch: [416/1000]:
train:
----------------------------
 tu:
     loss:18409.852902
     acc:0.829274
 tv:
     loss:19897.032227
     acc:0.522979
 lu:
     loss:14048.837425
     acc:0.920821
 lv:
     loss:17153.183242
     acc:0.279728
 le:
     loss:1518.085750
     acc:0.999601
 encoder:
     loss:0.213096
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17545.871484
     acc:0.827537
 tv:
     loss:19124.272852
     acc:0.485753
 lu:
     loss:13419.431934
     acc:0.912697
 lv:
     loss:16749.394043
     acc:0.189564
 le:
     loss:1448.798944
     acc:0.999084
 encoder:
     loss:0.232331
----------------------------


Epoch: [417/1000]:
train:
----------------------------
 tu:
     loss:18409.629633
     acc:0.829324
 tv:
     loss:19897.844590
     acc:0.522776
 lu:
     loss:14049.939805
     acc:0.920623
 lv:
     loss:17153.385356
     acc:0.279734
 le:
     loss:1518.237980
     acc:0.999570
 encoder:
     loss:0.171885
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17545.780176
     acc:0.827498
 tv:
     loss:19130.216016
     acc:0.484885
 lu:
     loss:13420.024268
     acc:0.912403
 lv:
     loss:16743.740723
     acc:0.190973
 le:
     loss:1448.724670
     acc:0.999085
 encoder:
     loss:0.188910
----------------------------


Epoch: [418/1000]:
train:
----------------------------
 tu:
     loss:18409.522302
     acc:0.829273
 tv:
     loss:19896.425565
     acc:0.522927
 lu:
     loss:14049.625409
     acc:0.920665
 lv:
     loss:17152.293979
     acc:0.279961
 le:
     loss:1518.154146
     acc:0.999583
 encoder:
     loss:0.157854
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17545.951270
     acc:0.827529
 tv:
     loss:19132.760937
     acc:0.483449
 lu:
     loss:13419.368848
     acc:0.912606
 lv:
     loss:16746.668750
     acc:0.190087
 le:
     loss:1448.632977
     acc:0.999107
 encoder:
     loss:0.161543
----------------------------


Epoch: [419/1000]:
train:
----------------------------
 tu:
     loss:18409.868948
     acc:0.829220
 tv:
     loss:19896.711483
     acc:0.523053
 lu:
     loss:14050.182379
     acc:0.920501
 lv:
     loss:17151.932197
     acc:0.279797
 le:
     loss:1518.176785
     acc:0.999585
 encoder:
     loss:0.171999
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17545.913770
     acc:0.827516
 tv:
     loss:19128.531055
     acc:0.485062
 lu:
     loss:13419.905859
     acc:0.912685
 lv:
     loss:16744.645801
     acc:0.190248
 le:
     loss:1448.613354
     acc:0.999126
 encoder:
     loss:0.146931
----------------------------


Epoch: [420/1000]:
train:
----------------------------
 tu:
     loss:18410.280251
     acc:0.829095
 tv:
     loss:19896.502260
     acc:0.522982
 lu:
     loss:14049.316679
     acc:0.920682
 lv:
     loss:17151.555812
     acc:0.280111
 le:
     loss:1518.228768
     acc:0.999557
 encoder:
     loss:0.221075
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17545.809180
     acc:0.827535
 tv:
     loss:19128.446680
     acc:0.484791
 lu:
     loss:13418.305225
     acc:0.912709
 lv:
     loss:16741.747266
     acc:0.190608
 le:
     loss:1448.662750
     acc:0.999162
 encoder:
     loss:0.379474
----------------------------


Epoch: [421/1000]:
train:
----------------------------
 tu:
     loss:18409.807015
     acc:0.829327
 tv:
     loss:19894.598213
     acc:0.523408
 lu:
     loss:14049.604435
     acc:0.920568
 lv:
     loss:17149.882108
     acc:0.280260
 le:
     loss:1518.123393
     acc:0.999589
 encoder:
     loss:0.209505
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.099707
     acc:0.827524
 tv:
     loss:19128.589844
     acc:0.484722
 lu:
     loss:13418.230762
     acc:0.913052
 lv:
     loss:16740.226953
     acc:0.191419
 le:
     loss:1448.570001
     acc:0.999107
 encoder:
     loss:0.136957
----------------------------


Epoch: [422/1000]:
train:
----------------------------
 tu:
     loss:18409.943689
     acc:0.829137
 tv:
     loss:19894.105775
     acc:0.523550
 lu:
     loss:14049.913688
     acc:0.920653
 lv:
     loss:17151.137218
     acc:0.280298
 le:
     loss:1518.250979
     acc:0.999566
 encoder:
     loss:0.144335
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.199023
     acc:0.827491
 tv:
     loss:19127.430664
     acc:0.484762
 lu:
     loss:13417.714648
     acc:0.913155
 lv:
     loss:16743.504102
     acc:0.190096
 le:
     loss:1448.547131
     acc:0.999122
 encoder:
     loss:0.140682
----------------------------


Epoch: [423/1000]:
train:
----------------------------
 tu:
     loss:18410.050304
     acc:0.829149
 tv:
     loss:19893.960699
     acc:0.523351
 lu:
     loss:14050.087561
     acc:0.920429
 lv:
     loss:17148.838754
     acc:0.280421
 le:
     loss:1518.204499
     acc:0.999571
 encoder:
     loss:0.192212
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.207129
     acc:0.827530
 tv:
     loss:19129.915527
     acc:0.484098
 lu:
     loss:13421.185352
     acc:0.912317
 lv:
     loss:16740.208203
     acc:0.191068
 le:
     loss:1448.856915
     acc:0.999080
 encoder:
     loss:0.172754
----------------------------


Epoch: [424/1000]:
train:
----------------------------
 tu:
     loss:18409.581282
     acc:0.829171
 tv:
     loss:19896.842592
     acc:0.523007
 lu:
     loss:14049.849712
     acc:0.920528
 lv:
     loss:17149.887922
     acc:0.280239
 le:
     loss:1518.151535
     acc:0.999585
 encoder:
     loss:0.152667
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17545.826367
     acc:0.827443
 tv:
     loss:19128.171875
     acc:0.485004
 lu:
     loss:13419.315088
     acc:0.912611
 lv:
     loss:16742.713477
     acc:0.190760
 le:
     loss:1448.787469
     acc:0.999084
 encoder:
     loss:0.144580
----------------------------


Epoch: [425/1000]:
train:
----------------------------
 tu:
     loss:18409.582792
     acc:0.829109
 tv:
     loss:19892.466195
     acc:0.524043
 lu:
     loss:14049.833065
     acc:0.920591
 lv:
     loss:17147.558367
     acc:0.280999
 le:
     loss:1518.155122
     acc:0.999587
 encoder:
     loss:0.159293
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17545.903418
     acc:0.827368
 tv:
     loss:19128.354492
     acc:0.485260
 lu:
     loss:13420.860059
     acc:0.912452
 lv:
     loss:16742.167773
     acc:0.190857
 le:
     loss:1448.761133
     acc:0.999065
 encoder:
     loss:0.133513
----------------------------


Epoch: [426/1000]:
train:
----------------------------
 tu:
     loss:18409.709938
     acc:0.829260
 tv:
     loss:19893.893839
     acc:0.523602
 lu:
     loss:14049.356888
     acc:0.920659
 lv:
     loss:17148.501272
     acc:0.280755
 le:
     loss:1518.279131
     acc:0.999557
 encoder:
     loss:0.196371
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17545.608008
     acc:0.827556
 tv:
     loss:19132.190527
     acc:0.484157
 lu:
     loss:13419.253076
     acc:0.912677
 lv:
     loss:16748.437891
     acc:0.189482
 le:
     loss:1448.634424
     acc:0.999127
 encoder:
     loss:0.264856
----------------------------


Epoch: [427/1000]:
train:
----------------------------
 tu:
     loss:18409.467751
     acc:0.829235
 tv:
     loss:19892.767442
     acc:0.523892
 lu:
     loss:14049.015432
     acc:0.920761
 lv:
     loss:17147.924339
     acc:0.280777
 le:
     loss:1518.177073
     acc:0.999587
 encoder:
     loss:0.285601
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.095508
     acc:0.827471
 tv:
     loss:19128.816992
     acc:0.484625
 lu:
     loss:13422.383740
     acc:0.912468
 lv:
     loss:16746.810547
     acc:0.189823
 le:
     loss:1448.574707
     acc:0.999127
 encoder:
     loss:0.310496
----------------------------


Epoch: [428/1000]:
train:
----------------------------
 tu:
     loss:18408.476574
     acc:0.829540
 tv:
     loss:19894.229549
     acc:0.523486
 lu:
     loss:14050.188454
     acc:0.920494
 lv:
     loss:17146.387854
     acc:0.281180
 le:
     loss:1518.177253
     acc:0.999580
 encoder:
     loss:0.245442
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.170898
     acc:0.827388
 tv:
     loss:19128.780273
     acc:0.485271
 lu:
     loss:13421.661426
     acc:0.912210
 lv:
     loss:16738.086914
     acc:0.191780
 le:
     loss:1448.445734
     acc:0.999188
 encoder:
     loss:0.179549
----------------------------


Epoch: [429/1000]:
train:
----------------------------
 tu:
     loss:18408.892669
     acc:0.829383
 tv:
     loss:19891.503180
     acc:0.523909
 lu:
     loss:14049.732467
     acc:0.920545
 lv:
     loss:17145.504224
     acc:0.281220
 le:
     loss:1518.124363
     acc:0.999580
 encoder:
     loss:0.225492
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.014355
     acc:0.827495
 tv:
     loss:19130.496582
     acc:0.484747
 lu:
     loss:13422.979932
     acc:0.911764
 lv:
     loss:16747.787891
     acc:0.189411
 le:
     loss:1448.684460
     acc:0.999107
 encoder:
     loss:0.170994
----------------------------


Epoch: [430/1000]:
train:
----------------------------
 tu:
     loss:18409.755155
     acc:0.829090
 tv:
     loss:19892.748558
     acc:0.523778
 lu:
     loss:14049.503634
     acc:0.920595
 lv:
     loss:17144.873058
     acc:0.281529
 le:
     loss:1518.156625
     acc:0.999584
 encoder:
     loss:0.169106
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.450000
     acc:0.827525
 tv:
     loss:19130.441992
     acc:0.484359
 lu:
     loss:13422.254932
     acc:0.912266
 lv:
     loss:16740.399609
     acc:0.190772
 le:
     loss:1448.869989
     acc:0.999043
 encoder:
     loss:0.166932
----------------------------


Epoch: [431/1000]:
train:
----------------------------
 tu:
     loss:18409.616915
     acc:0.829158
 tv:
     loss:19893.196584
     acc:0.523444
 lu:
     loss:14049.699628
     acc:0.920601
 lv:
     loss:17147.478152
     acc:0.280802
 le:
     loss:1518.114468
     acc:0.999587
 encoder:
     loss:0.150823
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.215918
     acc:0.827549
 tv:
     loss:19129.131445
     acc:0.484836
 lu:
     loss:13419.128125
     acc:0.912847
 lv:
     loss:16742.894727
     acc:0.190987
 le:
     loss:1449.005975
     acc:0.999025
 encoder:
     loss:0.164770
----------------------------


Epoch: [432/1000]:
train:
----------------------------
 tu:
     loss:18409.277889
     acc:0.829246
 tv:
     loss:19893.200139
     acc:0.523535
 lu:
     loss:14049.717126
     acc:0.920642
 lv:
     loss:17145.247207
     acc:0.281334
 le:
     loss:1518.111280
     acc:0.999585
 encoder:
     loss:0.154696
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.592090
     acc:0.827368
 tv:
     loss:19132.099902
     acc:0.483933
 lu:
     loss:13420.393945
     acc:0.912717
 lv:
     loss:16739.617578
     acc:0.191645
 le:
     loss:1448.932129
     acc:0.999103
 encoder:
     loss:0.130028
----------------------------


Epoch: [433/1000]:
train:
----------------------------
 tu:
     loss:18409.534782
     acc:0.829179
 tv:
     loss:19890.813295
     acc:0.524386
 lu:
     loss:14049.470340
     acc:0.920716
 lv:
     loss:17142.510742
     acc:0.282084
 le:
     loss:1518.105898
     acc:0.999594
 encoder:
     loss:0.166057
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17545.925098
     acc:0.827529
 tv:
     loss:19134.804492
     acc:0.483396
 lu:
     loss:13420.735107
     acc:0.912756
 lv:
     loss:16738.812012
     acc:0.191741
 le:
     loss:1448.573962
     acc:0.999146
 encoder:
     loss:0.134000
----------------------------


Epoch: [434/1000]:
train:
----------------------------
 tu:
     loss:18410.464401
     acc:0.828984
 tv:
     loss:19891.922772
     acc:0.523731
 lu:
     loss:14049.017101
     acc:0.920702
 lv:
     loss:17142.959791
     acc:0.281824
 le:
     loss:1518.098971
     acc:0.999601
 encoder:
     loss:0.133166
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.157617
     acc:0.827492
 tv:
     loss:19130.638086
     acc:0.484192
 lu:
     loss:13419.963965
     acc:0.912665
 lv:
     loss:16742.552441
     acc:0.190783
 le:
     loss:1448.555792
     acc:0.999126
 encoder:
     loss:0.123222
----------------------------


Epoch: [435/1000]:
train:
----------------------------
 tu:
     loss:18409.199457
     acc:0.829195
 tv:
     loss:19891.898403
     acc:0.523984
 lu:
     loss:14048.697777
     acc:0.920812
 lv:
     loss:17143.249625
     acc:0.281795
 le:
     loss:1518.152517
     acc:0.999596
 encoder:
     loss:0.162619
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.268262
     acc:0.827589
 tv:
     loss:19131.513867
     acc:0.484222
 lu:
     loss:13420.832227
     acc:0.912363
 lv:
     loss:16743.771191
     acc:0.190706
 le:
     loss:1448.465308
     acc:0.999147
 encoder:
     loss:0.173432
----------------------------


Epoch: [436/1000]:
train:
----------------------------
 tu:
     loss:18409.148188
     acc:0.829270
 tv:
     loss:19893.056572
     acc:0.523703
 lu:
     loss:14049.209598
     acc:0.920755
 lv:
     loss:17141.328363
     acc:0.282034
 le:
     loss:1518.194916
     acc:0.999571
 encoder:
     loss:0.236080
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17545.983496
     acc:0.827515
 tv:
     loss:19134.368848
     acc:0.483463
 lu:
     loss:13421.824609
     acc:0.912187
 lv:
     loss:16741.406445
     acc:0.190939
 le:
     loss:1448.917480
     acc:0.999022
 encoder:
     loss:0.220815
----------------------------


Epoch: [437/1000]:
train:
----------------------------
 tu:
     loss:18408.703602
     acc:0.829407
 tv:
     loss:19889.600393
     acc:0.524502
 lu:
     loss:14048.906227
     acc:0.920656
 lv:
     loss:17142.424816
     acc:0.282052
 le:
     loss:1518.146003
     acc:0.999585
 encoder:
     loss:0.210368
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.843066
     acc:0.827285
 tv:
     loss:19134.582422
     acc:0.483558
 lu:
     loss:13421.046631
     acc:0.912227
 lv:
     loss:16736.950098
     acc:0.191768
 le:
     loss:1448.628180
     acc:0.999127
 encoder:
     loss:0.338972
----------------------------


Epoch: [438/1000]:
train:
----------------------------
 tu:
     loss:18408.034940
     acc:0.829525
 tv:
     loss:19889.377782
     acc:0.524283
 lu:
     loss:14049.098269
     acc:0.920767
 lv:
     loss:17139.295024
     acc:0.282256
 le:
     loss:1518.081976
     acc:0.999610
 encoder:
     loss:0.260052
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.258203
     acc:0.827405
 tv:
     loss:19132.624609
     acc:0.483836
 lu:
     loss:13420.821631
     acc:0.912705
 lv:
     loss:16741.134570
     acc:0.190974
 le:
     loss:1448.785529
     acc:0.999084
 encoder:
     loss:0.133581
----------------------------


Epoch: [439/1000]:
train:
----------------------------
 tu:
     loss:18409.208519
     acc:0.829235
 tv:
     loss:19889.747956
     acc:0.524355
 lu:
     loss:14047.829772
     acc:0.920980
 lv:
     loss:17142.116904
     acc:0.281714
 le:
     loss:1518.093186
     acc:0.999603
 encoder:
     loss:0.253714
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.753516
     acc:0.827328
 tv:
     loss:19134.149121
     acc:0.483580
 lu:
     loss:13418.387598
     acc:0.913099
 lv:
     loss:16754.672949
     acc:0.187809
 le:
     loss:1448.893042
     acc:0.999046
 encoder:
     loss:0.407858
----------------------------


Epoch: [440/1000]:
train:
----------------------------
 tu:
     loss:18408.483523
     acc:0.829304
 tv:
     loss:19888.516590
     acc:0.524841
 lu:
     loss:14048.352641
     acc:0.920939
 lv:
     loss:17138.487191
     acc:0.283173
 le:
     loss:1518.081617
     acc:0.999604
 encoder:
     loss:0.322110
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.162305
     acc:0.827512
 tv:
     loss:19128.143750
     acc:0.484678
 lu:
     loss:13418.944873
     acc:0.912726
 lv:
     loss:16742.194531
     acc:0.190875
 le:
     loss:1448.502777
     acc:0.999148
 encoder:
     loss:0.261056
----------------------------


Epoch: [441/1000]:
train:
----------------------------
 tu:
     loss:18409.430641
     acc:0.829193
 tv:
     loss:19887.985704
     acc:0.524897
 lu:
     loss:14048.072720
     acc:0.920866
 lv:
     loss:17138.933378
     acc:0.282664
 le:
     loss:1518.075317
     acc:0.999599
 encoder:
     loss:0.235322
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.217187
     acc:0.827611
 tv:
     loss:19133.287305
     acc:0.483847
 lu:
     loss:13419.412500
     acc:0.912863
 lv:
     loss:16742.186719
     acc:0.190623
 le:
     loss:1448.983417
     acc:0.999006
 encoder:
     loss:0.140972
----------------------------


Epoch: [442/1000]:
train:
----------------------------
 tu:
     loss:18410.172011
     acc:0.829003
 tv:
     loss:19890.017976
     acc:0.524090
 lu:
     loss:14048.130087
     acc:0.920965
 lv:
     loss:17141.291720
     acc:0.282210
 le:
     loss:1518.024115
     acc:0.999617
 encoder:
     loss:0.150796
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.092969
     acc:0.827627
 tv:
     loss:19134.279785
     acc:0.483860
 lu:
     loss:13421.753223
     acc:0.912210
 lv:
     loss:16745.787500
     acc:0.189965
 le:
     loss:1448.855676
     acc:0.999104
 encoder:
     loss:0.136008
----------------------------


Epoch: [443/1000]:
train:
----------------------------
 tu:
     loss:18408.169627
     acc:0.829432
 tv:
     loss:19888.396348
     acc:0.524574
 lu:
     loss:14048.533703
     acc:0.920836
 lv:
     loss:17138.793684
     acc:0.282890
 le:
     loss:1518.051258
     acc:0.999610
 encoder:
     loss:0.143555
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.034473
     acc:0.827491
 tv:
     loss:19130.240820
     acc:0.484520
 lu:
     loss:13418.296924
     acc:0.913321
 lv:
     loss:16741.754688
     acc:0.191447
 le:
     loss:1449.044250
     acc:0.999045
 encoder:
     loss:0.213997
----------------------------


Epoch: [444/1000]:
train:
----------------------------
 tu:
     loss:18408.538029
     acc:0.829393
 tv:
     loss:19889.175123
     acc:0.524318
 lu:
     loss:14047.083803
     acc:0.921217
 lv:
     loss:17136.921705
     acc:0.282720
 le:
     loss:1518.263907
     acc:0.999561
 encoder:
     loss:0.215265
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.319824
     acc:0.827325
 tv:
     loss:19134.517188
     acc:0.483661
 lu:
     loss:13420.003125
     acc:0.912438
 lv:
     loss:16745.767383
     acc:0.189648
 le:
     loss:1448.358167
     acc:0.999229
 encoder:
     loss:0.283100
----------------------------


Epoch: [445/1000]:
train:
----------------------------
 tu:
     loss:18408.906863
     acc:0.829283
 tv:
     loss:19889.875534
     acc:0.524339
 lu:
     loss:14048.814340
     acc:0.920646
 lv:
     loss:17138.784634
     acc:0.282912
 le:
     loss:1518.103479
     acc:0.999591
 encoder:
     loss:0.290360
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.428516
     acc:0.827371
 tv:
     loss:19129.209570
     acc:0.484836
 lu:
     loss:13417.839844
     acc:0.912949
 lv:
     loss:16743.796777
     acc:0.189865
 le:
     loss:1448.676874
     acc:0.999159
 encoder:
     loss:0.187306
----------------------------


Epoch: [446/1000]:
train:
----------------------------
 tu:
     loss:18408.302712
     acc:0.829378
 tv:
     loss:19888.656999
     acc:0.524494
 lu:
     loss:14049.387945
     acc:0.920842
 lv:
     loss:17138.777310
     acc:0.282522
 le:
     loss:1518.156026
     acc:0.999587
 encoder:
     loss:0.241049
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.225977
     acc:0.827553
 tv:
     loss:19132.562598
     acc:0.483854
 lu:
     loss:13418.465918
     acc:0.912976
 lv:
     loss:16736.261523
     acc:0.192564
 le:
     loss:1448.672626
     acc:0.999105
 encoder:
     loss:0.267146
----------------------------


Epoch: [447/1000]:
train:
----------------------------
 tu:
     loss:18408.499966
     acc:0.829418
 tv:
     loss:19887.586642
     acc:0.524511
 lu:
     loss:14049.225563
     acc:0.920593
 lv:
     loss:17136.099223
     acc:0.282810
 le:
     loss:1518.092775
     acc:0.999598
 encoder:
     loss:0.223748
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17545.443848
     acc:0.827556
 tv:
     loss:19131.858398
     acc:0.483911
 lu:
     loss:13417.276270
     acc:0.913284
 lv:
     loss:16738.021680
     acc:0.192171
 le:
     loss:1448.724060
     acc:0.999080
 encoder:
     loss:0.205066
----------------------------


Epoch: [448/1000]:
train:
----------------------------
 tu:
     loss:18408.843318
     acc:0.829227
 tv:
     loss:19889.116188
     acc:0.524458
 lu:
     loss:14049.572697
     acc:0.920692
 lv:
     loss:17137.908510
     acc:0.282951
 le:
     loss:1518.142262
     acc:0.999586
 encoder:
     loss:0.314381
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.831738
     acc:0.827227
 tv:
     loss:19130.725781
     acc:0.484626
 lu:
     loss:13418.921582
     acc:0.912784
 lv:
     loss:16744.526953
     acc:0.190078
 le:
     loss:1448.472009
     acc:0.999146
 encoder:
     loss:0.282549
----------------------------


Epoch: [449/1000]:
train:
----------------------------
 tu:
     loss:18409.095737
     acc:0.829332
 tv:
     loss:19885.820437
     acc:0.525187
 lu:
     loss:14049.318257
     acc:0.920674
 lv:
     loss:17136.563340
     acc:0.283135
 le:
     loss:1518.220703
     acc:0.999573
 encoder:
     loss:0.231827
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.405371
     acc:0.827352
 tv:
     loss:19136.152539
     acc:0.482961
 lu:
     loss:13419.253076
     acc:0.912557
 lv:
     loss:16744.659766
     acc:0.190615
 le:
     loss:1448.226843
     acc:0.999227
 encoder:
     loss:0.161217
----------------------------


Epoch: [450/1000]:
train:
----------------------------
 tu:
     loss:18409.213254
     acc:0.829191
 tv:
     loss:19888.051826
     acc:0.524833
 lu:
     loss:14048.019781
     acc:0.920854
 lv:
     loss:17134.287382
     acc:0.283609
 le:
     loss:1518.059426
     acc:0.999606
 encoder:
     loss:0.139438
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.831934
     acc:0.827310
 tv:
     loss:19129.857910
     acc:0.484576
 lu:
     loss:13418.230518
     acc:0.913026
 lv:
     loss:16742.074609
     acc:0.190903
 le:
     loss:1448.686560
     acc:0.999123
 encoder:
     loss:0.123262
----------------------------


Epoch: [451/1000]:
train:
----------------------------
 tu:
     loss:18409.015954
     acc:0.829235
 tv:
     loss:19887.400345
     acc:0.524899
 lu:
     loss:14048.855344
     acc:0.920848
 lv:
     loss:17133.370310
     acc:0.284079
 le:
     loss:1518.041796
     acc:0.999613
 encoder:
     loss:0.125323
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.561719
     acc:0.827411
 tv:
     loss:19131.137500
     acc:0.484462
 lu:
     loss:13419.216162
     acc:0.912555
 lv:
     loss:16742.929492
     acc:0.190764
 le:
     loss:1449.135254
     acc:0.999003
 encoder:
     loss:0.121595
----------------------------


Epoch: [452/1000]:
train:
----------------------------
 tu:
     loss:18408.831498
     acc:0.829271
 tv:
     loss:19887.834461
     acc:0.524706
 lu:
     loss:14047.101017
     acc:0.921209
 lv:
     loss:17132.434207
     acc:0.283910
 le:
     loss:1518.035026
     acc:0.999617
 encoder:
     loss:0.293728
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.423730
     acc:0.827326
 tv:
     loss:19133.287207
     acc:0.484055
 lu:
     loss:13418.380176
     acc:0.912859
 lv:
     loss:16742.751270
     acc:0.190402
 le:
     loss:1448.877106
     acc:0.999121
 encoder:
     loss:0.253451
----------------------------


Epoch: [453/1000]:
train:
----------------------------
 tu:
     loss:18408.837811
     acc:0.829300
 tv:
     loss:19887.317133
     acc:0.524805
 lu:
     loss:14048.080101
     acc:0.920894
 lv:
     loss:17132.986158
     acc:0.283858
 le:
     loss:1518.125901
     acc:0.999594
 encoder:
     loss:0.212843
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17546.636133
     acc:0.827290
 tv:
     loss:19129.965820
     acc:0.484497
 lu:
     loss:13417.517334
     acc:0.913159
 lv:
     loss:16739.285156
     acc:0.191590
 le:
     loss:1448.604047
     acc:0.999086
 encoder:
     loss:0.280588
----------------------------


Epoch: [454/1000]:
train:
----------------------------
 tu:
     loss:18406.122297
     acc:0.829918
 tv:
     loss:19888.627123
     acc:0.524540
 lu:
     loss:14048.997842
     acc:0.920887
 lv:
     loss:17133.730446
     acc:0.283666
 le:
     loss:1518.075735
     acc:0.999588
 encoder:
     loss:0.238681
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17517.251172
     acc:0.833873
 tv:
     loss:19137.948730
     acc:0.482866
 lu:
     loss:13420.277881
     acc:0.912629
 lv:
     loss:16745.680469
     acc:0.190185
 le:
     loss:1448.637811
     acc:0.999168
 encoder:
     loss:0.151464
----------------------------


Epoch: [455/1000]:
train:
----------------------------
 tu:
     loss:18373.796841
     acc:0.836807
 tv:
     loss:19888.450048
     acc:0.524515
 lu:
     loss:14047.535838
     acc:0.920990
 lv:
     loss:17130.150538
     acc:0.284361
 le:
     loss:1518.001668
     acc:0.999617
 encoder:
     loss:0.223814
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17507.187402
     acc:0.835942
 tv:
     loss:19134.290430
     acc:0.483333
 lu:
     loss:13419.646436
     acc:0.912789
 lv:
     loss:16742.291504
     acc:0.190695
 le:
     loss:1448.639655
     acc:0.999087
 encoder:
     loss:0.170227
----------------------------


Epoch: [456/1000]:
train:
----------------------------
 tu:
     loss:18369.773585
     acc:0.837554
 tv:
     loss:19886.378725
     acc:0.525017
 lu:
     loss:14048.024005
     acc:0.920855
 lv:
     loss:17133.152707
     acc:0.284067
 le:
     loss:1518.176976
     acc:0.999576
 encoder:
     loss:0.159316
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.992676
     acc:0.836000
 tv:
     loss:19134.502832
     acc:0.483538
 lu:
     loss:13418.221826
     acc:0.913051
 lv:
     loss:16750.975391
     acc:0.188484
 le:
     loss:1448.617322
     acc:0.999107
 encoder:
     loss:0.150781
----------------------------


Epoch: [457/1000]:
train:
----------------------------
 tu:
     loss:18369.118936
     acc:0.837722
 tv:
     loss:19886.084620
     acc:0.524967
 lu:
     loss:14048.722804
     acc:0.920717
 lv:
     loss:17130.904910
     acc:0.284296
 le:
     loss:1518.035619
     acc:0.999615
 encoder:
     loss:0.212378
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.703027
     acc:0.836124
 tv:
     loss:19129.598145
     acc:0.484333
 lu:
     loss:13418.769531
     acc:0.912892
 lv:
     loss:16747.590625
     acc:0.189999
 le:
     loss:1448.024109
     acc:0.999252
 encoder:
     loss:0.187776
----------------------------


Epoch: [458/1000]:
train:
----------------------------
 tu:
     loss:18368.379576
     acc:0.837830
 tv:
     loss:19886.785327
     acc:0.524818
 lu:
     loss:14049.315714
     acc:0.920560
 lv:
     loss:17129.560047
     acc:0.284651
 le:
     loss:1518.195083
     acc:0.999581
 encoder:
     loss:0.151958
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.395117
     acc:0.836184
 tv:
     loss:19131.397070
     acc:0.484275
 lu:
     loss:13417.920068
     acc:0.913012
 lv:
     loss:16737.384863
     acc:0.191584
 le:
     loss:1448.266559
     acc:0.999169
 encoder:
     loss:0.112175
----------------------------


Epoch: [459/1000]:
train:
----------------------------
 tu:
     loss:18367.756632
     acc:0.837872
 tv:
     loss:19886.291356
     acc:0.525252
 lu:
     loss:14049.047817
     acc:0.920732
 lv:
     loss:17129.931618
     acc:0.284575
 le:
     loss:1518.079746
     acc:0.999601
 encoder:
     loss:0.153189
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17507.236816
     acc:0.835938
 tv:
     loss:19136.939746
     acc:0.483006
 lu:
     loss:13418.308887
     acc:0.912950
 lv:
     loss:16747.728711
     acc:0.189489
 le:
     loss:1448.145886
     acc:0.999229
 encoder:
     loss:0.256013
----------------------------


Epoch: [460/1000]:
train:
----------------------------
 tu:
     loss:18367.930210
     acc:0.837837
 tv:
     loss:19885.474893
     acc:0.525274
 lu:
     loss:14048.657556
     acc:0.920775
 lv:
     loss:17129.422613
     acc:0.284536
 le:
     loss:1518.090233
     acc:0.999595
 encoder:
     loss:0.197186
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17507.011230
     acc:0.836078
 tv:
     loss:19121.172559
     acc:0.486799
 lu:
     loss:13417.916846
     acc:0.912980
 lv:
     loss:16742.403320
     acc:0.190326
 le:
     loss:1448.683405
     acc:0.999064
 encoder:
     loss:0.173061
----------------------------


Epoch: [461/1000]:
train:
----------------------------
 tu:
     loss:18368.203613
     acc:0.837685
 tv:
     loss:19871.559320
     acc:0.528367
 lu:
     loss:14047.752544
     acc:0.920836
 lv:
     loss:17132.179903
     acc:0.284371
 le:
     loss:1518.203757
     acc:0.999579
 encoder:
     loss:0.151790
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.400000
     acc:0.836181
 tv:
     loss:19115.694824
     acc:0.487907
 lu:
     loss:13419.149902
     acc:0.912589
 lv:
     loss:16742.257422
     acc:0.191172
 le:
     loss:1448.623993
     acc:0.999107
 encoder:
     loss:0.142647
----------------------------


Epoch: [462/1000]:
train:
----------------------------
 tu:
     loss:18367.788926
     acc:0.837882
 tv:
     loss:19869.133426
     acc:0.528804
 lu:
     loss:14048.519020
     acc:0.920777
 lv:
     loss:17129.011719
     acc:0.284688
 le:
     loss:1518.104924
     acc:0.999603
 encoder:
     loss:0.143029
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.557031
     acc:0.836102
 tv:
     loss:19117.650586
     acc:0.487391
 lu:
     loss:13417.464600
     acc:0.913212
 lv:
     loss:16745.049219
     acc:0.190389
 le:
     loss:1448.654614
     acc:0.999121
 encoder:
     loss:0.120458
----------------------------


Epoch: [463/1000]:
train:
----------------------------
 tu:
     loss:18366.989326
     acc:0.837963
 tv:
     loss:19868.760356
     acc:0.528801
 lu:
     loss:14047.722077
     acc:0.920902
 lv:
     loss:17124.285622
     acc:0.285927
 le:
     loss:1518.171797
     acc:0.999582
 encoder:
     loss:0.167722
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17507.219629
     acc:0.835875
 tv:
     loss:19119.036230
     acc:0.486828
 lu:
     loss:13418.847607
     acc:0.913006
 lv:
     loss:16746.278418
     acc:0.189875
 le:
     loss:1448.434204
     acc:0.999166
 encoder:
     loss:0.145574
----------------------------


Epoch: [464/1000]:
train:
----------------------------
 tu:
     loss:18367.305062
     acc:0.837902
 tv:
     loss:19867.335279
     acc:0.529052
 lu:
     loss:14046.771359
     acc:0.921253
 lv:
     loss:17127.150788
     acc:0.285421
 le:
     loss:1518.211402
     acc:0.999578
 encoder:
     loss:0.141488
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17507.257520
     acc:0.836000
 tv:
     loss:19118.628418
     acc:0.487140
 lu:
     loss:13419.649072
     acc:0.912861
 lv:
     loss:16747.381934
     acc:0.189416
 le:
     loss:1448.882971
     acc:0.999044
 encoder:
     loss:0.146255
----------------------------


Epoch: [465/1000]:
train:
----------------------------
 tu:
     loss:18367.106797
     acc:0.837981
 tv:
     loss:19867.519531
     acc:0.529160
 lu:
     loss:14046.216524
     acc:0.921454
 lv:
     loss:17125.962153
     acc:0.285416
 le:
     loss:1518.221323
     acc:0.999563
 encoder:
     loss:0.143562
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.522559
     acc:0.836140
 tv:
     loss:19117.592676
     acc:0.487273
 lu:
     loss:13420.411572
     acc:0.912607
 lv:
     loss:16741.526562
     acc:0.190629
 le:
     loss:1449.186694
     acc:0.998982
 encoder:
     loss:0.179468
----------------------------


Epoch: [466/1000]:
train:
----------------------------
 tu:
     loss:18366.931652
     acc:0.837981
 tv:
     loss:19865.895008
     acc:0.529169
 lu:
     loss:14047.279615
     acc:0.921093
 lv:
     loss:17126.746798
     acc:0.285035
 le:
     loss:1518.265560
     acc:0.999560
 encoder:
     loss:0.163412
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.800098
     acc:0.836163
 tv:
     loss:19119.169238
     acc:0.487058
 lu:
     loss:13419.501172
     acc:0.912823
 lv:
     loss:16746.624023
     acc:0.189210
 le:
     loss:1448.802509
     acc:0.999065
 encoder:
     loss:0.139022
----------------------------


Epoch: [467/1000]:
train:
----------------------------
 tu:
     loss:18365.876578
     acc:0.838260
 tv:
     loss:19866.839117
     acc:0.529080
 lu:
     loss:14047.494970
     acc:0.921017
 lv:
     loss:17126.056232
     acc:0.285344
 le:
     loss:1518.162129
     acc:0.999576
 encoder:
     loss:0.127533
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.858203
     acc:0.835918
 tv:
     loss:19120.012695
     acc:0.486598
 lu:
     loss:13421.399023
     acc:0.912331
 lv:
     loss:16746.800293
     acc:0.189363
 le:
     loss:1448.903802
     acc:0.999084
 encoder:
     loss:0.130361
----------------------------


Epoch: [468/1000]:
train:
----------------------------
 tu:
     loss:18366.710154
     acc:0.838033
 tv:
     loss:19868.769588
     acc:0.528694
 lu:
     loss:14047.676270
     acc:0.920925
 lv:
     loss:17126.636321
     acc:0.285337
 le:
     loss:1518.046560
     acc:0.999605
 encoder:
     loss:0.173258
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.519727
     acc:0.836203
 tv:
     loss:19118.297266
     acc:0.486755
 lu:
     loss:13419.221680
     acc:0.912969
 lv:
     loss:16745.620312
     acc:0.189550
 le:
     loss:1448.709534
     acc:0.999106
 encoder:
     loss:0.166640
----------------------------


Epoch: [469/1000]:
train:
----------------------------
 tu:
     loss:18366.936773
     acc:0.837814
 tv:
     loss:19867.343353
     acc:0.529043
 lu:
     loss:14046.316588
     acc:0.921251
 lv:
     loss:17125.217240
     acc:0.285594
 le:
     loss:1518.026197
     acc:0.999615
 encoder:
     loss:0.153744
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.953223
     acc:0.836044
 tv:
     loss:19118.460254
     acc:0.486728
 lu:
     loss:13418.014258
     acc:0.913073
 lv:
     loss:16750.490723
     acc:0.189012
 le:
     loss:1448.534564
     acc:0.999145
 encoder:
     loss:0.145833
----------------------------


Epoch: [470/1000]:
train:
----------------------------
 tu:
     loss:18366.088697
     acc:0.838194
 tv:
     loss:19867.420183
     acc:0.528869
 lu:
     loss:14048.078046
     acc:0.920837
 lv:
     loss:17126.839094
     acc:0.285197
 le:
     loss:1518.118986
     acc:0.999599
 encoder:
     loss:0.147186
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.479785
     acc:0.836166
 tv:
     loss:19119.050488
     acc:0.486634
 lu:
     loss:13417.910889
     acc:0.913098
 lv:
     loss:16747.233496
     acc:0.189369
 le:
     loss:1448.416779
     acc:0.999167
 encoder:
     loss:0.144002
----------------------------


Epoch: [471/1000]:
train:
----------------------------
 tu:
     loss:18365.716865
     acc:0.838312
 tv:
     loss:19863.772143
     acc:0.529720
 lu:
     loss:14047.112713
     acc:0.921119
 lv:
     loss:17124.259289
     acc:0.285448
 le:
     loss:1518.170487
     acc:0.999589
 encoder:
     loss:0.169989
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.753125
     acc:0.836074
 tv:
     loss:19119.226953
     acc:0.486775
 lu:
     loss:13421.432129
     acc:0.912144
 lv:
     loss:16745.121680
     acc:0.190329
 le:
     loss:1448.603308
     acc:0.999107
 encoder:
     loss:0.155241
----------------------------


Epoch: [472/1000]:
train:
----------------------------
 tu:
     loss:18365.895860
     acc:0.838131
 tv:
     loss:19865.651867
     acc:0.529508
 lu:
     loss:14047.927609
     acc:0.921052
 lv:
     loss:17124.316190
     acc:0.285974
 le:
     loss:1518.134590
     acc:0.999588
 encoder:
     loss:0.172971
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.473828
     acc:0.836105
 tv:
     loss:19115.225391
     acc:0.487889
 lu:
     loss:13417.848633
     acc:0.913266
 lv:
     loss:16747.977637
     acc:0.190025
 le:
     loss:1448.581744
     acc:0.999126
 encoder:
     loss:0.188382
----------------------------


Epoch: [473/1000]:
train:
----------------------------
 tu:
     loss:18366.618539
     acc:0.837993
 tv:
     loss:19865.388309
     acc:0.529340
 lu:
     loss:14047.049226
     acc:0.921196
 lv:
     loss:17120.857774
     acc:0.286253
 le:
     loss:1518.019951
     acc:0.999607
 encoder:
     loss:0.351862
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.394922
     acc:0.836105
 tv:
     loss:19115.152148
     acc:0.487534
 lu:
     loss:13418.680176
     acc:0.912947
 lv:
     loss:16739.813184
     acc:0.191502
 le:
     loss:1448.476764
     acc:0.999127
 encoder:
     loss:0.467699
----------------------------


Epoch: [474/1000]:
train:
----------------------------
 tu:
     loss:18366.454170
     acc:0.837997
 tv:
     loss:19866.071743
     acc:0.529395
 lu:
     loss:14048.892396
     acc:0.920575
 lv:
     loss:17121.215241
     acc:0.286495
 le:
     loss:1518.087462
     acc:0.999600
 encoder:
     loss:0.933815
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17505.777051
     acc:0.836243
 tv:
     loss:19117.165137
     acc:0.487244
 lu:
     loss:13419.957666
     acc:0.912682
 lv:
     loss:16740.924902
     acc:0.190832
 le:
     loss:1448.029456
     acc:0.999232
 encoder:
     loss:1.218708
----------------------------


Epoch: [475/1000]:
train:
----------------------------
 tu:
     loss:18366.428949
     acc:0.838067
 tv:
     loss:19864.431255
     acc:0.529594
 lu:
     loss:14047.111930
     acc:0.921064
 lv:
     loss:17125.090911
     acc:0.285399
 le:
     loss:1517.982179
     acc:0.999619
 encoder:
     loss:0.773609
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.608301
     acc:0.836142
 tv:
     loss:19118.888379
     acc:0.486999
 lu:
     loss:13420.778711
     acc:0.912490
 lv:
     loss:16743.226855
     acc:0.190022
 le:
     loss:1448.335638
     acc:0.999189
 encoder:
     loss:0.365592
----------------------------


Epoch: [476/1000]:
train:
----------------------------
 tu:
     loss:18366.837811
     acc:0.837845
 tv:
     loss:19864.498388
     acc:0.529603
 lu:
     loss:14047.936523
     acc:0.920919
 lv:
     loss:17124.402594
     acc:0.285612
 le:
     loss:1518.195119
     acc:0.999563
 encoder:
     loss:0.264068
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.815527
     acc:0.836020
 tv:
     loss:19115.905371
     acc:0.487242
 lu:
     loss:13419.239160
     acc:0.912822
 lv:
     loss:16738.132520
     acc:0.192382
 le:
     loss:1448.389484
     acc:0.999148
 encoder:
     loss:0.174640
----------------------------


Epoch: [477/1000]:
train:
----------------------------
 tu:
     loss:18366.639058
     acc:0.837973
 tv:
     loss:19863.313738
     acc:0.529739
 lu:
     loss:14046.165471
     acc:0.921247
 lv:
     loss:17122.581906
     acc:0.285781
 le:
     loss:1518.052130
     acc:0.999611
 encoder:
     loss:0.175433
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.775488
     acc:0.836107
 tv:
     loss:19117.999023
     acc:0.487224
 lu:
     loss:13419.873535
     acc:0.912485
 lv:
     loss:16741.946875
     acc:0.190602
 le:
     loss:1448.453168
     acc:0.999189
 encoder:
     loss:0.157499
----------------------------


Epoch: [478/1000]:
train:
----------------------------
 tu:
     loss:18365.417991
     acc:0.838296
 tv:
     loss:19863.913869
     acc:0.529620
 lu:
     loss:14047.258573
     acc:0.920987
 lv:
     loss:17120.706441
     acc:0.286362
 le:
     loss:1517.997418
     acc:0.999615
 encoder:
     loss:0.156700
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.304492
     acc:0.836208
 tv:
     loss:19119.211328
     acc:0.486862
 lu:
     loss:13420.378027
     acc:0.912542
 lv:
     loss:16745.062207
     acc:0.190475
 le:
     loss:1448.472998
     acc:0.999146
 encoder:
     loss:0.150685
----------------------------


Epoch: [479/1000]:
train:
----------------------------
 tu:
     loss:18365.343296
     acc:0.838351
 tv:
     loss:19863.576762
     acc:0.529723
 lu:
     loss:14047.731400
     acc:0.920858
 lv:
     loss:17117.037143
     acc:0.287092
 le:
     loss:1518.064930
     acc:0.999596
 encoder:
     loss:0.158321
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17507.045508
     acc:0.835899
 tv:
     loss:19120.793555
     acc:0.486272
 lu:
     loss:13419.944141
     acc:0.912375
 lv:
     loss:16742.684668
     acc:0.190611
 le:
     loss:1448.851642
     acc:0.999063
 encoder:
     loss:0.203456
----------------------------


Epoch: [480/1000]:
train:
----------------------------
 tu:
     loss:18366.130871
     acc:0.838065
 tv:
     loss:19863.631189
     acc:0.529591
 lu:
     loss:14047.524970
     acc:0.920951
 lv:
     loss:17117.261548
     acc:0.286970
 le:
     loss:1518.009476
     acc:0.999623
 encoder:
     loss:0.181094
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.382227
     acc:0.836165
 tv:
     loss:19120.951367
     acc:0.486316
 lu:
     loss:13418.063574
     acc:0.912919
 lv:
     loss:16740.036719
     acc:0.191118
 le:
     loss:1448.470313
     acc:0.999129
 encoder:
     loss:0.155779
----------------------------


Epoch: [481/1000]:
train:
----------------------------
 tu:
     loss:18366.884470
     acc:0.838024
 tv:
     loss:19863.157624
     acc:0.529635
 lu:
     loss:14046.909657
     acc:0.921095
 lv:
     loss:17116.242619
     acc:0.287176
 le:
     loss:1518.030688
     acc:0.999611
 encoder:
     loss:0.157404
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.906641
     acc:0.836001
 tv:
     loss:19122.416211
     acc:0.485868
 lu:
     loss:13418.581982
     acc:0.912920
 lv:
     loss:16749.635547
     acc:0.189065
 le:
     loss:1448.537109
     acc:0.999128
 encoder:
     loss:0.146384
----------------------------


Epoch: [482/1000]:
train:
----------------------------
 tu:
     loss:18366.278025
     acc:0.838050
 tv:
     loss:19862.752021
     acc:0.530032
 lu:
     loss:14047.342160
     acc:0.921038
 lv:
     loss:17116.190691
     acc:0.287569
 le:
     loss:1518.074995
     acc:0.999602
 encoder:
     loss:0.126419
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.706738
     acc:0.836086
 tv:
     loss:19120.498437
     acc:0.486880
 lu:
     loss:13418.181738
     acc:0.912928
 lv:
     loss:16746.577930
     acc:0.189734
 le:
     loss:1448.217480
     acc:0.999210
 encoder:
     loss:0.123805
----------------------------


Epoch: [483/1000]:
train:
----------------------------
 tu:
     loss:18365.429154
     acc:0.838274
 tv:
     loss:19863.669468
     acc:0.529624
 lu:
     loss:14046.431300
     acc:0.921224
 lv:
     loss:17121.843750
     acc:0.286115
 le:
     loss:1518.037223
     acc:0.999614
 encoder:
     loss:0.148932
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.782520
     acc:0.836081
 tv:
     loss:19121.012207
     acc:0.486471
 lu:
     loss:13417.680566
     acc:0.913226
 lv:
     loss:16752.582617
     acc:0.188266
 le:
     loss:1448.608868
     acc:0.999144
 encoder:
     loss:0.303263
----------------------------


Epoch: [484/1000]:
train:
----------------------------
 tu:
     loss:18365.982161
     acc:0.838136
 tv:
     loss:19861.832826
     acc:0.530035
 lu:
     loss:14046.462368
     acc:0.921221
 lv:
     loss:17117.864371
     acc:0.286896
 le:
     loss:1518.100289
     acc:0.999597
 encoder:
     loss:0.225264
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.750195
     acc:0.836004
 tv:
     loss:19121.765234
     acc:0.486167
 lu:
     loss:13417.150195
     acc:0.913146
 lv:
     loss:16745.265625
     acc:0.189749
 le:
     loss:1448.392767
     acc:0.999168
 encoder:
     loss:0.141816
----------------------------


Epoch: [485/1000]:
train:
----------------------------
 tu:
     loss:18367.129247
     acc:0.837915
 tv:
     loss:19863.851790
     acc:0.529595
 lu:
     loss:14046.225870
     acc:0.921262
 lv:
     loss:17116.837970
     acc:0.287164
 le:
     loss:1518.021649
     acc:0.999607
 encoder:
     loss:0.148361
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.624316
     acc:0.836006
 tv:
     loss:19120.722754
     acc:0.486571
 lu:
     loss:13418.645459
     acc:0.913071
 lv:
     loss:16742.390332
     acc:0.190760
 le:
     loss:1448.576013
     acc:0.999127
 encoder:
     loss:0.138282
----------------------------


Epoch: [486/1000]:
train:
----------------------------
 tu:
     loss:18365.729890
     acc:0.838166
 tv:
     loss:19861.116540
     acc:0.530285
 lu:
     loss:14045.506200
     acc:0.921495
 lv:
     loss:17116.143805
     acc:0.286980
 le:
     loss:1518.028154
     acc:0.999610
 encoder:
     loss:0.227576
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.943164
     acc:0.835979
 tv:
     loss:19118.500781
     acc:0.486746
 lu:
     loss:13419.524268
     acc:0.912604
 lv:
     loss:16745.271289
     acc:0.190789
 le:
     loss:1448.709351
     acc:0.999086
 encoder:
     loss:0.287463
----------------------------


Epoch: [487/1000]:
train:
----------------------------
 tu:
     loss:18366.333360
     acc:0.838146
 tv:
     loss:19862.523528
     acc:0.529798
 lu:
     loss:14046.164562
     acc:0.921324
 lv:
     loss:17119.520133
     acc:0.286426
 le:
     loss:1517.955581
     acc:0.999630
 encoder:
     loss:0.201757
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.255273
     acc:0.836223
 tv:
     loss:19114.823535
     acc:0.487806
 lu:
     loss:13419.326172
     acc:0.912585
 lv:
     loss:16743.989746
     acc:0.191019
 le:
     loss:1448.591302
     acc:0.999144
 encoder:
     loss:0.132565
----------------------------


Epoch: [488/1000]:
train:
----------------------------
 tu:
     loss:18365.934809
     acc:0.838027
 tv:
     loss:19863.244663
     acc:0.529804
 lu:
     loss:14046.838379
     acc:0.921098
 lv:
     loss:17116.906398
     acc:0.287128
 le:
     loss:1518.053003
     acc:0.999599
 encoder:
     loss:0.125136
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17507.045312
     acc:0.835884
 tv:
     loss:19116.889941
     acc:0.487214
 lu:
     loss:13418.329443
     acc:0.912880
 lv:
     loss:16737.979102
     acc:0.192047
 le:
     loss:1448.590399
     acc:0.999127
 encoder:
     loss:0.125247
----------------------------


Epoch: [489/1000]:
train:
----------------------------
 tu:
     loss:18365.640387
     acc:0.838247
 tv:
     loss:19860.949571
     acc:0.530161
 lu:
     loss:14046.059900
     acc:0.921131
 lv:
     loss:17116.152060
     acc:0.287241
 le:
     loss:1518.037063
     acc:0.999612
 encoder:
     loss:0.131524
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.812598
     acc:0.836020
 tv:
     loss:19117.037695
     acc:0.487196
 lu:
     loss:13420.546484
     acc:0.912608
 lv:
     loss:16741.587109
     acc:0.190565
 le:
     loss:1449.076038
     acc:0.999002
 encoder:
     loss:0.136371
----------------------------


Epoch: [490/1000]:
train:
----------------------------
 tu:
     loss:18365.966990
     acc:0.838063
 tv:
     loss:19855.735488
     acc:0.531467
 lu:
     loss:14047.464628
     acc:0.921046
 lv:
     loss:17117.928893
     acc:0.287078
 le:
     loss:1518.076788
     acc:0.999603
 encoder:
     loss:0.163509
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.308984
     acc:0.836266
 tv:
     loss:19105.515430
     acc:0.489905
 lu:
     loss:13417.880664
     acc:0.912987
 lv:
     loss:16757.035059
     acc:0.187181
 le:
     loss:1448.623853
     acc:0.999087
 encoder:
     loss:0.240560
----------------------------


Epoch: [491/1000]:
train:
----------------------------
 tu:
     loss:18365.408884
     acc:0.838253
 tv:
     loss:19849.859954
     acc:0.532631
 lu:
     loss:14046.677098
     acc:0.921138
 lv:
     loss:17118.690407
     acc:0.286687
 le:
     loss:1518.146040
     acc:0.999581
 encoder:
     loss:0.185998
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17507.044043
     acc:0.836105
 tv:
     loss:19101.374805
     acc:0.490746
 lu:
     loss:13418.832617
     acc:0.912900
 lv:
     loss:16750.257227
     acc:0.189277
 le:
     loss:1449.079309
     acc:0.999021
 encoder:
     loss:0.148072
----------------------------


Epoch: [492/1000]:
train:
----------------------------
 tu:
     loss:18365.848406
     acc:0.838192
 tv:
     loss:19846.057538
     acc:0.533680
 lu:
     loss:14047.386923
     acc:0.920998
 lv:
     loss:17115.204908
     acc:0.287633
 le:
     loss:1518.013785
     acc:0.999619
 encoder:
     loss:0.163911
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17505.917578
     acc:0.836345
 tv:
     loss:19102.158496
     acc:0.490859
 lu:
     loss:13417.767529
     acc:0.912951
 lv:
     loss:16756.616211
     acc:0.187478
 le:
     loss:1448.487463
     acc:0.999126
 encoder:
     loss:0.269115
----------------------------


Epoch: [493/1000]:
train:
----------------------------
 tu:
     loss:18366.200309
     acc:0.838098
 tv:
     loss:19845.072107
     acc:0.533439
 lu:
     loss:14046.483841
     acc:0.921136
 lv:
     loss:17116.304290
     acc:0.287012
 le:
     loss:1518.206106
     acc:0.999572
 encoder:
     loss:0.336275
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.433398
     acc:0.836160
 tv:
     loss:19103.964160
     acc:0.489984
 lu:
     loss:13420.055273
     acc:0.912323
 lv:
     loss:16743.998242
     acc:0.189934
 le:
     loss:1448.471460
     acc:0.999168
 encoder:
     loss:0.233570
----------------------------


Epoch: [494/1000]:
train:
----------------------------
 tu:
     loss:18365.570858
     acc:0.838228
 tv:
     loss:19844.417356
     acc:0.533731
 lu:
     loss:14045.207474
     acc:0.921518
 lv:
     loss:17115.113747
     acc:0.287482
 le:
     loss:1518.165006
     acc:0.999577
 encoder:
     loss:0.202588
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.470117
     acc:0.836161
 tv:
     loss:19101.616211
     acc:0.490915
 lu:
     loss:13418.029980
     acc:0.913110
 lv:
     loss:16744.667383
     acc:0.189964
 le:
     loss:1448.533008
     acc:0.999148
 encoder:
     loss:0.131106
----------------------------


Epoch: [495/1000]:
train:
----------------------------
 tu:
     loss:18366.963288
     acc:0.837898
 tv:
     loss:19845.296534
     acc:0.533431
 lu:
     loss:14047.383187
     acc:0.920899
 lv:
     loss:17112.557765
     acc:0.287931
 le:
     loss:1518.106626
     acc:0.999591
 encoder:
     loss:0.180016
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.962207
     acc:0.835895
 tv:
     loss:19103.284766
     acc:0.490307
 lu:
     loss:13419.922217
     acc:0.912449
 lv:
     loss:16741.920508
     acc:0.190452
 le:
     loss:1448.238800
     acc:0.999168
 encoder:
     loss:0.169451
----------------------------


Epoch: [496/1000]:
train:
----------------------------
 tu:
     loss:18365.475154
     acc:0.838232
 tv:
     loss:19842.687012
     acc:0.533869
 lu:
     loss:14047.382676
     acc:0.921085
 lv:
     loss:17113.636923
     acc:0.287562
 le:
     loss:1518.056054
     acc:0.999605
 encoder:
     loss:0.160555
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.478809
     acc:0.836184
 tv:
     loss:19101.166992
     acc:0.491053
 lu:
     loss:13420.698437
     acc:0.912477
 lv:
     loss:16747.236816
     acc:0.189911
 le:
     loss:1448.494696
     acc:0.999127
 encoder:
     loss:0.144403
----------------------------


Epoch: [497/1000]:
train:
----------------------------
 tu:
     loss:18365.386764
     acc:0.838220
 tv:
     loss:19844.362486
     acc:0.533679
 lu:
     loss:14047.525311
     acc:0.920992
 lv:
     loss:17113.336539
     acc:0.288041
 le:
     loss:1517.977301
     acc:0.999617
 encoder:
     loss:0.175271
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.012793
     acc:0.836263
 tv:
     loss:19095.885352
     acc:0.491996
 lu:
     loss:13420.899463
     acc:0.912214
 lv:
     loss:16744.182520
     acc:0.190385
 le:
     loss:1448.461401
     acc:0.999167
 encoder:
     loss:0.169315
----------------------------


Epoch: [498/1000]:
train:
----------------------------
 tu:
     loss:18364.918673
     acc:0.838330
 tv:
     loss:19841.179131
     acc:0.534470
 lu:
     loss:14045.988804
     acc:0.921331
 lv:
     loss:17109.673226
     acc:0.288729
 le:
     loss:1518.036747
     acc:0.999605
 encoder:
     loss:0.173962
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.017676
     acc:0.836219
 tv:
     loss:19100.993750
     acc:0.490797
 lu:
     loss:13419.905469
     acc:0.912450
 lv:
     loss:16744.545508
     acc:0.190754
 le:
     loss:1448.802283
     acc:0.999083
 encoder:
     loss:0.251897
----------------------------


Epoch: [499/1000]:
train:
----------------------------
 tu:
     loss:18365.666243
     acc:0.838116
 tv:
     loss:19841.498047
     acc:0.534208
 lu:
     loss:14045.983785
     acc:0.921324
 lv:
     loss:17110.793309
     acc:0.288600
 le:
     loss:1517.932303
     acc:0.999634
 encoder:
     loss:0.234964
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.774219
     acc:0.836116
 tv:
     loss:19098.288477
     acc:0.491612
 lu:
     loss:13418.654932
     acc:0.912856
 lv:
     loss:16747.154102
     acc:0.189350
 le:
     loss:1448.917303
     acc:0.999025
 encoder:
     loss:0.197566
----------------------------


Epoch: [500/1000]:
train:
----------------------------
 tu:
     loss:18365.625636
     acc:0.838195
 tv:
     loss:19843.476290
     acc:0.533953
 lu:
     loss:14045.843671
     acc:0.921318
 lv:
     loss:17113.518861
     acc:0.287771
 le:
     loss:1518.005776
     acc:0.999627
 encoder:
     loss:0.175150
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.757520
     acc:0.836019
 tv:
     loss:19102.644043
     acc:0.490413
 lu:
     loss:13416.873584
     acc:0.913547
 lv:
     loss:16742.943555
     acc:0.190595
 le:
     loss:1448.537726
     acc:0.999107
 encoder:
     loss:0.160940
----------------------------


Epoch: [501/1000]:
train:
----------------------------
 tu:
     loss:18366.030046
     acc:0.838008
 tv:
     loss:19843.702693
     acc:0.533933
 lu:
     loss:14047.462255
     acc:0.920980
 lv:
     loss:17111.850404
     acc:0.288449
 le:
     loss:1517.963497
     acc:0.999619
 encoder:
     loss:0.157710
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17507.009277
     acc:0.836025
 tv:
     loss:19100.720020
     acc:0.490883
 lu:
     loss:13417.560400
     acc:0.913411
 lv:
     loss:16748.222949
     acc:0.189161
 le:
     loss:1448.929156
     acc:0.999062
 encoder:
     loss:0.127934
----------------------------


Epoch: [502/1000]:
train:
----------------------------
 tu:
     loss:18365.815225
     acc:0.838037
 tv:
     loss:19843.210801
     acc:0.533956
 lu:
     loss:14046.449980
     acc:0.921132
 lv:
     loss:17110.503849
     acc:0.288582
 le:
     loss:1518.036980
     acc:0.999608
 encoder:
     loss:0.141253
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17507.317871
     acc:0.835916
 tv:
     loss:19100.563672
     acc:0.490865
 lu:
     loss:13419.704590
     acc:0.912539
 lv:
     loss:16747.235645
     acc:0.189534
 le:
     loss:1448.591644
     acc:0.999127
 encoder:
     loss:0.198085
----------------------------


Epoch: [503/1000]:
train:
----------------------------
 tu:
     loss:18366.226915
     acc:0.838157
 tv:
     loss:19841.169036
     acc:0.534221
 lu:
     loss:14045.793230
     acc:0.921207
 lv:
     loss:17107.579329
     acc:0.288812
 le:
     loss:1518.053908
     acc:0.999605
 encoder:
     loss:0.206476
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.684766
     acc:0.836045
 tv:
     loss:19101.095898
     acc:0.490526
 lu:
     loss:13421.007129
     acc:0.912195
 lv:
     loss:16746.627344
     acc:0.189547
 le:
     loss:1448.850830
     acc:0.999083
 encoder:
     loss:0.135294
----------------------------


Epoch: [504/1000]:
train:
----------------------------
 tu:
     loss:18365.691656
     acc:0.838059
 tv:
     loss:19843.053268
     acc:0.534133
 lu:
     loss:14046.401140
     acc:0.921196
 lv:
     loss:17107.225302
     acc:0.289170
 le:
     loss:1517.958644
     acc:0.999626
 encoder:
     loss:0.244162
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17507.043457
     acc:0.835966
 tv:
     loss:19102.944727
     acc:0.490589
 lu:
     loss:13420.057520
     acc:0.912629
 lv:
     loss:16747.064844
     acc:0.190038
 le:
     loss:1448.840643
     acc:0.999064
 encoder:
     loss:0.219648
----------------------------


Epoch: [505/1000]:
train:
----------------------------
 tu:
     loss:18365.115155
     acc:0.838326
 tv:
     loss:19840.683946
     acc:0.534418
 lu:
     loss:14046.118005
     acc:0.921247
 lv:
     loss:17108.016715
     acc:0.289295
 le:
     loss:1518.094805
     acc:0.999596
 encoder:
     loss:0.196014
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.561230
     acc:0.836207
 tv:
     loss:19098.934570
     acc:0.491114
 lu:
     loss:13418.836035
     acc:0.912853
 lv:
     loss:16748.002637
     acc:0.189975
 le:
     loss:1448.648114
     acc:0.999126
 encoder:
     loss:0.249243
----------------------------


Epoch: [506/1000]:
train:
----------------------------
 tu:
     loss:18365.648687
     acc:0.838096
 tv:
     loss:19841.200502
     acc:0.534360
 lu:
     loss:14046.858194
     acc:0.921238
 lv:
     loss:17109.122854
     acc:0.289030
 le:
     loss:1518.135759
     acc:0.999576
 encoder:
     loss:0.207459
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.458594
     acc:0.836025
 tv:
     loss:19099.466406
     acc:0.491032
 lu:
     loss:13417.430811
     acc:0.913123
 lv:
     loss:16745.114355
     acc:0.190259
 le:
     loss:1448.872314
     acc:0.999024
 encoder:
     loss:0.184270
----------------------------


Epoch: [507/1000]:
train:
----------------------------
 tu:
     loss:18365.517964
     acc:0.838171
 tv:
     loss:19840.899244
     acc:0.534320
 lu:
     loss:14046.854663
     acc:0.920929
 lv:
     loss:17108.269202
     acc:0.288845
 le:
     loss:1517.911076
     acc:0.999630
 encoder:
     loss:0.652852
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17507.152246
     acc:0.836001
 tv:
     loss:19106.152539
     acc:0.489585
 lu:
     loss:13419.332617
     acc:0.912589
 lv:
     loss:16748.817676
     acc:0.189582
 le:
     loss:1448.809784
     acc:0.999066
 encoder:
     loss:0.360864
----------------------------


Epoch: [508/1000]:
train:
----------------------------
 tu:
     loss:18366.066043
     acc:0.838007
 tv:
     loss:19840.907147
     acc:0.534486
 lu:
     loss:14047.102096
     acc:0.921081
 lv:
     loss:17110.055460
     acc:0.288522
 le:
     loss:1517.983698
     acc:0.999620
 encoder:
     loss:0.251763
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.522461
     acc:0.836084
 tv:
     loss:19103.746191
     acc:0.490004
 lu:
     loss:13419.175781
     acc:0.912784
 lv:
     loss:16747.786523
     acc:0.190176
 le:
     loss:1448.147180
     acc:0.999209
 encoder:
     loss:0.206392
----------------------------


Epoch: [509/1000]:
train:
----------------------------
 tu:
     loss:18365.171954
     acc:0.838229
 tv:
     loss:19838.807106
     acc:0.534806
 lu:
     loss:14046.072288
     acc:0.921335
 lv:
     loss:17108.791776
     acc:0.288917
 le:
     loss:1518.122977
     acc:0.999587
 encoder:
     loss:0.200387
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.302246
     acc:0.836350
 tv:
     loss:19098.282520
     acc:0.491300
 lu:
     loss:13418.282471
     acc:0.913224
 lv:
     loss:16746.934863
     acc:0.189539
 le:
     loss:1448.643298
     acc:0.999086
 encoder:
     loss:0.224361
----------------------------


Epoch: [510/1000]:
train:
----------------------------
 tu:
     loss:18365.471282
     acc:0.838111
 tv:
     loss:19839.076910
     acc:0.534712
 lu:
     loss:14047.304767
     acc:0.920905
 lv:
     loss:17104.695085
     acc:0.289731
 le:
     loss:1518.007968
     acc:0.999614
 encoder:
     loss:0.318302
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17507.629492
     acc:0.835938
 tv:
     loss:19100.716016
     acc:0.491118
 lu:
     loss:13416.603662
     acc:0.913431
 lv:
     loss:16746.197070
     acc:0.189705
 le:
     loss:1449.073871
     acc:0.999004
 encoder:
     loss:0.470563
----------------------------


Epoch: [511/1000]:
train:
----------------------------
 tu:
     loss:18365.582769
     acc:0.838150
 tv:
     loss:19839.126351
     acc:0.534756
 lu:
     loss:14046.440555
     acc:0.921126
 lv:
     loss:17106.781659
     acc:0.289126
 le:
     loss:1518.066173
     acc:0.999602
 encoder:
     loss:0.368519
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17507.109277
     acc:0.836021
 tv:
     loss:19101.317480
     acc:0.490657
 lu:
     loss:13417.521533
     acc:0.913162
 lv:
     loss:16747.321777
     acc:0.189462
 le:
     loss:1448.981970
     acc:0.999003
 encoder:
     loss:0.233016
----------------------------


Epoch: [512/1000]:
train:
----------------------------
 tu:
     loss:18365.513524
     acc:0.838142
 tv:
     loss:19842.041402
     acc:0.534188
 lu:
     loss:14046.282090
     acc:0.921137
 lv:
     loss:17106.331588
     acc:0.289524
 le:
     loss:1518.065129
     acc:0.999602
 encoder:
     loss:0.351540
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.999512
     acc:0.836065
 tv:
     loss:19105.679590
     acc:0.489894
 lu:
     loss:13418.252539
     acc:0.912917
 lv:
     loss:16747.886914
     acc:0.189489
 le:
     loss:1449.196149
     acc:0.999003
 encoder:
     loss:0.403637
----------------------------


Epoch: [513/1000]:
train:
----------------------------
 tu:
     loss:18365.789994
     acc:0.838099
 tv:
     loss:19837.988985
     acc:0.535134
 lu:
     loss:14046.644702
     acc:0.921133
 lv:
     loss:17106.476392
     acc:0.289166
 le:
     loss:1518.102428
     acc:0.999598
 encoder:
     loss:0.348294
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17506.927930
     acc:0.836003
 tv:
     loss:19102.343652
     acc:0.490378
 lu:
     loss:13418.719336
     acc:0.912802
 lv:
     loss:16746.524316
     acc:0.189391
 le:
     loss:1448.901697
     acc:0.999063
 encoder:
     loss:0.310530
----------------------------


Epoch: [514/1000]:
train:
----------------------------
 tu:
     loss:18347.199787
     acc:0.842200
 tv:
     loss:19838.571857
     acc:0.534785
 lu:
     loss:14046.312000
     acc:0.921107
 lv:
     loss:17103.779955
     acc:0.290089
 le:
     loss:1518.032885
     acc:0.999606
 encoder:
     loss:0.238386
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17473.396094
     acc:0.843142
 tv:
     loss:19103.129883
     acc:0.490749
 lu:
     loss:13418.115723
     acc:0.912672
 lv:
     loss:16751.519238
     acc:0.188431
 le:
     loss:1448.873059
     acc:0.999043
 encoder:
     loss:0.218681
----------------------------


Epoch: [515/1000]:
train:
----------------------------
 tu:
     loss:18330.262536
     acc:0.845689
 tv:
     loss:19840.212084
     acc:0.534303
 lu:
     loss:14045.773165
     acc:0.921434
 lv:
     loss:17101.815611
     acc:0.290513
 le:
     loss:1517.956334
     acc:0.999628
 encoder:
     loss:0.243348
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17472.592871
     acc:0.843477
 tv:
     loss:19102.170703
     acc:0.490564
 lu:
     loss:13418.237939
     acc:0.912950
 lv:
     loss:16746.384766
     acc:0.189730
 le:
     loss:1449.288629
     acc:0.998960
 encoder:
     loss:0.239015
----------------------------


Epoch: [516/1000]:
train:
----------------------------
 tu:
     loss:18329.440611
     acc:0.845622
 tv:
     loss:19839.562330
     acc:0.534830
 lu:
     loss:14045.965207
     acc:0.921363
 lv:
     loss:17103.760924
     acc:0.290229
 le:
     loss:1518.040036
     acc:0.999610
 encoder:
     loss:0.193599
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17472.521094
     acc:0.843568
 tv:
     loss:19106.343750
     acc:0.489830
 lu:
     loss:13419.471875
     acc:0.912827
 lv:
     loss:16745.245996
     acc:0.189732
 le:
     loss:1449.036896
     acc:0.998981
 encoder:
     loss:0.196477
----------------------------


Epoch: [517/1000]:
train:
----------------------------
 tu:
     loss:18329.497138
     acc:0.845668
 tv:
     loss:19838.563204
     acc:0.534927
 lu:
     loss:14045.690963
     acc:0.921267
 lv:
     loss:17105.576319
     acc:0.289557
 le:
     loss:1517.923612
     acc:0.999632
 encoder:
     loss:0.151424
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17472.285059
     acc:0.843501
 tv:
     loss:19096.878027
     acc:0.491779
 lu:
     loss:13418.286963
     acc:0.912810
 lv:
     loss:16740.868066
     acc:0.190413
 le:
     loss:1448.876257
     acc:0.999064
 encoder:
     loss:0.136927
----------------------------


Epoch: [518/1000]:
train:
----------------------------
 tu:
     loss:18329.277253
     acc:0.845714
 tv:
     loss:19838.694643
     acc:0.534919
 lu:
     loss:14045.099450
     acc:0.921419
 lv:
     loss:17101.923851
     acc:0.290285
 le:
     loss:1518.050626
     acc:0.999603
 encoder:
     loss:0.163339
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17472.872070
     acc:0.843378
 tv:
     loss:19099.969727
     acc:0.490946
 lu:
     loss:13417.938135
     acc:0.912948
 lv:
     loss:16744.258203
     acc:0.190106
 le:
     loss:1448.836115
     acc:0.999065
 encoder:
     loss:0.207376
----------------------------


Epoch: [519/1000]:
train:
----------------------------
 tu:
     loss:18329.237952
     acc:0.845759
 tv:
     loss:19837.567349
     acc:0.535237
 lu:
     loss:14046.202217
     acc:0.921115
 lv:
     loss:17101.911224
     acc:0.290431
 le:
     loss:1517.945941
     acc:0.999635
 encoder:
     loss:0.186665
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17472.862500
     acc:0.843299
 tv:
     loss:19102.711230
     acc:0.490460
 lu:
     loss:13418.652002
     acc:0.912915
 lv:
     loss:16740.886035
     acc:0.191468
 le:
     loss:1448.682141
     acc:0.999127
 encoder:
     loss:0.143298
----------------------------


Epoch: [520/1000]:
train:
----------------------------
 tu:
     loss:18329.258891
     acc:0.845765
 tv:
     loss:19837.153570
     acc:0.535215
 lu:
     loss:14045.151787
     acc:0.921334
 lv:
     loss:17102.175145
     acc:0.290178
 le:
     loss:1518.041177
     acc:0.999601
 encoder:
     loss:0.143395
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17472.946484
     acc:0.843315
 tv:
     loss:19100.972754
     acc:0.490932
 lu:
     loss:13417.300928
     acc:0.913056
 lv:
     loss:16744.542090
     acc:0.189840
 le:
     loss:1448.394550
     acc:0.999186
 encoder:
     loss:0.155332
----------------------------


Epoch: [521/1000]:
train:
----------------------------
 tu:
     loss:18329.161701
     acc:0.845863
 tv:
     loss:19837.900516
     acc:0.534862
 lu:
     loss:14043.859511
     acc:0.921712
 lv:
     loss:17103.123660
     acc:0.289910
 le:
     loss:1518.013541
     acc:0.999615
 encoder:
     loss:0.183256
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17472.837402
     acc:0.843358
 tv:
     loss:19100.717090
     acc:0.491084
 lu:
     loss:13417.039941
     acc:0.913180
 lv:
     loss:16742.315820
     acc:0.190752
 le:
     loss:1448.562994
     acc:0.999166
 encoder:
     loss:0.200863
----------------------------


Epoch: [522/1000]:
train:
----------------------------
 tu:
     loss:18328.084938
     acc:0.846017
 tv:
     loss:19837.674282
     acc:0.535003
 lu:
     loss:14043.713663
     acc:0.921820
 lv:
     loss:17102.211006
     acc:0.290354
 le:
     loss:1517.955285
     acc:0.999626
 encoder:
     loss:0.190148
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17473.136230
     acc:0.843180
 tv:
     loss:19097.586914
     acc:0.491381
 lu:
     loss:13417.758105
     acc:0.912916
 lv:
     loss:16742.701953
     acc:0.191048
 le:
     loss:1448.445624
     acc:0.999169
 encoder:
     loss:0.186054
----------------------------


Epoch: [523/1000]:
train:
----------------------------
 tu:
     loss:18328.661031
     acc:0.845922
 tv:
     loss:19837.529967
     acc:0.534855
 lu:
     loss:14044.062205
     acc:0.921613
 lv:
     loss:17097.476108
     acc:0.291182
 le:
     loss:1518.053474
     acc:0.999602
 encoder:
     loss:0.187994
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17472.200098
     acc:0.843479
 tv:
     loss:19102.060156
     acc:0.490745
 lu:
     loss:13418.308984
     acc:0.912819
 lv:
     loss:16743.587109
     acc:0.189955
 le:
     loss:1448.844366
     acc:0.999063
 encoder:
     loss:0.167815
----------------------------


Epoch: [524/1000]:
train:
----------------------------
 tu:
     loss:18328.486669
     acc:0.845911
 tv:
     loss:19836.458189
     acc:0.535107
 lu:
     loss:14043.605049
     acc:0.921807
 lv:
     loss:17102.265114
     acc:0.290080
 le:
     loss:1518.013949
     acc:0.999607
 encoder:
     loss:0.199310
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17472.796289
     acc:0.843406
 tv:
     loss:19099.289648
     acc:0.490999
 lu:
     loss:13419.963281
     acc:0.912588
 lv:
     loss:16742.711328
     acc:0.190263
 le:
     loss:1448.928162
     acc:0.999044
 encoder:
     loss:0.176298
----------------------------


Epoch: [525/1000]:
train:
----------------------------
 tu:
     loss:18328.419354
     acc:0.845854
 tv:
     loss:19838.058310
     acc:0.534846
 lu:
     loss:14045.132756
     acc:0.921502
 lv:
     loss:17097.984772
     acc:0.291085
 le:
     loss:1518.075824
     acc:0.999595
 encoder:
     loss:0.250870
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17473.014648
     acc:0.843380
 tv:
     loss:19098.149805
     acc:0.491417
 lu:
     loss:13420.211523
     acc:0.912445
 lv:
     loss:16749.258496
     acc:0.189932
 le:
     loss:1448.816760
     acc:0.999064
 encoder:
     loss:0.513003
----------------------------


Epoch: [526/1000]:
train:
----------------------------
 tu:
     loss:18328.032567
     acc:0.845965
 tv:
     loss:19835.653377
     acc:0.535404
 lu:
     loss:14045.732751
     acc:0.921411
 lv:
     loss:17098.198924
     acc:0.290980
 le:
     loss:1517.990359
     acc:0.999621
 encoder:
     loss:0.488228
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17473.296387
     acc:0.843379
 tv:
     loss:19101.111621
     acc:0.490773
 lu:
     loss:13419.369434
     acc:0.912638
 lv:
     loss:16740.485352
     acc:0.191246
 le:
     loss:1448.790466
     acc:0.999064
 encoder:
     loss:0.346797
----------------------------


Epoch: [527/1000]:
train:
----------------------------
 tu:
     loss:18327.909975
     acc:0.846026
 tv:
     loss:19836.945381
     acc:0.535179
 lu:
     loss:14044.475393
     acc:0.921544
 lv:
     loss:17098.860783
     acc:0.291006
 le:
     loss:1518.060020
     acc:0.999613
 encoder:
     loss:0.288001
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17472.973828
     acc:0.843298
 tv:
     loss:19101.446094
     acc:0.490520
 lu:
     loss:13417.587061
     acc:0.913122
 lv:
     loss:16743.283496
     acc:0.190391
 le:
     loss:1448.926129
     acc:0.999087
 encoder:
     loss:0.240489
----------------------------


Epoch: [528/1000]:
train:
----------------------------
 tu:
     loss:18329.371525
     acc:0.845664
 tv:
     loss:19837.892328
     acc:0.534856
 lu:
     loss:14045.836982
     acc:0.921261
 lv:
     loss:17101.550520
     acc:0.290392
 le:
     loss:1518.048127
     acc:0.999613
 encoder:
     loss:0.370965
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17473.064453
     acc:0.843306
 tv:
     loss:19099.101270
     acc:0.491443
 lu:
     loss:13420.188281
     acc:0.912461
 lv:
     loss:16744.047168
     acc:0.190207
 le:
     loss:1448.897162
     acc:0.999086
 encoder:
     loss:0.339832
----------------------------


Epoch: [529/1000]:
train:
----------------------------
 tu:
     loss:18328.452932
     acc:0.845896
 tv:
     loss:19835.418911
     acc:0.535388
 lu:
     loss:14045.476608
     acc:0.921374
 lv:
     loss:17098.222622
     acc:0.291172
 le:
     loss:1518.127554
     acc:0.999584
 encoder:
     loss:0.219881
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17472.807617
     acc:0.843487
 tv:
     loss:19100.026367
     acc:0.491160
 lu:
     loss:13418.978613
     acc:0.912431
 lv:
     loss:16744.726758
     acc:0.189749
 le:
     loss:1449.219122
     acc:0.999004
 encoder:
     loss:0.172791
----------------------------


Epoch: [530/1000]:
train:
----------------------------
 tu:
     loss:18328.047727
     acc:0.845965
 tv:
     loss:19836.237623
     acc:0.535423
 lu:
     loss:14044.948185
     acc:0.921469
 lv:
     loss:17098.420285
     acc:0.291130
 le:
     loss:1517.981066
     acc:0.999624
 encoder:
     loss:0.154863
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17472.556934
     acc:0.843400
 tv:
     loss:19103.003027
     acc:0.490009
 lu:
     loss:13419.082959
     acc:0.912921
 lv:
     loss:16743.459180
     acc:0.190808
 le:
     loss:1448.686932
     acc:0.999106
 encoder:
     loss:0.151229
----------------------------


Epoch: [531/1000]:
train:
----------------------------
 tu:
     loss:18328.177757
     acc:0.845923
 tv:
     loss:19835.310853
     acc:0.535531
 lu:
     loss:14044.965605
     acc:0.921557
 lv:
     loss:17095.918957
     acc:0.291668
 le:
     loss:1518.080295
     acc:0.999598
 encoder:
     loss:0.246137
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17472.176563
     acc:0.843570
 tv:
     loss:19099.072949
     acc:0.491021
 lu:
     loss:13420.226465
     acc:0.912350
 lv:
     loss:16743.673730
     acc:0.190211
 le:
     loss:1448.508423
     acc:0.999148
 encoder:
     loss:0.288597
----------------------------


Epoch: [532/1000]:
train:
----------------------------
 tu:
     loss:18328.905489
     acc:0.845771
 tv:
     loss:19835.501999
     acc:0.535778
 lu:
     loss:14044.667094
     acc:0.921428
 lv:
     loss:17098.037461
     acc:0.291203
 le:
     loss:1518.048987
     acc:0.999608
 encoder:
     loss:0.224857
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17473.161328
     acc:0.843177
 tv:
     loss:19098.984082
     acc:0.491062
 lu:
     loss:13420.398340
     acc:0.912366
 lv:
     loss:16750.598145
     acc:0.189297
 le:
     loss:1448.705865
     acc:0.999087
 encoder:
     loss:0.140871
----------------------------


Epoch: [533/1000]:
train:
----------------------------
 tu:
     loss:18328.081895
     acc:0.845938
 tv:
     loss:19837.318212
     acc:0.535146
 lu:
     loss:14045.000000
     acc:0.921521
 lv:
     loss:17096.808560
     acc:0.291616
 le:
     loss:1517.971843
     acc:0.999628
 encoder:
     loss:0.136463
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17473.449219
     acc:0.843296
 tv:
     loss:19099.354297
     acc:0.491193
 lu:
     loss:13420.702490
     acc:0.912595
 lv:
     loss:16746.691504
     acc:0.189463
 le:
     loss:1449.065320
     acc:0.999060
 encoder:
     loss:0.137467
----------------------------


Epoch: [534/1000]:
train:
----------------------------
 tu:
     loss:18328.364326
     acc:0.845878
 tv:
     loss:19834.101290
     acc:0.535779
 lu:
     loss:14045.132585
     acc:0.921445
 lv:
     loss:17093.992914
     acc:0.291889
 le:
     loss:1518.001160
     acc:0.999615
 encoder:
     loss:0.229272
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17472.721094
     acc:0.843374
 tv:
     loss:19097.786328
     acc:0.491644
 lu:
     loss:13417.780225
     acc:0.913020
 lv:
     loss:16740.226660
     acc:0.191770
 le:
     loss:1448.445905
     acc:0.999169
 encoder:
     loss:0.179218
----------------------------


Epoch: [535/1000]:
train:
----------------------------
 tu:
     loss:18327.421421
     acc:0.845977
 tv:
     loss:19836.638581
     acc:0.535239
 lu:
     loss:14045.303177
     acc:0.921375
 lv:
     loss:17093.556925
     acc:0.292033
 le:
     loss:1517.949196
     acc:0.999616
 encoder:
     loss:0.229440
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17472.865430
     acc:0.843404
 tv:
     loss:19101.004980
     acc:0.490930
 lu:
     loss:13419.706201
     acc:0.912611
 lv:
     loss:16738.108203
     acc:0.192177
 le:
     loss:1448.771161
     acc:0.999105
 encoder:
     loss:0.212159
----------------------------


Epoch: [536/1000]:
train:
----------------------------
 tu:
     loss:18328.368686
     acc:0.845767
 tv:
     loss:19837.197754
     acc:0.535074
 lu:
     loss:14043.351937
     acc:0.921804
 lv:
     loss:17094.186546
     acc:0.292154
 le:
     loss:1517.979328
     acc:0.999623
 encoder:
     loss:0.181221
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17472.163574
     acc:0.843538
 tv:
     loss:19103.763379
     acc:0.490313
 lu:
     loss:13416.173291
     acc:0.913584
 lv:
     loss:16740.205273
     acc:0.190928
 le:
     loss:1448.561792
     acc:0.999148
 encoder:
     loss:0.140563
----------------------------


Epoch: [537/1000]:
train:
----------------------------
 tu:
     loss:18327.456157
     acc:0.846064
 tv:
     loss:19834.437716
     acc:0.535672
 lu:
     loss:14043.701308
     acc:0.921806
 lv:
     loss:17094.041697
     acc:0.291876
 le:
     loss:1517.953778
     acc:0.999624
 encoder:
     loss:0.215883
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17473.164941
     acc:0.843217
 tv:
     loss:19100.095508
     acc:0.491244
 lu:
     loss:13420.149121
     acc:0.912442
 lv:
     loss:16748.585352
     acc:0.189590
 le:
     loss:1448.942731
     acc:0.999066
 encoder:
     loss:0.194181
----------------------------


Epoch: [538/1000]:
train:
----------------------------
 tu:
     loss:18328.068768
     acc:0.845934
 tv:
     loss:19835.702830
     acc:0.535511
 lu:
     loss:14044.995333
     acc:0.921437
 lv:
     loss:17095.773914
     acc:0.291642
 le:
     loss:1517.987742
     acc:0.999624
 encoder:
     loss:0.159339
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17472.948145
     acc:0.843235
 tv:
     loss:19105.912598
     acc:0.489786
 lu:
     loss:13418.897705
     acc:0.912688
 lv:
     loss:16746.355957
     acc:0.189960
 le:
     loss:1448.684015
     acc:0.999106
 encoder:
     loss:0.145234
----------------------------


Epoch: [539/1000]:
train:
----------------------------
 tu:
     loss:18328.091161
     acc:0.845824
 tv:
     loss:19835.734023
     acc:0.535460
 lu:
     loss:14044.418684
     acc:0.921616
 lv:
     loss:17093.853709
     acc:0.291752
 le:
     loss:1518.131434
     acc:0.999587
 encoder:
     loss:0.216416
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17472.458594
     acc:0.843380
 tv:
     loss:19104.685742
     acc:0.490164
 lu:
     loss:13418.386182
     acc:0.912945
 lv:
     loss:16744.768945
     acc:0.190146
 le:
     loss:1449.032635
     acc:0.998981
 encoder:
     loss:0.194562
----------------------------


Epoch: [540/1000]:
train:
----------------------------
 tu:
     loss:18327.156046
     acc:0.846101
 tv:
     loss:19836.672590
     acc:0.535083
 lu:
     loss:14045.714469
     acc:0.921297
 lv:
     loss:17092.586471
     acc:0.292161
 le:
     loss:1518.070077
     acc:0.999605
 encoder:
     loss:0.157105
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17472.918457
     acc:0.843320
 tv:
     loss:19103.542187
     acc:0.490417
 lu:
     loss:13418.600342
     acc:0.912786
 lv:
     loss:16743.484375
     acc:0.190584
 le:
     loss:1448.869464
     acc:0.999085
 encoder:
     loss:0.149383
----------------------------


Epoch: [541/1000]:
train:
----------------------------
 tu:
     loss:18327.599995
     acc:0.845940
 tv:
     loss:19833.195812
     acc:0.535863
 lu:
     loss:14045.106661
     acc:0.921321
 lv:
     loss:17093.908112
     acc:0.292023
 le:
     loss:1517.939737
     acc:0.999627
 encoder:
     loss:0.171016
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17472.884473
     acc:0.843275
 tv:
     loss:19097.006836
     acc:0.491825
 lu:
     loss:13417.717041
     acc:0.913243
 lv:
     loss:16747.322168
     acc:0.189829
 le:
     loss:1448.840088
     acc:0.999086
 encoder:
     loss:0.185310
----------------------------


Epoch: [542/1000]:
train:
----------------------------
 tu:
     loss:18327.235306
     acc:0.846083
 tv:
     loss:19833.875579
     acc:0.535885
 lu:
     loss:14043.979867
     acc:0.921639
 lv:
     loss:17094.317701
     acc:0.291748
 le:
     loss:1518.221207
     acc:0.999577
 encoder:
     loss:0.186423
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17472.468164
     acc:0.843413
 tv:
     loss:19098.396973
     acc:0.491448
 lu:
     loss:13418.575635
     acc:0.912740
 lv:
     loss:16742.286816
     acc:0.190530
 le:
     loss:1449.063824
     acc:0.999057
 encoder:
     loss:0.145213
----------------------------


Epoch: [543/1000]:
train:
----------------------------
 tu:
     loss:18327.770247
     acc:0.845966
 tv:
     loss:19832.949798
     acc:0.535806
 lu:
     loss:14044.295955
     acc:0.921537
 lv:
     loss:17091.622945
     acc:0.292145
 le:
     loss:1518.023083
     acc:0.999613
 encoder:
     loss:0.157495
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17472.279590
     acc:0.843473
 tv:
     loss:19098.535352
     acc:0.491431
 lu:
     loss:13418.221826
     acc:0.913102
 lv:
     loss:16753.124707
     acc:0.188378
 le:
     loss:1448.571307
     acc:0.999124
 encoder:
     loss:0.148736
----------------------------


Epoch: [544/1000]:
train:
----------------------------
 tu:
     loss:18327.458519
     acc:0.845975
 tv:
     loss:19834.709507
     acc:0.535606
 lu:
     loss:14044.449457
     acc:0.921630
 lv:
     loss:17089.813715
     acc:0.292815
 le:
     loss:1518.014631
     acc:0.999619
 encoder:
     loss:0.131377
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17472.503320
     acc:0.843459
 tv:
     loss:19102.893262
     acc:0.490098
 lu:
     loss:13419.843262
     acc:0.912538
 lv:
     loss:16748.609082
     acc:0.189749
 le:
     loss:1448.757849
     acc:0.999082
 encoder:
     loss:0.119305
----------------------------


Epoch: [545/1000]:
train:
----------------------------
 tu:
     loss:18327.138774
     acc:0.846112
 tv:
     loss:19834.363826
     acc:0.535698
 lu:
     loss:14043.626624
     acc:0.921848
 lv:
     loss:17089.237532
     acc:0.292849
 le:
     loss:1517.965034
     acc:0.999626
 encoder:
     loss:0.156388
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17472.020313
     acc:0.843558
 tv:
     loss:19101.035645
     acc:0.491150
 lu:
     loss:13420.839648
     acc:0.912405
 lv:
     loss:16740.390234
     acc:0.191523
 le:
     loss:1448.414398
     acc:0.999149
 encoder:
     loss:0.159441
----------------------------


Epoch: [546/1000]:
train:
----------------------------
 tu:
     loss:18327.658476
     acc:0.845940
 tv:
     loss:19833.799271
     acc:0.535584
 lu:
     loss:14044.288904
     acc:0.921714
 lv:
     loss:17090.866393
     acc:0.292673
 le:
     loss:1518.054811
     acc:0.999599
 encoder:
     loss:0.172372
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17472.627246
     acc:0.843390
 tv:
     loss:19103.862305
     acc:0.490345
 lu:
     loss:13421.087305
     acc:0.912245
 lv:
     loss:16750.249805
     acc:0.189221
 le:
     loss:1448.921765
     acc:0.999064
 encoder:
     loss:0.135008
----------------------------


Epoch: [547/1000]:
train:
----------------------------
 tu:
     loss:18328.049986
     acc:0.845860
 tv:
     loss:19833.206793
     acc:0.535994
 lu:
     loss:14044.008653
     acc:0.921697
 lv:
     loss:17090.697947
     acc:0.292758
 le:
     loss:1517.956489
     acc:0.999624
 encoder:
     loss:0.146830
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17472.594434
     acc:0.843539
 tv:
     loss:19099.730762
     acc:0.491248
 lu:
     loss:13422.316943
     acc:0.912033
 lv:
     loss:16742.113281
     acc:0.191437
 le:
     loss:1448.627734
     acc:0.999107
 encoder:
     loss:0.131094
----------------------------


Epoch: [548/1000]:
train:
----------------------------
 tu:
     loss:18322.850291
     acc:0.847032
 tv:
     loss:19835.683367
     acc:0.535212
 lu:
     loss:14044.988043
     acc:0.921351
 lv:
     loss:17094.065237
     acc:0.291727
 le:
     loss:1518.119974
     acc:0.999593
 encoder:
     loss:0.176075
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17433.484082
     acc:0.851904
 tv:
     loss:19103.687793
     acc:0.490727
 lu:
     loss:13420.475879
     acc:0.912553
 lv:
     loss:16746.608789
     acc:0.189956
 le:
     loss:1448.294971
     acc:0.999190
 encoder:
     loss:0.172802
----------------------------


Epoch: [549/1000]:
train:
----------------------------
 tu:
     loss:18292.483410
     acc:0.853482
 tv:
     loss:19835.252975
     acc:0.535494
 lu:
     loss:14044.211448
     acc:0.921517
 lv:
     loss:17095.864814
     acc:0.291344
 le:
     loss:1518.023432
     acc:0.999619
 encoder:
     loss:0.199467
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.468555
     acc:0.852233
 tv:
     loss:19102.494141
     acc:0.490457
 lu:
     loss:13420.738184
     acc:0.912321
 lv:
     loss:16748.113184
     acc:0.189411
 le:
     loss:1448.471814
     acc:0.999148
 encoder:
     loss:0.237862
----------------------------


Epoch: [550/1000]:
train:
----------------------------
 tu:
     loss:18290.155353
     acc:0.853908
 tv:
     loss:19835.173783
     acc:0.535279
 lu:
     loss:14044.490484
     acc:0.921592
 lv:
     loss:17093.086971
     acc:0.291738
 le:
     loss:1517.975299
     acc:0.999620
 encoder:
     loss:0.255775
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.541797
     acc:0.852131
 tv:
     loss:19100.612109
     acc:0.490893
 lu:
     loss:13419.214551
     acc:0.912617
 lv:
     loss:16745.782715
     acc:0.190045
 le:
     loss:1448.934741
     acc:0.999025
 encoder:
     loss:0.187694
----------------------------


Epoch: [551/1000]:
train:
----------------------------
 tu:
     loss:18289.661973
     acc:0.853938
 tv:
     loss:19832.824026
     acc:0.536132
 lu:
     loss:14044.994799
     acc:0.921457
 lv:
     loss:17088.981354
     acc:0.293253
 le:
     loss:1518.074778
     acc:0.999597
 encoder:
     loss:0.196799
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.377930
     acc:0.852177
 tv:
     loss:19104.747852
     acc:0.489992
 lu:
     loss:13418.123340
     acc:0.913072
 lv:
     loss:16746.367285
     acc:0.190041
 le:
     loss:1448.766406
     acc:0.999105
 encoder:
     loss:0.178020
----------------------------


Epoch: [552/1000]:
train:
----------------------------
 tu:
     loss:18288.302360
     acc:0.854244
 tv:
     loss:19834.245878
     acc:0.535607
 lu:
     loss:14045.167901
     acc:0.921355
 lv:
     loss:17092.250568
     acc:0.292261
 le:
     loss:1518.023604
     acc:0.999623
 encoder:
     loss:0.177046
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.130762
     acc:0.852399
 tv:
     loss:19100.297559
     acc:0.491027
 lu:
     loss:13419.824707
     acc:0.912695
 lv:
     loss:16749.474609
     acc:0.189499
 le:
     loss:1448.872473
     acc:0.999043
 encoder:
     loss:0.154529
----------------------------


Epoch: [553/1000]:
train:
----------------------------
 tu:
     loss:18288.296909
     acc:0.854169
 tv:
     loss:19833.302507
     acc:0.535907
 lu:
     loss:14045.210131
     acc:0.921350
 lv:
     loss:17091.914971
     acc:0.292654
 le:
     loss:1517.979474
     acc:0.999616
 encoder:
     loss:0.147790
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.887109
     acc:0.852353
 tv:
     loss:19099.603809
     acc:0.491035
 lu:
     loss:13421.062207
     acc:0.912043
 lv:
     loss:16750.958887
     acc:0.188636
 le:
     loss:1448.699512
     acc:0.999106
 encoder:
     loss:0.217016
----------------------------


Epoch: [554/1000]:
train:
----------------------------
 tu:
     loss:18288.139546
     acc:0.854252
 tv:
     loss:19832.873649
     acc:0.535866
 lu:
     loss:14045.544411
     acc:0.921352
 lv:
     loss:17088.329261
     acc:0.292994
 le:
     loss:1518.096780
     acc:0.999597
 encoder:
     loss:0.184568
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.053418
     acc:0.852411
 tv:
     loss:19106.509570
     acc:0.489698
 lu:
     loss:13420.052441
     acc:0.912720
 lv:
     loss:16751.718652
     acc:0.188794
 le:
     loss:1448.627844
     acc:0.999127
 encoder:
     loss:0.144292
----------------------------


Epoch: [555/1000]:
train:
----------------------------
 tu:
     loss:18289.044184
     acc:0.853999
 tv:
     loss:19832.350711
     acc:0.536102
 lu:
     loss:14045.054120
     acc:0.921345
 lv:
     loss:17090.616949
     acc:0.292755
 le:
     loss:1517.916582
     acc:0.999645
 encoder:
     loss:0.152239
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.096875
     acc:0.852253
 tv:
     loss:19104.961914
     acc:0.489973
 lu:
     loss:13418.780273
     acc:0.912880
 lv:
     loss:16745.769043
     acc:0.190073
 le:
     loss:1448.873419
     acc:0.999086
 encoder:
     loss:0.154329
----------------------------


Epoch: [556/1000]:
train:
----------------------------
 tu:
     loss:18289.010765
     acc:0.854013
 tv:
     loss:19831.192723
     acc:0.536272
 lu:
     loss:14044.485976
     acc:0.921607
 lv:
     loss:17089.050293
     acc:0.292934
 le:
     loss:1518.015567
     acc:0.999608
 encoder:
     loss:0.180843
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.217676
     acc:0.852371
 tv:
     loss:19103.340039
     acc:0.490300
 lu:
     loss:13417.425732
     acc:0.913071
 lv:
     loss:16742.852051
     acc:0.190685
 le:
     loss:1448.487048
     acc:0.999168
 encoder:
     loss:0.182619
----------------------------


Epoch: [557/1000]:
train:
----------------------------
 tu:
     loss:18288.515216
     acc:0.854116
 tv:
     loss:19833.178518
     acc:0.535892
 lu:
     loss:14043.761594
     acc:0.921691
 lv:
     loss:17088.378509
     acc:0.292768
 le:
     loss:1518.115523
     acc:0.999587
 encoder:
     loss:0.207867
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.675000
     acc:0.852088
 tv:
     loss:19104.249219
     acc:0.490059
 lu:
     loss:13418.601660
     acc:0.912732
 lv:
     loss:16743.971875
     acc:0.190374
 le:
     loss:1449.600665
     acc:0.998878
 encoder:
     loss:0.181895
----------------------------


Epoch: [558/1000]:
train:
----------------------------
 tu:
     loss:18287.915607
     acc:0.854272
 tv:
     loss:19833.502850
     acc:0.535648
 lu:
     loss:14043.454454
     acc:0.921711
 lv:
     loss:17088.714117
     acc:0.292744
 le:
     loss:1518.045192
     acc:0.999607
 encoder:
     loss:0.334671
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.267383
     acc:0.852291
 tv:
     loss:19099.930957
     acc:0.490991
 lu:
     loss:13418.287646
     acc:0.913060
 lv:
     loss:16743.376270
     acc:0.190206
 le:
     loss:1448.968744
     acc:0.999044
 encoder:
     loss:0.180890
----------------------------


Epoch: [559/1000]:
train:
----------------------------
 tu:
     loss:18288.387877
     acc:0.854220
 tv:
     loss:19831.461221
     acc:0.536187
 lu:
     loss:14043.508335
     acc:0.921781
 lv:
     loss:17084.272665
     acc:0.293976
 le:
     loss:1517.971306
     acc:0.999620
 encoder:
     loss:0.175496
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.515234
     acc:0.852462
 tv:
     loss:19099.408496
     acc:0.491161
 lu:
     loss:13417.664551
     acc:0.913103
 lv:
     loss:16745.039648
     acc:0.190057
 le:
     loss:1448.447589
     acc:0.999127
 encoder:
     loss:0.137422
----------------------------


Epoch: [560/1000]:
train:
----------------------------
 tu:
     loss:18288.524062
     acc:0.854028
 tv:
     loss:19831.021280
     acc:0.536562
 lu:
     loss:14044.080362
     acc:0.921635
 lv:
     loss:17082.833848
     acc:0.294340
 le:
     loss:1518.056072
     acc:0.999610
 encoder:
     loss:0.163582
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.768555
     acc:0.852108
 tv:
     loss:19103.323535
     acc:0.490383
 lu:
     loss:13417.747217
     acc:0.913191
 lv:
     loss:16747.216797
     acc:0.189842
 le:
     loss:1448.841815
     acc:0.999083
 encoder:
     loss:0.143219
----------------------------


Epoch: [561/1000]:
train:
----------------------------
 tu:
     loss:18288.586040
     acc:0.854044
 tv:
     loss:19831.746798
     acc:0.536251
 lu:
     loss:14043.838583
     acc:0.921773
 lv:
     loss:17085.385174
     acc:0.293948
 le:
     loss:1518.040262
     acc:0.999611
 encoder:
     loss:0.147726
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.806641
     acc:0.852535
 tv:
     loss:19101.323535
     acc:0.490858
 lu:
     loss:13419.545361
     acc:0.912621
 lv:
     loss:16747.594922
     acc:0.189699
 le:
     loss:1448.318317
     acc:0.999208
 encoder:
     loss:0.148863
----------------------------


Epoch: [562/1000]:
train:
----------------------------
 tu:
     loss:18287.988259
     acc:0.854295
 tv:
     loss:19830.466218
     acc:0.536267
 lu:
     loss:14044.101608
     acc:0.921737
 lv:
     loss:17086.079385
     acc:0.293259
 le:
     loss:1517.890421
     acc:0.999639
 encoder:
     loss:0.177094
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.596289
     acc:0.852276
 tv:
     loss:19103.268066
     acc:0.490652
 lu:
     loss:13418.958643
     acc:0.912700
 lv:
     loss:16744.009180
     acc:0.190685
 le:
     loss:1448.350555
     acc:0.999168
 encoder:
     loss:0.144940
----------------------------


Epoch: [563/1000]:
train:
----------------------------
 tu:
     loss:18287.625250
     acc:0.854259
 tv:
     loss:19830.277026
     acc:0.536322
 lu:
     loss:14042.982785
     acc:0.921810
 lv:
     loss:17085.975188
     acc:0.293314
 le:
     loss:1518.071127
     acc:0.999600
 encoder:
     loss:0.165245
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.838477
     acc:0.852153
 tv:
     loss:19103.382031
     acc:0.490271
 lu:
     loss:13418.534961
     acc:0.912860
 lv:
     loss:16744.585254
     acc:0.189771
 le:
     loss:1448.639264
     acc:0.999144
 encoder:
     loss:0.194557
----------------------------


Epoch: [564/1000]:
train:
----------------------------
 tu:
     loss:18288.427519
     acc:0.854109
 tv:
     loss:19830.955771
     acc:0.536394
 lu:
     loss:14044.042480
     acc:0.921668
 lv:
     loss:17085.478663
     acc:0.293882
 le:
     loss:1518.002908
     acc:0.999624
 encoder:
     loss:0.200779
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.816797
     acc:0.852435
 tv:
     loss:19103.229199
     acc:0.490557
 lu:
     loss:13418.948437
     acc:0.912770
 lv:
     loss:16744.618359
     acc:0.190943
 le:
     loss:1448.637952
     acc:0.999163
 encoder:
     loss:0.196801
----------------------------


Epoch: [565/1000]:
train:
----------------------------
 tu:
     loss:18287.884062
     acc:0.854160
 tv:
     loss:19830.867449
     acc:0.536382
 lu:
     loss:14045.470067
     acc:0.921290
 lv:
     loss:17088.051281
     acc:0.293000
 le:
     loss:1517.921517
     acc:0.999628
 encoder:
     loss:0.190064
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.344922
     acc:0.852254
 tv:
     loss:19099.188867
     acc:0.491547
 lu:
     loss:13420.487744
     acc:0.912537
 lv:
     loss:16742.267676
     acc:0.190681
 le:
     loss:1448.763654
     acc:0.999066
 encoder:
     loss:0.189136
----------------------------


Epoch: [566/1000]:
train:
----------------------------
 tu:
     loss:18287.990564
     acc:0.854264
 tv:
     loss:19829.461925
     acc:0.536528
 lu:
     loss:14043.503997
     acc:0.921786
 lv:
     loss:17086.472645
     acc:0.293316
 le:
     loss:1517.913530
     acc:0.999645
 encoder:
     loss:0.205698
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.383789
     acc:0.852277
 tv:
     loss:19104.222656
     acc:0.489920
 lu:
     loss:13416.072363
     acc:0.913424
 lv:
     loss:16752.395410
     acc:0.188416
 le:
     loss:1448.609772
     acc:0.999107
 encoder:
     loss:0.169167
----------------------------


Epoch: [567/1000]:
train:
----------------------------
 tu:
     loss:18288.119538
     acc:0.854177
 tv:
     loss:19830.130723
     acc:0.536583
 lu:
     loss:14044.447095
     acc:0.921468
 lv:
     loss:17083.730753
     acc:0.293811
 le:
     loss:1517.897565
     acc:0.999644
 encoder:
     loss:0.188898
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.891504
     acc:0.852377
 tv:
     loss:19102.149609
     acc:0.490476
 lu:
     loss:13416.694580
     acc:0.913313
 lv:
     loss:16748.819727
     acc:0.189194
 le:
     loss:1448.446582
     acc:0.999148
 encoder:
     loss:0.135525
----------------------------


Epoch: [568/1000]:
train:
----------------------------
 tu:
     loss:18287.428041
     acc:0.854313
 tv:
     loss:19831.997547
     acc:0.535992
 lu:
     loss:14044.985760
     acc:0.921371
 lv:
     loss:17084.017340
     acc:0.293765
 le:
     loss:1518.136981
     acc:0.999594
 encoder:
     loss:0.160928
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.538965
     acc:0.852150
 tv:
     loss:19105.388672
     acc:0.490025
 lu:
     loss:13418.187207
     acc:0.913021
 lv:
     loss:16747.461914
     acc:0.189596
 le:
     loss:1448.787598
     acc:0.999059
 encoder:
     loss:0.172970
----------------------------


Epoch: [569/1000]:
train:
----------------------------
 tu:
     loss:18288.080180
     acc:0.854204
 tv:
     loss:19831.819938
     acc:0.536074
 lu:
     loss:14043.845192
     acc:0.921591
 lv:
     loss:17086.755769
     acc:0.293447
 le:
     loss:1517.933594
     acc:0.999625
 encoder:
     loss:0.199536
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.365527
     acc:0.852170
 tv:
     loss:19105.362207
     acc:0.489707
 lu:
     loss:13417.562695
     acc:0.913236
 lv:
     loss:16742.481055
     acc:0.191313
 le:
     loss:1448.652209
     acc:0.999105
 encoder:
     loss:0.194172
----------------------------


Epoch: [570/1000]:
train:
----------------------------
 tu:
     loss:18288.318689
     acc:0.854150
 tv:
     loss:19831.512480
     acc:0.536087
 lu:
     loss:14044.285156
     acc:0.921594
 lv:
     loss:17087.156125
     acc:0.293055
 le:
     loss:1518.054594
     acc:0.999606
 encoder:
     loss:0.183802
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.287500
     acc:0.852295
 tv:
     loss:19102.001074
     acc:0.490534
 lu:
     loss:13418.332373
     acc:0.912659
 lv:
     loss:16747.652441
     acc:0.189658
 le:
     loss:1448.813287
     acc:0.999083
 encoder:
     loss:0.171617
----------------------------


Epoch: [571/1000]:
train:
----------------------------
 tu:
     loss:18288.616552
     acc:0.854053
 tv:
     loss:19830.486714
     acc:0.536540
 lu:
     loss:14043.766113
     acc:0.921663
 lv:
     loss:17081.868323
     acc:0.294239
 le:
     loss:1517.982605
     acc:0.999610
 encoder:
     loss:0.226754
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.594336
     acc:0.852191
 tv:
     loss:19106.476367
     acc:0.489843
 lu:
     loss:13418.274170
     acc:0.913052
 lv:
     loss:16746.337500
     acc:0.190091
 le:
     loss:1448.792938
     acc:0.999122
 encoder:
     loss:0.256748
----------------------------


Epoch: [572/1000]:
train:
----------------------------
 tu:
     loss:18287.876647
     acc:0.854168
 tv:
     loss:19831.645360
     acc:0.536068
 lu:
     loss:14045.559514
     acc:0.921299
 lv:
     loss:17085.187375
     acc:0.293775
 le:
     loss:1517.924849
     acc:0.999635
 encoder:
     loss:0.186614
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.979395
     acc:0.852256
 tv:
     loss:19103.588184
     acc:0.490266
 lu:
     loss:13419.341260
     acc:0.912785
 lv:
     loss:16746.474902
     acc:0.189627
 le:
     loss:1448.732983
     acc:0.999086
 encoder:
     loss:0.136742
----------------------------


Epoch: [573/1000]:
train:
----------------------------
 tu:
     loss:18287.855650
     acc:0.854114
 tv:
     loss:19830.604810
     acc:0.536256
 lu:
     loss:14044.657454
     acc:0.921526
 lv:
     loss:17084.842876
     acc:0.293953
 le:
     loss:1518.074796
     acc:0.999604
 encoder:
     loss:0.153118
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.515820
     acc:0.852213
 tv:
     loss:19109.323242
     acc:0.488963
 lu:
     loss:13418.939307
     acc:0.912676
 lv:
     loss:16743.768945
     acc:0.190422
 le:
     loss:1449.219257
     acc:0.998981
 encoder:
     loss:0.150454
----------------------------


Epoch: [574/1000]:
train:
----------------------------
 tu:
     loss:18288.546387
     acc:0.853939
 tv:
     loss:19830.141068
     acc:0.536746
 lu:
     loss:14043.639705
     acc:0.921710
 lv:
     loss:17081.143078
     acc:0.294858
 le:
     loss:1517.954774
     acc:0.999626
 encoder:
     loss:0.180818
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.254590
     acc:0.852271
 tv:
     loss:19100.301563
     acc:0.491155
 lu:
     loss:13419.950732
     acc:0.912569
 lv:
     loss:16748.273242
     acc:0.189278
 le:
     loss:1448.967187
     acc:0.999044
 encoder:
     loss:0.206283
----------------------------


Epoch: [575/1000]:
train:
----------------------------
 tu:
     loss:18288.060274
     acc:0.854015
 tv:
     loss:19830.405308
     acc:0.536607
 lu:
     loss:14044.237361
     acc:0.921566
 lv:
     loss:17082.719261
     acc:0.294560
 le:
     loss:1517.946217
     acc:0.999627
 encoder:
     loss:0.201603
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.215723
     acc:0.852320
 tv:
     loss:19103.754395
     acc:0.490285
 lu:
     loss:13416.593066
     acc:0.913407
 lv:
     loss:16745.161523
     acc:0.190459
 le:
     loss:1448.539954
     acc:0.999149
 encoder:
     loss:0.168995
----------------------------


Epoch: [576/1000]:
train:
----------------------------
 tu:
     loss:18287.401583
     acc:0.854243
 tv:
     loss:19829.120356
     acc:0.536734
 lu:
     loss:14044.122138
     acc:0.921672
 lv:
     loss:17081.703693
     acc:0.294874
 le:
     loss:1518.032865
     acc:0.999604
 encoder:
     loss:0.267290
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.069922
     acc:0.852294
 tv:
     loss:19101.658496
     acc:0.490888
 lu:
     loss:13419.477734
     acc:0.912553
 lv:
     loss:16752.265137
     acc:0.189047
 le:
     loss:1448.836932
     acc:0.999067
 encoder:
     loss:0.210038
----------------------------


Epoch: [577/1000]:
train:
----------------------------
 tu:
     loss:18287.479492
     acc:0.854286
 tv:
     loss:19829.786485
     acc:0.536499
 lu:
     loss:14043.800259
     acc:0.921621
 lv:
     loss:17081.186183
     acc:0.294584
 le:
     loss:1517.926928
     acc:0.999639
 encoder:
     loss:0.195150
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.442285
     acc:0.852350
 tv:
     loss:19105.433203
     acc:0.490150
 lu:
     loss:13419.507031
     acc:0.912692
 lv:
     loss:16748.271777
     acc:0.190093
 le:
     loss:1448.603503
     acc:0.999106
 encoder:
     loss:0.165061
----------------------------


Epoch: [578/1000]:
train:
----------------------------
 tu:
     loss:18287.842683
     acc:0.854278
 tv:
     loss:19830.257131
     acc:0.536527
 lu:
     loss:14042.965888
     acc:0.921848
 lv:
     loss:17080.883698
     acc:0.294548
 le:
     loss:1518.073725
     acc:0.999602
 encoder:
     loss:0.204280
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.868652
     acc:0.852319
 tv:
     loss:19105.746973
     acc:0.489645
 lu:
     loss:13418.862012
     acc:0.912900
 lv:
     loss:16742.625488
     acc:0.190691
 le:
     loss:1448.542468
     acc:0.999125
 encoder:
     loss:0.149592
----------------------------


Epoch: [579/1000]:
train:
----------------------------
 tu:
     loss:18288.269906
     acc:0.854077
 tv:
     loss:19829.942951
     acc:0.536513
 lu:
     loss:14043.284395
     acc:0.921887
 lv:
     loss:17079.631632
     acc:0.294826
 le:
     loss:1517.935591
     acc:0.999630
 encoder:
     loss:0.165856
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.362207
     acc:0.852310
 tv:
     loss:19102.707129
     acc:0.490298
 lu:
     loss:13418.665723
     acc:0.912821
 lv:
     loss:16738.582324
     acc:0.192166
 le:
     loss:1448.827991
     acc:0.999082
 encoder:
     loss:0.192343
----------------------------


Epoch: [580/1000]:
train:
----------------------------
 tu:
     loss:18288.114417
     acc:0.854158
 tv:
     loss:19830.998388
     acc:0.536053
 lu:
     loss:14043.320006
     acc:0.921737
 lv:
     loss:17080.807447
     acc:0.294447
 le:
     loss:1518.079891
     acc:0.999586
 encoder:
     loss:0.234022
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.273438
     acc:0.852291
 tv:
     loss:19101.877441
     acc:0.490615
 lu:
     loss:13418.981006
     acc:0.912746
 lv:
     loss:16753.959961
     acc:0.188522
 le:
     loss:1448.970209
     acc:0.999041
 encoder:
     loss:0.183722
----------------------------


Epoch: [581/1000]:
train:
----------------------------
 tu:
     loss:18287.954737
     acc:0.854142
 tv:
     loss:19826.584734
     acc:0.537029
 lu:
     loss:14043.376283
     acc:0.921814
 lv:
     loss:17078.658180
     acc:0.294984
 le:
     loss:1518.019685
     acc:0.999617
 encoder:
     loss:0.248485
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.092187
     acc:0.852209
 tv:
     loss:19104.738867
     acc:0.490056
 lu:
     loss:13418.674854
     acc:0.912818
 lv:
     loss:16744.261914
     acc:0.190994
 le:
     loss:1448.798865
     acc:0.999082
 encoder:
     loss:0.205629
----------------------------


Epoch: [582/1000]:
train:
----------------------------
 tu:
     loss:18287.116075
     acc:0.854378
 tv:
     loss:19828.684968
     acc:0.536864
 lu:
     loss:14043.185774
     acc:0.921815
 lv:
     loss:17076.636866
     acc:0.295594
 le:
     loss:1518.169070
     acc:0.999579
 encoder:
     loss:0.172027
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.563867
     acc:0.852209
 tv:
     loss:19102.898340
     acc:0.490669
 lu:
     loss:13419.516309
     acc:0.912654
 lv:
     loss:16747.017969
     acc:0.189937
 le:
     loss:1448.700024
     acc:0.999141
 encoder:
     loss:0.134327
----------------------------


Epoch: [583/1000]:
train:
----------------------------
 tu:
     loss:18287.029274
     acc:0.854394
 tv:
     loss:19827.869845
     acc:0.536817
 lu:
     loss:14042.654320
     acc:0.921947
 lv:
     loss:17075.620242
     acc:0.295707
 le:
     loss:1517.991122
     acc:0.999623
 encoder:
     loss:0.157539
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.146289
     acc:0.852316
 tv:
     loss:19101.102344
     acc:0.491287
 lu:
     loss:13419.760010
     acc:0.912461
 lv:
     loss:16744.968262
     acc:0.190874
 le:
     loss:1448.903552
     acc:0.999041
 encoder:
     loss:0.316603
----------------------------


Epoch: [584/1000]:
train:
----------------------------
 tu:
     loss:18286.729004
     acc:0.854322
 tv:
     loss:19827.192054
     acc:0.537154
 lu:
     loss:14042.723633
     acc:0.922018
 lv:
     loss:17075.508482
     acc:0.295863
 le:
     loss:1518.028402
     acc:0.999607
 encoder:
     loss:0.273773
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.975391
     acc:0.852355
 tv:
     loss:19095.817383
     acc:0.492166
 lu:
     loss:13418.925244
     acc:0.912888
 lv:
     loss:16741.223438
     acc:0.191067
 le:
     loss:1448.710773
     acc:0.999106
 encoder:
     loss:0.209860
----------------------------


Epoch: [585/1000]:
train:
----------------------------
 tu:
     loss:18287.230685
     acc:0.854296
 tv:
     loss:19828.562557
     acc:0.536974
 lu:
     loss:14042.846509
     acc:0.921869
 lv:
     loss:17078.289165
     acc:0.295250
 le:
     loss:1518.011259
     acc:0.999614
 encoder:
     loss:0.208954
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.673730
     acc:0.852315
 tv:
     loss:19103.250781
     acc:0.490135
 lu:
     loss:13416.217529
     acc:0.913452
 lv:
     loss:16739.515527
     acc:0.191368
 le:
     loss:1448.635980
     acc:0.999141
 encoder:
     loss:0.152106
----------------------------


Epoch: [586/1000]:
train:
----------------------------
 tu:
     loss:18287.797204
     acc:0.854188
 tv:
     loss:19827.228572
     acc:0.536999
 lu:
     loss:14043.097713
     acc:0.921796
 lv:
     loss:17079.133074
     acc:0.294907
 le:
     loss:1518.002935
     acc:0.999618
 encoder:
     loss:0.191235
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.167383
     acc:0.852291
 tv:
     loss:19108.107520
     acc:0.489042
 lu:
     loss:13418.485937
     acc:0.912949
 lv:
     loss:16746.737598
     acc:0.189968
 le:
     loss:1448.788049
     acc:0.999060
 encoder:
     loss:0.178635
----------------------------


Epoch: [587/1000]:
train:
----------------------------
 tu:
     loss:18287.575139
     acc:0.854287
 tv:
     loss:19827.981570
     acc:0.536876
 lu:
     loss:14041.717206
     acc:0.921888
 lv:
     loss:17075.863281
     acc:0.295354
 le:
     loss:1517.936664
     acc:0.999637
 encoder:
     loss:0.182233
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.297266
     acc:0.852295
 tv:
     loss:19099.942285
     acc:0.491050
 lu:
     loss:13418.905469
     acc:0.912868
 lv:
     loss:16748.862793
     acc:0.189729
 le:
     loss:1448.621204
     acc:0.999128
 encoder:
     loss:0.160561
----------------------------


Epoch: [588/1000]:
train:
----------------------------
 tu:
     loss:18287.534134
     acc:0.854261
 tv:
     loss:19828.269474
     acc:0.536816
 lu:
     loss:14044.200161
     acc:0.921713
 lv:
     loss:17077.207009
     acc:0.295433
 le:
     loss:1517.933476
     acc:0.999625
 encoder:
     loss:0.279622
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.619043
     acc:0.852194
 tv:
     loss:19101.935156
     acc:0.490392
 lu:
     loss:13418.555127
     acc:0.912992
 lv:
     loss:16746.865234
     acc:0.189900
 le:
     loss:1448.514368
     acc:0.999146
 encoder:
     loss:0.300153
----------------------------


Epoch: [589/1000]:
train:
----------------------------
 tu:
     loss:18287.436535
     acc:0.854308
 tv:
     loss:19826.159430
     acc:0.537137
 lu:
     loss:14041.612997
     acc:0.922282
 lv:
     loss:17077.086199
     acc:0.295349
 le:
     loss:1517.915713
     acc:0.999627
 encoder:
     loss:0.184723
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.915625
     acc:0.852380
 tv:
     loss:19102.712402
     acc:0.490274
 lu:
     loss:13419.541211
     acc:0.912713
 lv:
     loss:16746.697461
     acc:0.189967
 le:
     loss:1448.859393
     acc:0.999084
 encoder:
     loss:0.192062
----------------------------


Epoch: [590/1000]:
train:
----------------------------
 tu:
     loss:18287.834166
     acc:0.854230
 tv:
     loss:19829.034782
     acc:0.536753
 lu:
     loss:14043.300588
     acc:0.921810
 lv:
     loss:17078.288120
     acc:0.294954
 le:
     loss:1518.120560
     acc:0.999591
 encoder:
     loss:0.198687
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.083691
     acc:0.852275
 tv:
     loss:19101.605664
     acc:0.490817
 lu:
     loss:13417.961084
     acc:0.913179
 lv:
     loss:16751.253809
     acc:0.189256
 le:
     loss:1448.670563
     acc:0.999107
 encoder:
     loss:0.161868
----------------------------


Epoch: [591/1000]:
train:
----------------------------
 tu:
     loss:18287.203250
     acc:0.854281
 tv:
     loss:19827.919729
     acc:0.536807
 lu:
     loss:14042.224360
     acc:0.922038
 lv:
     loss:17075.379043
     acc:0.295719
 le:
     loss:1517.985105
     acc:0.999616
 encoder:
     loss:0.224933
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.287109
     acc:0.852518
 tv:
     loss:19100.606348
     acc:0.490788
 lu:
     loss:13416.794873
     acc:0.913274
 lv:
     loss:16751.721484
     acc:0.188722
 le:
     loss:1448.882843
     acc:0.999081
 encoder:
     loss:0.219170
----------------------------


Epoch: [592/1000]:
train:
----------------------------
 tu:
     loss:18287.682515
     acc:0.854091
 tv:
     loss:19827.268441
     acc:0.537063
 lu:
     loss:14042.200979
     acc:0.922073
 lv:
     loss:17076.358762
     acc:0.295675
 le:
     loss:1517.915417
     acc:0.999637
 encoder:
     loss:0.209780
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.103418
     acc:0.852349
 tv:
     loss:19098.802734
     acc:0.491318
 lu:
     loss:13417.016113
     acc:0.913342
 lv:
     loss:16741.798242
     acc:0.191107
 le:
     loss:1449.042639
     acc:0.999020
 encoder:
     loss:0.232598
----------------------------


Epoch: [593/1000]:
train:
----------------------------
 tu:
     loss:18287.375908
     acc:0.854214
 tv:
     loss:19827.348020
     acc:0.537187
 lu:
     loss:14042.062091
     acc:0.921993
 lv:
     loss:17077.579737
     acc:0.295290
 le:
     loss:1518.046554
     acc:0.999607
 encoder:
     loss:0.255033
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.498340
     acc:0.852255
 tv:
     loss:19099.467090
     acc:0.491503
 lu:
     loss:13417.218311
     acc:0.913057
 lv:
     loss:16746.580078
     acc:0.190416
 le:
     loss:1448.554633
     acc:0.999125
 encoder:
     loss:0.238804
----------------------------


Epoch: [594/1000]:
train:
----------------------------
 tu:
     loss:18287.420206
     acc:0.854242
 tv:
     loss:19827.859625
     acc:0.536901
 lu:
     loss:14041.627771
     acc:0.922070
 lv:
     loss:17076.718943
     acc:0.295434
 le:
     loss:1517.972482
     acc:0.999624
 encoder:
     loss:0.323316
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.224707
     acc:0.852350
 tv:
     loss:19102.386133
     acc:0.490521
 lu:
     loss:13415.674512
     acc:0.913417
 lv:
     loss:16747.695508
     acc:0.189274
 le:
     loss:1448.590912
     acc:0.999127
 encoder:
     loss:0.601264
----------------------------


Epoch: [595/1000]:
train:
----------------------------
 tu:
     loss:18286.955612
     acc:0.854336
 tv:
     loss:19826.610034
     acc:0.537419
 lu:
     loss:14040.792083
     acc:0.922349
 lv:
     loss:17071.780501
     acc:0.296827
 le:
     loss:1517.976494
     acc:0.999625
 encoder:
     loss:0.355722
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.910156
     acc:0.852293
 tv:
     loss:19105.902246
     acc:0.489756
 lu:
     loss:13415.605664
     acc:0.913478
 lv:
     loss:16749.546973
     acc:0.189558
 le:
     loss:1448.686340
     acc:0.999107
 encoder:
     loss:0.259472
----------------------------


Epoch: [596/1000]:
train:
----------------------------
 tu:
     loss:18287.734057
     acc:0.854160
 tv:
     loss:19826.971771
     acc:0.536995
 lu:
     loss:14040.165913
     acc:0.922522
 lv:
     loss:17074.001306
     acc:0.296069
 le:
     loss:1517.973793
     acc:0.999622
 encoder:
     loss:0.200772
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.228027
     acc:0.852250
 tv:
     loss:19102.419922
     acc:0.490282
 lu:
     loss:13416.642578
     acc:0.912906
 lv:
     loss:16748.631055
     acc:0.189663
 le:
     loss:1448.548987
     acc:0.999148
 encoder:
     loss:0.187533
----------------------------


Epoch: [597/1000]:
train:
----------------------------
 tu:
     loss:18287.636458
     acc:0.854230
 tv:
     loss:19825.188704
     acc:0.537215
 lu:
     loss:14040.930891
     acc:0.922346
 lv:
     loss:17073.057231
     acc:0.296294
 le:
     loss:1517.921832
     acc:0.999634
 encoder:
     loss:0.226072
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.279102
     acc:0.852340
 tv:
     loss:19103.609668
     acc:0.490261
 lu:
     loss:13417.453320
     acc:0.912923
 lv:
     loss:16749.411426
     acc:0.189149
 le:
     loss:1448.696436
     acc:0.999106
 encoder:
     loss:0.172171
----------------------------


Epoch: [598/1000]:
train:
----------------------------
 tu:
     loss:18286.666322
     acc:0.854315
 tv:
     loss:19825.933083
     acc:0.537379
 lu:
     loss:14040.524789
     acc:0.922252
 lv:
     loss:17074.057788
     acc:0.296214
 le:
     loss:1517.876878
     acc:0.999649
 encoder:
     loss:0.177447
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.786523
     acc:0.852151
 tv:
     loss:19102.450586
     acc:0.490196
 lu:
     loss:13418.403418
     acc:0.912793
 lv:
     loss:16751.736426
     acc:0.188636
 le:
     loss:1449.054846
     acc:0.999003
 encoder:
     loss:0.159904
----------------------------


Epoch: [599/1000]:
train:
----------------------------
 tu:
     loss:18287.090616
     acc:0.854319
 tv:
     loss:19827.112861
     acc:0.537020
 lu:
     loss:14041.258744
     acc:0.922249
 lv:
     loss:17074.078398
     acc:0.295961
 le:
     loss:1518.013785
     acc:0.999609
 encoder:
     loss:0.185042
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.137793
     acc:0.852236
 tv:
     loss:19101.774512
     acc:0.490850
 lu:
     loss:13415.607422
     acc:0.913512
 lv:
     loss:16751.543848
     acc:0.188620
 le:
     loss:1448.808099
     acc:0.999105
 encoder:
     loss:0.250846
----------------------------


Epoch: [600/1000]:
train:
----------------------------
 tu:
     loss:18286.795331
     acc:0.854497
 tv:
     loss:19829.547727
     acc:0.536579
 lu:
     loss:14041.563681
     acc:0.922154
 lv:
     loss:17076.390023
     acc:0.295447
 le:
     loss:1517.851440
     acc:0.999639
 encoder:
     loss:0.233961
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.219531
     acc:0.852215
 tv:
     loss:19101.174805
     acc:0.490712
 lu:
     loss:13415.967188
     acc:0.913561
 lv:
     loss:16747.076953
     acc:0.189708
 le:
     loss:1448.792645
     acc:0.999086
 encoder:
     loss:0.195311
----------------------------


Epoch: [601/1000]:
train:
----------------------------
 tu:
     loss:18286.528809
     acc:0.854476
 tv:
     loss:19826.663779
     acc:0.537201
 lu:
     loss:14039.514330
     acc:0.922505
 lv:
     loss:17070.742040
     acc:0.296787
 le:
     loss:1517.903962
     acc:0.999636
 encoder:
     loss:0.202248
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.665820
     acc:0.852375
 tv:
     loss:19102.416113
     acc:0.490540
 lu:
     loss:13416.179590
     acc:0.913396
 lv:
     loss:16742.239648
     acc:0.191343
 le:
     loss:1448.843225
     acc:0.999065
 encoder:
     loss:0.181838
----------------------------


Epoch: [602/1000]:
train:
----------------------------
 tu:
     loss:18286.456305
     acc:0.854477
 tv:
     loss:19827.371116
     acc:0.536834
 lu:
     loss:14039.545001
     acc:0.922583
 lv:
     loss:17072.607115
     acc:0.296107
 le:
     loss:1518.062186
     acc:0.999600
 encoder:
     loss:0.232218
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.879395
     acc:0.852230
 tv:
     loss:19101.775781
     acc:0.490895
 lu:
     loss:13415.379688
     acc:0.913523
 lv:
     loss:16749.278027
     acc:0.189926
 le:
     loss:1449.101337
     acc:0.999004
 encoder:
     loss:0.358810
----------------------------


Epoch: [603/1000]:
train:
----------------------------
 tu:
     loss:18286.525209
     acc:0.854439
 tv:
     loss:19827.003441
     acc:0.537048
 lu:
     loss:14039.272620
     acc:0.922650
 lv:
     loss:17071.689828
     acc:0.296379
 le:
     loss:1517.928058
     acc:0.999644
 encoder:
     loss:0.279738
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.257031
     acc:0.852230
 tv:
     loss:19102.895898
     acc:0.490511
 lu:
     loss:13416.680078
     acc:0.913375
 lv:
     loss:16745.957813
     acc:0.189622
 le:
     loss:1448.976318
     acc:0.999045
 encoder:
     loss:0.234248
----------------------------


Epoch: [604/1000]:
train:
----------------------------
 tu:
     loss:18287.272983
     acc:0.854361
 tv:
     loss:19826.434377
     acc:0.537287
 lu:
     loss:14038.674839
     acc:0.922700
 lv:
     loss:17067.689635
     acc:0.297420
 le:
     loss:1518.009842
     acc:0.999609
 encoder:
     loss:0.202645
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.452344
     acc:0.852522
 tv:
     loss:19104.820117
     acc:0.490111
 lu:
     loss:13414.935645
     acc:0.913442
 lv:
     loss:16740.747168
     acc:0.191448
 le:
     loss:1448.708563
     acc:0.999107
 encoder:
     loss:0.166356
----------------------------


Epoch: [605/1000]:
train:
----------------------------
 tu:
     loss:18286.325456
     acc:0.854469
 tv:
     loss:19825.987804
     acc:0.537329
 lu:
     loss:14039.650447
     acc:0.922522
 lv:
     loss:17069.503111
     acc:0.297126
 le:
     loss:1517.862258
     acc:0.999649
 encoder:
     loss:0.199117
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.730176
     acc:0.852558
 tv:
     loss:19105.435156
     acc:0.489937
 lu:
     loss:13416.003125
     acc:0.913413
 lv:
     loss:16746.204492
     acc:0.190062
 le:
     loss:1448.935382
     acc:0.999043
 encoder:
     loss:0.212250
----------------------------


Epoch: [606/1000]:
train:
----------------------------
 tu:
     loss:18287.425611
     acc:0.854331
 tv:
     loss:19827.213913
     acc:0.537028
 lu:
     loss:14038.685308
     acc:0.922865
 lv:
     loss:17072.398914
     acc:0.296560
 le:
     loss:1517.943372
     acc:0.999633
 encoder:
     loss:0.199542
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.469336
     acc:0.852247
 tv:
     loss:19103.776367
     acc:0.490114
 lu:
     loss:13415.557910
     acc:0.913455
 lv:
     loss:16746.143359
     acc:0.190151
 le:
     loss:1448.812299
     acc:0.999104
 encoder:
     loss:0.176631
----------------------------


Epoch: [607/1000]:
train:
----------------------------
 tu:
     loss:18286.942917
     acc:0.854287
 tv:
     loss:19824.491824
     acc:0.537631
 lu:
     loss:14039.298567
     acc:0.922562
 lv:
     loss:17069.405410
     acc:0.296795
 le:
     loss:1517.816166
     acc:0.999657
 encoder:
     loss:0.263974
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.142969
     acc:0.852236
 tv:
     loss:19101.877441
     acc:0.490632
 lu:
     loss:13416.154346
     acc:0.913299
 lv:
     loss:16746.016992
     acc:0.189869
 le:
     loss:1448.835693
     acc:0.999086
 encoder:
     loss:0.232111
----------------------------


Epoch: [608/1000]:
train:
----------------------------
 tu:
     loss:18286.933446
     acc:0.854472
 tv:
     loss:19825.014592
     acc:0.537382
 lu:
     loss:14039.205703
     acc:0.922663
 lv:
     loss:17070.053370
     acc:0.296548
 le:
     loss:1518.060340
     acc:0.999606
 encoder:
     loss:0.246415
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.433203
     acc:0.852252
 tv:
     loss:19101.845703
     acc:0.490716
 lu:
     loss:13416.475684
     acc:0.913140
 lv:
     loss:16746.032227
     acc:0.190234
 le:
     loss:1448.780414
     acc:0.999106
 encoder:
     loss:0.289561
----------------------------


Epoch: [609/1000]:
train:
----------------------------
 tu:
     loss:18287.211199
     acc:0.854242
 tv:
     loss:19825.294195
     acc:0.537425
 lu:
     loss:14038.894781
     acc:0.922648
 lv:
     loss:17069.542935
     acc:0.297105
 le:
     loss:1517.870851
     acc:0.999640
 encoder:
     loss:0.246328
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.085742
     acc:0.852296
 tv:
     loss:19100.117285
     acc:0.491016
 lu:
     loss:13415.426172
     acc:0.913530
 lv:
     loss:16752.096875
     acc:0.188555
 le:
     loss:1448.922504
     acc:0.999063
 encoder:
     loss:0.194299
----------------------------


Epoch: [610/1000]:
train:
----------------------------
 tu:
     loss:18287.729924
     acc:0.854181
 tv:
     loss:19824.262287
     acc:0.537643
 lu:
     loss:14039.052155
     acc:0.922659
 lv:
     loss:17066.869345
     acc:0.297258
 le:
     loss:1518.028532
     acc:0.999609
 encoder:
     loss:0.179576
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.805664
     acc:0.852316
 tv:
     loss:19105.149414
     acc:0.489939
 lu:
     loss:13416.911182
     acc:0.913396
 lv:
     loss:16753.965527
     acc:0.187868
 le:
     loss:1448.648724
     acc:0.999086
 encoder:
     loss:0.163583
----------------------------


Epoch: [611/1000]:
train:
----------------------------
 tu:
     loss:18287.582236
     acc:0.854133
 tv:
     loss:19825.659032
     acc:0.537166
 lu:
     loss:14038.891431
     acc:0.922656
 lv:
     loss:17068.314816
     acc:0.297162
 le:
     loss:1517.911242
     acc:0.999635
 encoder:
     loss:0.187822
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.435254
     acc:0.852533
 tv:
     loss:19103.661719
     acc:0.490177
 lu:
     loss:13416.020557
     acc:0.913605
 lv:
     loss:16747.574609
     acc:0.189116
 le:
     loss:1448.878052
     acc:0.999066
 encoder:
     loss:0.612698
----------------------------


Epoch: [612/1000]:
train:
----------------------------
 tu:
     loss:18286.501408
     acc:0.854420
 tv:
     loss:19824.938340
     acc:0.537502
 lu:
     loss:14039.934320
     acc:0.922366
 lv:
     loss:17067.087947
     acc:0.297426
 le:
     loss:1518.015858
     acc:0.999613
 encoder:
     loss:0.551087
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.260352
     acc:0.852276
 tv:
     loss:19101.535840
     acc:0.490715
 lu:
     loss:13415.161670
     acc:0.913791
 lv:
     loss:16750.089355
     acc:0.189543
 le:
     loss:1449.190039
     acc:0.998980
 encoder:
     loss:0.291038
----------------------------


Epoch: [613/1000]:
train:
----------------------------
 tu:
     loss:18286.210109
     acc:0.854485
 tv:
     loss:19824.151867
     acc:0.537980
 lu:
     loss:14037.409077
     acc:0.922924
 lv:
     loss:17067.206793
     acc:0.297651
 le:
     loss:1517.963684
     acc:0.999631
 encoder:
     loss:0.223826
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.697559
     acc:0.852146
 tv:
     loss:19104.352246
     acc:0.490107
 lu:
     loss:13414.892578
     acc:0.913845
 lv:
     loss:16753.641211
     acc:0.188124
 le:
     loss:1449.119238
     acc:0.999003
 encoder:
     loss:0.204075
----------------------------


Epoch: [614/1000]:
train:
----------------------------
 tu:
     loss:18287.322958
     acc:0.854166
 tv:
     loss:19824.618925
     acc:0.537496
 lu:
     loss:14038.936285
     acc:0.922656
 lv:
     loss:17066.330805
     acc:0.297479
 le:
     loss:1517.869615
     acc:0.999637
 encoder:
     loss:0.193104
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.219824
     acc:0.852659
 tv:
     loss:19105.449316
     acc:0.489761
 lu:
     loss:13415.036328
     acc:0.913922
 lv:
     loss:16747.025879
     acc:0.189323
 le:
     loss:1448.937109
     acc:0.999082
 encoder:
     loss:0.292949
----------------------------


Epoch: [615/1000]:
train:
----------------------------
 tu:
     loss:18287.048556
     acc:0.854342
 tv:
     loss:19824.831759
     acc:0.537515
 lu:
     loss:14040.174373
     acc:0.922414
 lv:
     loss:17066.778241
     acc:0.297615
 le:
     loss:1517.935513
     acc:0.999637
 encoder:
     loss:0.288102
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.695117
     acc:0.852455
 tv:
     loss:19105.911914
     acc:0.489641
 lu:
     loss:13417.376318
     acc:0.913054
 lv:
     loss:16748.127148
     acc:0.189515
 le:
     loss:1448.836639
     acc:0.999065
 encoder:
     loss:0.202916
----------------------------


Epoch: [616/1000]:
train:
----------------------------
 tu:
     loss:18288.031148
     acc:0.854091
 tv:
     loss:19824.994890
     acc:0.537560
 lu:
     loss:14038.710165
     acc:0.922791
 lv:
     loss:17064.108978
     acc:0.298338
 le:
     loss:1518.047116
     acc:0.999611
 encoder:
     loss:0.220201
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.506250
     acc:0.852497
 tv:
     loss:19106.297949
     acc:0.489711
 lu:
     loss:13418.296533
     acc:0.913086
 lv:
     loss:16751.794336
     acc:0.188709
 le:
     loss:1449.160828
     acc:0.999024
 encoder:
     loss:0.168198
----------------------------


Epoch: [617/1000]:
train:
----------------------------
 tu:
     loss:18286.776833
     acc:0.854450
 tv:
     loss:19824.655660
     acc:0.537614
 lu:
     loss:14038.962993
     acc:0.922707
 lv:
     loss:17066.623478
     acc:0.297691
 le:
     loss:1517.847408
     acc:0.999656
 encoder:
     loss:0.363172
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.746875
     acc:0.852088
 tv:
     loss:19106.594141
     acc:0.489581
 lu:
     loss:13417.029590
     acc:0.913306
 lv:
     loss:16751.587598
     acc:0.188175
 le:
     loss:1449.437006
     acc:0.999000
 encoder:
     loss:0.343988
----------------------------


Epoch: [618/1000]:
train:
----------------------------
 tu:
     loss:18285.816293
     acc:0.854613
 tv:
     loss:19827.188726
     acc:0.536803
 lu:
     loss:14040.211460
     acc:0.922464
 lv:
     loss:17069.022200
     acc:0.297070
 le:
     loss:1517.935113
     acc:0.999620
 encoder:
     loss:0.214985
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.485742
     acc:0.852351
 tv:
     loss:19106.259277
     acc:0.489423
 lu:
     loss:13416.321387
     acc:0.913427
 lv:
     loss:16750.736523
     acc:0.189074
 le:
     loss:1449.497192
     acc:0.998938
 encoder:
     loss:0.140594
----------------------------


Epoch: [619/1000]:
train:
----------------------------
 tu:
     loss:18286.741915
     acc:0.854228
 tv:
     loss:19823.891057
     acc:0.537930
 lu:
     loss:14038.283169
     acc:0.922766
 lv:
     loss:17068.896700
     acc:0.297404
 le:
     loss:1517.913569
     acc:0.999637
 encoder:
     loss:0.148027
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.493457
     acc:0.852208
 tv:
     loss:19111.549023
     acc:0.488406
 lu:
     loss:13415.861670
     acc:0.913383
 lv:
     loss:16746.240039
     acc:0.189816
 le:
     loss:1448.897229
     acc:0.999062
 encoder:
     loss:0.133272
----------------------------


Epoch: [620/1000]:
train:
----------------------------
 tu:
     loss:18286.760651
     acc:0.854386
 tv:
     loss:19824.231741
     acc:0.537616
 lu:
     loss:14038.837959
     acc:0.922402
 lv:
     loss:17066.672795
     acc:0.297197
 le:
     loss:1517.875674
     acc:0.999642
 encoder:
     loss:0.151349
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.896582
     acc:0.852374
 tv:
     loss:19107.383398
     acc:0.489058
 lu:
     loss:13413.714258
     acc:0.914047
 lv:
     loss:16747.013281
     acc:0.189538
 le:
     loss:1448.731793
     acc:0.999103
 encoder:
     loss:0.141122
----------------------------


Epoch: [621/1000]:
train:
----------------------------
 tu:
     loss:18287.411962
     acc:0.854246
 tv:
     loss:19822.238122
     acc:0.538050
 lu:
     loss:14036.408794
     acc:0.923203
 lv:
     loss:17064.739610
     acc:0.297944
 le:
     loss:1517.911085
     acc:0.999638
 encoder:
     loss:0.182675
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.461328
     acc:0.852476
 tv:
     loss:19101.986230
     acc:0.490778
 lu:
     loss:13416.248291
     acc:0.913293
 lv:
     loss:16747.948926
     acc:0.189250
 le:
     loss:1448.751678
     acc:0.999103
 encoder:
     loss:0.165748
----------------------------


Epoch: [622/1000]:
train:
----------------------------
 tu:
     loss:18286.839651
     acc:0.854354
 tv:
     loss:19823.228152
     acc:0.537814
 lu:
     loss:14039.416845
     acc:0.922535
 lv:
     loss:17064.512650
     acc:0.297934
 le:
     loss:1517.944088
     acc:0.999626
 encoder:
     loss:0.218960
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.821680
     acc:0.852356
 tv:
     loss:19105.464648
     acc:0.490096
 lu:
     loss:13416.433594
     acc:0.913268
 lv:
     loss:16744.332910
     acc:0.190245
 le:
     loss:1448.935767
     acc:0.999040
 encoder:
     loss:0.199973
----------------------------


Epoch: [623/1000]:
train:
----------------------------
 tu:
     loss:18286.361044
     acc:0.854459
 tv:
     loss:19823.460052
     acc:0.537763
 lu:
     loss:14037.805868
     acc:0.922905
 lv:
     loss:17064.263467
     acc:0.297998
 le:
     loss:1517.919490
     acc:0.999632
 encoder:
     loss:0.233412
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.327344
     acc:0.852145
 tv:
     loss:19103.519141
     acc:0.490403
 lu:
     loss:13415.678125
     acc:0.913668
 lv:
     loss:16745.595410
     acc:0.190137
 le:
     loss:1449.000720
     acc:0.999062
 encoder:
     loss:0.175731
----------------------------


Epoch: [624/1000]:
train:
----------------------------
 tu:
     loss:18287.051429
     acc:0.854298
 tv:
     loss:19825.006438
     acc:0.537314
 lu:
     loss:14039.407102
     acc:0.922583
 lv:
     loss:17067.976392
     acc:0.297265
 le:
     loss:1517.988787
     acc:0.999617
 encoder:
     loss:0.182661
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.887500
     acc:0.852393
 tv:
     loss:19104.012207
     acc:0.490322
 lu:
     loss:13414.740771
     acc:0.913766
 lv:
     loss:16744.276953
     acc:0.190174
 le:
     loss:1448.541571
     acc:0.999148
 encoder:
     loss:0.164772
----------------------------


Epoch: [625/1000]:
train:
----------------------------
 tu:
     loss:18286.900141
     acc:0.854318
 tv:
     loss:19823.222543
     acc:0.537734
 lu:
     loss:14039.568450
     acc:0.922471
 lv:
     loss:17063.538915
     acc:0.298257
 le:
     loss:1517.945558
     acc:0.999623
 encoder:
     loss:0.171877
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.122070
     acc:0.852604
 tv:
     loss:19102.723828
     acc:0.490204
 lu:
     loss:13416.528613
     acc:0.913293
 lv:
     loss:16748.272949
     acc:0.189766
 le:
     loss:1448.902887
     acc:0.999081
 encoder:
     loss:0.160391
----------------------------


Epoch: [626/1000]:
train:
----------------------------
 tu:
     loss:18287.079522
     acc:0.854351
 tv:
     loss:19823.034384
     acc:0.537872
 lu:
     loss:14039.001601
     acc:0.922626
 lv:
     loss:17066.309786
     acc:0.297454
 le:
     loss:1517.928426
     acc:0.999629
 encoder:
     loss:0.216423
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.542285
     acc:0.852411
 tv:
     loss:19103.805176
     acc:0.490271
 lu:
     loss:13414.834326
     acc:0.913538
 lv:
     loss:16746.354297
     acc:0.190008
 le:
     loss:1448.804468
     acc:0.999081
 encoder:
     loss:0.183576
----------------------------


Epoch: [627/1000]:
train:
----------------------------
 tu:
     loss:18286.924884
     acc:0.854205
 tv:
     loss:19822.771575
     acc:0.537901
 lu:
     loss:14037.412257
     acc:0.923023
 lv:
     loss:17065.010277
     acc:0.297690
 le:
     loss:1517.937003
     acc:0.999629
 encoder:
     loss:0.410364
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.094824
     acc:0.852234
 tv:
     loss:19103.641992
     acc:0.490117
 lu:
     loss:13415.003906
     acc:0.913437
 lv:
     loss:16745.204102
     acc:0.190378
 le:
     loss:1448.539618
     acc:0.999128
 encoder:
     loss:0.877091
----------------------------


Epoch: [628/1000]:
train:
----------------------------
 tu:
     loss:18286.001431
     acc:0.854566
 tv:
     loss:19822.941213
     acc:0.538031
 lu:
     loss:14038.547034
     acc:0.922823
 lv:
     loss:17064.645883
     acc:0.297761
 le:
     loss:1517.841719
     acc:0.999648
 encoder:
     loss:0.536705
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.290723
     acc:0.852268
 tv:
     loss:19106.027148
     acc:0.489713
 lu:
     loss:13416.704346
     acc:0.913460
 lv:
     loss:16749.208789
     acc:0.189040
 le:
     loss:1448.575812
     acc:0.999127
 encoder:
     loss:0.300588
----------------------------


Epoch: [629/1000]:
train:
----------------------------
 tu:
     loss:18285.956327
     acc:0.854539
 tv:
     loss:19820.869334
     acc:0.538242
 lu:
     loss:14038.262661
     acc:0.922801
 lv:
     loss:17064.742846
     acc:0.298015
 le:
     loss:1517.911014
     acc:0.999636
 encoder:
     loss:0.236119
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.977734
     acc:0.852369
 tv:
     loss:19106.235059
     acc:0.489691
 lu:
     loss:13415.162158
     acc:0.913622
 lv:
     loss:16748.957910
     acc:0.188759
 le:
     loss:1448.651660
     acc:0.999126
 encoder:
     loss:0.173147
----------------------------


Epoch: [630/1000]:
train:
----------------------------
 tu:
     loss:18286.770190
     acc:0.854315
 tv:
     loss:19823.937398
     acc:0.537520
 lu:
     loss:14038.035747
     acc:0.922811
 lv:
     loss:17064.090832
     acc:0.297820
 le:
     loss:1517.958441
     acc:0.999612
 encoder:
     loss:0.170168
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.371387
     acc:0.852453
 tv:
     loss:19106.241699
     acc:0.489966
 lu:
     loss:13416.751074
     acc:0.913498
 lv:
     loss:16742.631445
     acc:0.190696
 le:
     loss:1449.163416
     acc:0.999001
 encoder:
     loss:0.163273
----------------------------


Epoch: [631/1000]:
train:
----------------------------
 tu:
     loss:18286.131007
     acc:0.854477
 tv:
     loss:19824.437216
     acc:0.537529
 lu:
     loss:14038.658249
     acc:0.922721
 lv:
     loss:17065.765296
     acc:0.297603
 le:
     loss:1518.006492
     acc:0.999617
 encoder:
     loss:0.189020
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.912207
     acc:0.852373
 tv:
     loss:19103.638184
     acc:0.490209
 lu:
     loss:13416.627832
     acc:0.913432
 lv:
     loss:16751.541113
     acc:0.188970
 le:
     loss:1448.782629
     acc:0.999087
 encoder:
     loss:0.278379
----------------------------


Epoch: [632/1000]:
train:
----------------------------
 tu:
     loss:18286.628077
     acc:0.854430
 tv:
     loss:19824.305687
     acc:0.537592
 lu:
     loss:14038.391965
     acc:0.922762
 lv:
     loss:17062.623649
     acc:0.298557
 le:
     loss:1517.957486
     acc:0.999621
 encoder:
     loss:0.287296
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.062988
     acc:0.852354
 tv:
     loss:19101.209277
     acc:0.490809
 lu:
     loss:13415.919385
     acc:0.913442
 lv:
     loss:16746.618555
     acc:0.190505
 le:
     loss:1448.922375
     acc:0.999065
 encoder:
     loss:0.624340
----------------------------


Epoch: [633/1000]:
train:
----------------------------
 tu:
     loss:18286.423874
     acc:0.854443
 tv:
     loss:19822.759300
     acc:0.537939
 lu:
     loss:14040.210324
     acc:0.922390
 lv:
     loss:17061.867165
     acc:0.298270
 le:
     loss:1517.900267
     acc:0.999643
 encoder:
     loss:0.575280
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.812207
     acc:0.852417
 tv:
     loss:19101.697559
     acc:0.490669
 lu:
     loss:13416.851660
     acc:0.913141
 lv:
     loss:16742.843457
     acc:0.190448
 le:
     loss:1448.566370
     acc:0.999125
 encoder:
     loss:0.448766
----------------------------


Epoch: [634/1000]:
train:
----------------------------
 tu:
     loss:18286.134050
     acc:0.854490
 tv:
     loss:19821.121855
     acc:0.538234
 lu:
     loss:14038.244527
     acc:0.922903
 lv:
     loss:17059.336403
     acc:0.299249
 le:
     loss:1517.827571
     acc:0.999657
 encoder:
     loss:0.365459
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.463281
     acc:0.852598
 tv:
     loss:19106.897070
     acc:0.489460
 lu:
     loss:13414.200195
     acc:0.913719
 lv:
     loss:16745.170801
     acc:0.189969
 le:
     loss:1448.709357
     acc:0.999124
 encoder:
     loss:0.287197
----------------------------


Epoch: [635/1000]:
train:
----------------------------
 tu:
     loss:18286.198651
     acc:0.854482
 tv:
     loss:19822.429392
     acc:0.538048
 lu:
     loss:14038.101506
     acc:0.922854
 lv:
     loss:17062.212857
     acc:0.298651
 le:
     loss:1517.920311
     acc:0.999629
 encoder:
     loss:0.220271
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.350195
     acc:0.852330
 tv:
     loss:19099.720313
     acc:0.491051
 lu:
     loss:13414.568457
     acc:0.913662
 lv:
     loss:16747.589062
     acc:0.189169
 le:
     loss:1448.508398
     acc:0.999125
 encoder:
     loss:0.192638
----------------------------


Epoch: [636/1000]:
train:
----------------------------
 tu:
     loss:18286.835188
     acc:0.854346
 tv:
     loss:19823.138910
     acc:0.537781
 lu:
     loss:14039.063011
     acc:0.922569
 lv:
     loss:17062.916674
     acc:0.298282
 le:
     loss:1518.094421
     acc:0.999594
 encoder:
     loss:0.183604
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.707227
     acc:0.852206
 tv:
     loss:19101.294434
     acc:0.490662
 lu:
     loss:13414.612500
     acc:0.913818
 lv:
     loss:16750.393164
     acc:0.188799
 le:
     loss:1448.736572
     acc:0.999084
 encoder:
     loss:0.147856
----------------------------


Epoch: [637/1000]:
train:
----------------------------
 tu:
     loss:18287.155046
     acc:0.854212
 tv:
     loss:19823.804347
     acc:0.537562
 lu:
     loss:14038.423783
     acc:0.922786
 lv:
     loss:17061.840752
     acc:0.298287
 le:
     loss:1517.965106
     acc:0.999622
 encoder:
     loss:0.169049
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.785547
     acc:0.852349
 tv:
     loss:19105.193066
     acc:0.489959
 lu:
     loss:13415.193604
     acc:0.913807
 lv:
     loss:16745.923145
     acc:0.190792
 le:
     loss:1448.580157
     acc:0.999126
 encoder:
     loss:0.175892
----------------------------


Epoch: [638/1000]:
train:
----------------------------
 tu:
     loss:18286.653173
     acc:0.854316
 tv:
     loss:19823.043593
     acc:0.537915
 lu:
     loss:14038.668934
     acc:0.922635
 lv:
     loss:17058.525254
     acc:0.299244
 le:
     loss:1517.887511
     acc:0.999641
 encoder:
     loss:0.185220
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.832422
     acc:0.852374
 tv:
     loss:19105.607422
     acc:0.489937
 lu:
     loss:13416.097656
     acc:0.913525
 lv:
     loss:16748.539844
     acc:0.189333
 le:
     loss:1448.904224
     acc:0.999022
 encoder:
     loss:0.215410
----------------------------


Epoch: [639/1000]:
train:
----------------------------
 tu:
     loss:18286.540800
     acc:0.854425
 tv:
     loss:19822.512332
     acc:0.538053
 lu:
     loss:14037.926054
     acc:0.922846
 lv:
     loss:17059.540357
     acc:0.299277
 le:
     loss:1517.907268
     acc:0.999636
 encoder:
     loss:0.199896
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.467090
     acc:0.852345
 tv:
     loss:19103.123340
     acc:0.490523
 lu:
     loss:13415.750586
     acc:0.913483
 lv:
     loss:16747.913086
     acc:0.190114
 le:
     loss:1449.280249
     acc:0.998979
 encoder:
     loss:0.184663
----------------------------


Epoch: [640/1000]:
train:
----------------------------
 tu:
     loss:18287.274369
     acc:0.854303
 tv:
     loss:19820.325207
     acc:0.538369
 lu:
     loss:14037.290300
     acc:0.923018
 lv:
     loss:17059.887445
     acc:0.298736
 le:
     loss:1517.931998
     acc:0.999621
 encoder:
     loss:0.186816
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.447070
     acc:0.852301
 tv:
     loss:19103.377734
     acc:0.490463
 lu:
     loss:13413.174268
     acc:0.914041
 lv:
     loss:16747.247168
     acc:0.190428
 le:
     loss:1449.005438
     acc:0.999004
 encoder:
     loss:0.274386
----------------------------


Epoch: [641/1000]:
train:
----------------------------
 tu:
     loss:18286.902571
     acc:0.854304
 tv:
     loss:19822.768191
     acc:0.537843
 lu:
     loss:14037.106502
     acc:0.923027
 lv:
     loss:17061.884913
     acc:0.298418
 le:
     loss:1517.953390
     acc:0.999623
 encoder:
     loss:0.236391
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.087109
     acc:0.852224
 tv:
     loss:19103.769434
     acc:0.490412
 lu:
     loss:13416.301367
     acc:0.913522
 lv:
     loss:16758.453125
     acc:0.187717
 le:
     loss:1448.514215
     acc:0.999126
 encoder:
     loss:0.163060
----------------------------


Epoch: [642/1000]:
train:
----------------------------
 tu:
     loss:18286.514898
     acc:0.854330
 tv:
     loss:19824.668343
     acc:0.537343
 lu:
     loss:14037.436262
     acc:0.923027
 lv:
     loss:17059.131336
     acc:0.298856
 le:
     loss:1517.872238
     acc:0.999648
 encoder:
     loss:0.165364
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.150977
     acc:0.852430
 tv:
     loss:19102.408984
     acc:0.490428
 lu:
     loss:13416.270898
     acc:0.913409
 lv:
     loss:16747.639746
     acc:0.189418
 le:
     loss:1449.242114
     acc:0.998979
 encoder:
     loss:0.150007
----------------------------


Epoch: [643/1000]:
train:
----------------------------
 tu:
     loss:18286.913086
     acc:0.854260
 tv:
     loss:19821.410236
     acc:0.537996
 lu:
     loss:14037.352198
     acc:0.922994
 lv:
     loss:17059.077466
     acc:0.299125
 le:
     loss:1517.941606
     acc:0.999630
 encoder:
     loss:0.168204
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.014160
     acc:0.852294
 tv:
     loss:19103.043359
     acc:0.490568
 lu:
     loss:13416.383057
     acc:0.913278
 lv:
     loss:16751.293066
     acc:0.188570
 le:
     loss:1449.160669
     acc:0.999003
 encoder:
     loss:0.164895
----------------------------


Epoch: [644/1000]:
train:
----------------------------
 tu:
     loss:18285.970487
     acc:0.854388
 tv:
     loss:19823.530012
     acc:0.537618
 lu:
     loss:14037.339469
     acc:0.923019
 lv:
     loss:17058.553813
     acc:0.299259
 le:
     loss:1517.875392
     acc:0.999638
 encoder:
     loss:0.203563
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.654883
     acc:0.852230
 tv:
     loss:19099.408105
     acc:0.491294
 lu:
     loss:13415.680176
     acc:0.913332
 lv:
     loss:16746.899609
     acc:0.189880
 le:
     loss:1449.213770
     acc:0.999002
 encoder:
     loss:0.233055
----------------------------


Epoch: [645/1000]:
train:
----------------------------
 tu:
     loss:18286.362078
     acc:0.854404
 tv:
     loss:19821.243539
     acc:0.538244
 lu:
     loss:14037.749739
     acc:0.923064
 lv:
     loss:17059.252578
     acc:0.299279
 le:
     loss:1517.865217
     acc:0.999646
 encoder:
     loss:0.199177
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.092480
     acc:0.852372
 tv:
     loss:19102.550488
     acc:0.490736
 lu:
     loss:13414.549268
     acc:0.913700
 lv:
     loss:16747.789453
     acc:0.190042
 le:
     loss:1448.892792
     acc:0.999065
 encoder:
     loss:0.158910
----------------------------


Epoch: [646/1000]:
train:
----------------------------
 tu:
     loss:18286.036712
     acc:0.854465
 tv:
     loss:19821.038665
     acc:0.538375
 lu:
     loss:14037.460540
     acc:0.922939
 lv:
     loss:17058.850688
     acc:0.299739
 le:
     loss:1517.919839
     acc:0.999629
 encoder:
     loss:0.166553
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.623535
     acc:0.852573
 tv:
     loss:19102.979297
     acc:0.490236
 lu:
     loss:13416.578125
     acc:0.913346
 lv:
     loss:16743.530078
     acc:0.190411
 le:
     loss:1448.983313
     acc:0.999025
 encoder:
     loss:0.145952
----------------------------


Epoch: [647/1000]:
train:
----------------------------
 tu:
     loss:18286.026662
     acc:0.854384
 tv:
     loss:19821.268078
     acc:0.538166
 lu:
     loss:14037.006404
     acc:0.923095
 lv:
     loss:17058.180051
     acc:0.299299
 le:
     loss:1517.966073
     acc:0.999631
 encoder:
     loss:0.161123
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.967578
     acc:0.852426
 tv:
     loss:19105.054883
     acc:0.490012
 lu:
     loss:13418.056689
     acc:0.912858
 lv:
     loss:16744.648633
     acc:0.190076
 le:
     loss:1448.790179
     acc:0.999085
 encoder:
     loss:0.175821
----------------------------


Epoch: [648/1000]:
train:
----------------------------
 tu:
     loss:18286.306890
     acc:0.854346
 tv:
     loss:19819.661280
     acc:0.538780
 lu:
     loss:14036.756495
     acc:0.923016
 lv:
     loss:17057.786621
     acc:0.299361
 le:
     loss:1517.837140
     acc:0.999650
 encoder:
     loss:0.207423
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.172266
     acc:0.852500
 tv:
     loss:19102.229883
     acc:0.490368
 lu:
     loss:13416.944727
     acc:0.913205
 lv:
     loss:16748.030859
     acc:0.190106
 le:
     loss:1448.676514
     acc:0.999106
 encoder:
     loss:0.177501
----------------------------


Epoch: [649/1000]:
train:
----------------------------
 tu:
     loss:18286.537359
     acc:0.854364
 tv:
     loss:19822.128497
     acc:0.538132
 lu:
     loss:14037.917480
     acc:0.922892
 lv:
     loss:17056.966025
     acc:0.299708
 le:
     loss:1517.914595
     acc:0.999638
 encoder:
     loss:0.295467
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.032910
     acc:0.852337
 tv:
     loss:19099.134473
     acc:0.491422
 lu:
     loss:13416.522656
     acc:0.913185
 lv:
     loss:16743.140625
     acc:0.190287
 le:
     loss:1448.847571
     acc:0.999064
 encoder:
     loss:0.332535
----------------------------


Epoch: [650/1000]:
train:
----------------------------
 tu:
     loss:18286.432140
     acc:0.854404
 tv:
     loss:19820.562909
     acc:0.538283
 lu:
     loss:14037.672727
     acc:0.922910
 lv:
     loss:17055.578852
     acc:0.299544
 le:
     loss:1517.931704
     acc:0.999633
 encoder:
     loss:0.229343
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.914355
     acc:0.852491
 tv:
     loss:19102.004199
     acc:0.490593
 lu:
     loss:13416.780713
     acc:0.913299
 lv:
     loss:16751.959277
     acc:0.188945
 le:
     loss:1448.776190
     acc:0.999107
 encoder:
     loss:0.188785
----------------------------


Epoch: [651/1000]:
train:
----------------------------
 tu:
     loss:18286.688136
     acc:0.854304
 tv:
     loss:19821.395281
     acc:0.538308
 lu:
     loss:14037.816815
     acc:0.922810
 lv:
     loss:17058.335483
     acc:0.299394
 le:
     loss:1517.863498
     acc:0.999650
 encoder:
     loss:0.171823
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.484570
     acc:0.852246
 tv:
     loss:19106.552734
     acc:0.489617
 lu:
     loss:13415.291943
     acc:0.913494
 lv:
     loss:16748.966406
     acc:0.189301
 le:
     loss:1448.954437
     acc:0.999082
 encoder:
     loss:0.190001
----------------------------


Epoch: [652/1000]:
train:
----------------------------
 tu:
     loss:18285.424305
     acc:0.854650
 tv:
     loss:19820.552496
     acc:0.538394
 lu:
     loss:14037.755575
     acc:0.922783
 lv:
     loss:17057.413938
     acc:0.299111
 le:
     loss:1517.850684
     acc:0.999644
 encoder:
     loss:0.257084
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.641602
     acc:0.852529
 tv:
     loss:19105.393555
     acc:0.489916
 lu:
     loss:13416.281641
     acc:0.913475
 lv:
     loss:16751.641016
     acc:0.188598
 le:
     loss:1448.985626
     acc:0.999042
 encoder:
     loss:0.282851
----------------------------


Epoch: [653/1000]:
train:
----------------------------
 tu:
     loss:18285.606354
     acc:0.854543
 tv:
     loss:19821.879406
     acc:0.538043
 lu:
     loss:14038.577659
     acc:0.922633
 lv:
     loss:17057.611056
     acc:0.299267
 le:
     loss:1517.796250
     acc:0.999665
 encoder:
     loss:0.213851
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.602832
     acc:0.852394
 tv:
     loss:19102.838086
     acc:0.490229
 lu:
     loss:13415.275586
     acc:0.913293
 lv:
     loss:16747.894629
     acc:0.189983
 le:
     loss:1448.948810
     acc:0.999065
 encoder:
     loss:0.185099
----------------------------


Epoch: [654/1000]:
train:
----------------------------
 tu:
     loss:18287.184707
     acc:0.854277
 tv:
     loss:19821.375886
     acc:0.538134
 lu:
     loss:14037.641113
     acc:0.922802
 lv:
     loss:17060.182106
     acc:0.298769
 le:
     loss:1517.978479
     acc:0.999614
 encoder:
     loss:0.238235
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.380273
     acc:0.852375
 tv:
     loss:19106.540430
     acc:0.489805
 lu:
     loss:13416.538037
     acc:0.913175
 lv:
     loss:16758.808105
     acc:0.186890
 le:
     loss:1449.039752
     acc:0.999022
 encoder:
     loss:0.277660
----------------------------


Epoch: [655/1000]:
train:
----------------------------
 tu:
     loss:18286.789823
     acc:0.854318
 tv:
     loss:19820.067508
     acc:0.538327
 lu:
     loss:14036.991279
     acc:0.923014
 lv:
     loss:17056.879860
     acc:0.299560
 le:
     loss:1517.890974
     acc:0.999632
 encoder:
     loss:0.299793
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.424023
     acc:0.852496
 tv:
     loss:19106.962207
     acc:0.489644
 lu:
     loss:13416.321875
     acc:0.913378
 lv:
     loss:16754.317871
     acc:0.188244
 le:
     loss:1449.325305
     acc:0.998960
 encoder:
     loss:0.330065
----------------------------


Epoch: [656/1000]:
train:
----------------------------
 tu:
     loss:18286.222361
     acc:0.854508
 tv:
     loss:19821.114076
     acc:0.538168
 lu:
     loss:14038.854356
     acc:0.922630
 lv:
     loss:17059.989235
     acc:0.298747
 le:
     loss:1517.991857
     acc:0.999622
 encoder:
     loss:0.285303
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.668750
     acc:0.852435
 tv:
     loss:19106.433789
     acc:0.489524
 lu:
     loss:13416.465723
     acc:0.913374
 lv:
     loss:16750.720508
     acc:0.189496
 le:
     loss:1448.668323
     acc:0.999128
 encoder:
     loss:0.237045
----------------------------


Epoch: [657/1000]:
train:
----------------------------
 tu:
     loss:18286.833598
     acc:0.854356
 tv:
     loss:19820.980310
     acc:0.538189
 lu:
     loss:14036.872536
     acc:0.922612
 lv:
     loss:17059.637343
     acc:0.298683
 le:
     loss:1517.999753
     acc:0.999612
 encoder:
     loss:0.268574
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.285254
     acc:0.852625
 tv:
     loss:19106.469238
     acc:0.489474
 lu:
     loss:13417.106396
     acc:0.913245
 lv:
     loss:16752.954395
     acc:0.188458
 le:
     loss:1448.863269
     acc:0.999066
 encoder:
     loss:0.271389
----------------------------


Epoch: [658/1000]:
train:
----------------------------
 tu:
     loss:18285.949185
     acc:0.854425
 tv:
     loss:19819.463651
     acc:0.538672
 lu:
     loss:14037.225143
     acc:0.923029
 lv:
     loss:17055.494765
     acc:0.299866
 le:
     loss:1517.840218
     acc:0.999650
 encoder:
     loss:0.248870
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.552637
     acc:0.852470
 tv:
     loss:19101.878516
     acc:0.490858
 lu:
     loss:13414.959326
     acc:0.913583
 lv:
     loss:16750.158691
     acc:0.189100
 le:
     loss:1449.081079
     acc:0.999018
 encoder:
     loss:0.249910
----------------------------


Epoch: [659/1000]:
train:
----------------------------
 tu:
     loss:18285.802098
     acc:0.854469
 tv:
     loss:19818.761412
     acc:0.538566
 lu:
     loss:14036.712516
     acc:0.923138
 lv:
     loss:17053.102210
     acc:0.300297
 le:
     loss:1517.968888
     acc:0.999621
 encoder:
     loss:0.194580
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.893555
     acc:0.852335
 tv:
     loss:19105.809082
     acc:0.489785
 lu:
     loss:13415.237744
     acc:0.913614
 lv:
     loss:16757.765527
     acc:0.187400
 le:
     loss:1448.781000
     acc:0.999103
 encoder:
     loss:0.165920
----------------------------


Epoch: [660/1000]:
train:
----------------------------
 tu:
     loss:18286.026696
     acc:0.854438
 tv:
     loss:19817.799782
     acc:0.539032
 lu:
     loss:14037.426235
     acc:0.923084
 lv:
     loss:17052.434843
     acc:0.300682
 le:
     loss:1517.861198
     acc:0.999650
 encoder:
     loss:0.227782
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.962598
     acc:0.852374
 tv:
     loss:19102.759277
     acc:0.490848
 lu:
     loss:13416.759326
     acc:0.913031
 lv:
     loss:16746.524414
     acc:0.190020
 le:
     loss:1448.956348
     acc:0.999043
 encoder:
     loss:0.238107
----------------------------


Epoch: [661/1000]:
train:
----------------------------
 tu:
     loss:18286.199571
     acc:0.854500
 tv:
     loss:19821.753475
     acc:0.537999
 lu:
     loss:14036.900992
     acc:0.923095
 lv:
     loss:17055.602380
     acc:0.299655
 le:
     loss:1517.822044
     acc:0.999654
 encoder:
     loss:0.184666
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.179297
     acc:0.852620
 tv:
     loss:19103.307129
     acc:0.490463
 lu:
     loss:13413.727148
     acc:0.913838
 lv:
     loss:16746.763965
     acc:0.190020
 le:
     loss:1448.718256
     acc:0.999106
 encoder:
     loss:0.149816
----------------------------


Epoch: [662/1000]:
train:
----------------------------
 tu:
     loss:18286.272552
     acc:0.854406
 tv:
     loss:19819.442564
     acc:0.538699
 lu:
     loss:14038.303995
     acc:0.922777
 lv:
     loss:17052.759732
     acc:0.300438
 le:
     loss:1517.886351
     acc:0.999645
 encoder:
     loss:0.185046
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.791797
     acc:0.852450
 tv:
     loss:19107.248633
     acc:0.489333
 lu:
     loss:13416.483691
     acc:0.913135
 lv:
     loss:16741.566602
     acc:0.191380
 le:
     loss:1449.119543
     acc:0.999023
 encoder:
     loss:0.182313
----------------------------


Epoch: [663/1000]:
train:
----------------------------
 tu:
     loss:18286.424271
     acc:0.854351
 tv:
     loss:19819.174282
     acc:0.538699
 lu:
     loss:14036.879349
     acc:0.923034
 lv:
     loss:17052.904921
     acc:0.300588
 le:
     loss:1517.969516
     acc:0.999625
 encoder:
     loss:0.184402
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.164453
     acc:0.852293
 tv:
     loss:19104.253223
     acc:0.490554
 lu:
     loss:13416.066797
     acc:0.913543
 lv:
     loss:16747.518945
     acc:0.190006
 le:
     loss:1449.044525
     acc:0.999044
 encoder:
     loss:0.183612
----------------------------


Epoch: [664/1000]:
train:
----------------------------
 tu:
     loss:18286.080555
     acc:0.854456
 tv:
     loss:19819.872241
     acc:0.538440
 lu:
     loss:14036.483489
     acc:0.923209
 lv:
     loss:17049.041163
     acc:0.300879
 le:
     loss:1517.883441
     acc:0.999646
 encoder:
     loss:0.232676
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.751563
     acc:0.852411
 tv:
     loss:19101.662402
     acc:0.490830
 lu:
     loss:13414.816357
     acc:0.913724
 lv:
     loss:16748.937109
     acc:0.189415
 le:
     loss:1449.003082
     acc:0.999044
 encoder:
     loss:0.275996
----------------------------


Epoch: [665/1000]:
train:
----------------------------
 tu:
     loss:18286.023415
     acc:0.854453
 tv:
     loss:19819.373751
     acc:0.538468
 lu:
     loss:14037.593693
     acc:0.922866
 lv:
     loss:17051.572379
     acc:0.300419
 le:
     loss:1517.866809
     acc:0.999645
 encoder:
     loss:0.454023
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.338281
     acc:0.852372
 tv:
     loss:19105.529687
     acc:0.489701
 lu:
     loss:13416.449707
     acc:0.913437
 lv:
     loss:16755.832813
     acc:0.187618
 le:
     loss:1449.157916
     acc:0.999000
 encoder:
     loss:0.464446
----------------------------


Epoch: [666/1000]:
train:
----------------------------
 tu:
     loss:18285.764501
     acc:0.854466
 tv:
     loss:19820.768294
     acc:0.538281
 lu:
     loss:14037.446391
     acc:0.922988
 lv:
     loss:17056.936523
     acc:0.299381
 le:
     loss:1517.899802
     acc:0.999634
 encoder:
     loss:0.349402
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.717773
     acc:0.852496
 tv:
     loss:19105.087207
     acc:0.489733
 lu:
     loss:13414.088379
     acc:0.913979
 lv:
     loss:16751.747070
     acc:0.188552
 le:
     loss:1448.628827
     acc:0.999106
 encoder:
     loss:0.263062
----------------------------


Epoch: [667/1000]:
train:
----------------------------
 tu:
     loss:18286.276186
     acc:0.854302
 tv:
     loss:19821.274641
     acc:0.538094
 lu:
     loss:14038.226040
     acc:0.922597
 lv:
     loss:17054.169786
     acc:0.300126
 le:
     loss:1518.008331
     acc:0.999619
 encoder:
     loss:0.271288
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.602637
     acc:0.852434
 tv:
     loss:19101.274414
     acc:0.490758
 lu:
     loss:13412.989600
     acc:0.914017
 lv:
     loss:16747.679883
     acc:0.189341
 le:
     loss:1448.592798
     acc:0.999127
 encoder:
     loss:0.232522
----------------------------


Epoch: [668/1000]:
train:
----------------------------
 tu:
     loss:18286.171057
     acc:0.854364
 tv:
     loss:19819.781352
     acc:0.538446
 lu:
     loss:14037.553348
     acc:0.922939
 lv:
     loss:17050.949707
     acc:0.300686
 le:
     loss:1517.859995
     acc:0.999650
 encoder:
     loss:0.396552
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.213086
     acc:0.852611
 tv:
     loss:19104.497852
     acc:0.490015
 lu:
     loss:13414.144238
     acc:0.913781
 lv:
     loss:16745.660645
     acc:0.189979
 le:
     loss:1449.173010
     acc:0.998981
 encoder:
     loss:0.524465
----------------------------


Epoch: [669/1000]:
train:
----------------------------
 tu:
     loss:18285.925656
     acc:0.854373
 tv:
     loss:19818.570994
     acc:0.538767
 lu:
     loss:14036.997865
     acc:0.923030
 lv:
     loss:17050.776844
     acc:0.300979
 le:
     loss:1517.878388
     acc:0.999639
 encoder:
     loss:0.369717
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.877051
     acc:0.852427
 tv:
     loss:19101.049707
     acc:0.490591
 lu:
     loss:13416.375732
     acc:0.913257
 lv:
     loss:16753.568359
     acc:0.188573
 le:
     loss:1448.955811
     acc:0.999045
 encoder:
     loss:0.194124
----------------------------


Epoch: [670/1000]:
train:
----------------------------
 tu:
     loss:18286.188908
     acc:0.854452
 tv:
     loss:19820.228447
     acc:0.538295
 lu:
     loss:14038.523120
     acc:0.922692
 lv:
     loss:17052.521723
     acc:0.300624
 le:
     loss:1517.870776
     acc:0.999645
 encoder:
     loss:0.188686
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.716797
     acc:0.852476
 tv:
     loss:19105.951562
     acc:0.489711
 lu:
     loss:13414.956006
     acc:0.913535
 lv:
     loss:16752.020898
     acc:0.187967
 le:
     loss:1448.591473
     acc:0.999127
 encoder:
     loss:0.161365
----------------------------


Epoch: [671/1000]:
train:
----------------------------
 tu:
     loss:18286.867131
     acc:0.854282
 tv:
     loss:19817.836539
     acc:0.538744
 lu:
     loss:14036.897949
     acc:0.923054
 lv:
     loss:17050.215071
     acc:0.300887
 le:
     loss:1518.009641
     acc:0.999617
 encoder:
     loss:0.182982
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.671387
     acc:0.852392
 tv:
     loss:19103.997070
     acc:0.490392
 lu:
     loss:13417.209131
     acc:0.913214
 lv:
     loss:16754.281641
     acc:0.188770
 le:
     loss:1448.655286
     acc:0.999127
 encoder:
     loss:0.180320
----------------------------


Epoch: [672/1000]:
train:
----------------------------
 tu:
     loss:18285.358410
     acc:0.854498
 tv:
     loss:19818.598360
     acc:0.538809
 lu:
     loss:14036.746627
     acc:0.922977
 lv:
     loss:17053.491722
     acc:0.300173
 le:
     loss:1517.871143
     acc:0.999643
 encoder:
     loss:0.203499
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.760840
     acc:0.852716
 tv:
     loss:19103.974316
     acc:0.490052
 lu:
     loss:13414.947363
     acc:0.913712
 lv:
     loss:16756.725977
     acc:0.187986
 le:
     loss:1448.573615
     acc:0.999127
 encoder:
     loss:0.191469
----------------------------


Epoch: [673/1000]:
train:
----------------------------
 tu:
     loss:18285.320767
     acc:0.854521
 tv:
     loss:19819.574718
     acc:0.538633
 lu:
     loss:14037.809411
     acc:0.922803
 lv:
     loss:17052.787654
     acc:0.300482
 le:
     loss:1517.902009
     acc:0.999647
 encoder:
     loss:0.218030
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.108789
     acc:0.852334
 tv:
     loss:19103.142676
     acc:0.490464
 lu:
     loss:13415.733203
     acc:0.913474
 lv:
     loss:16753.992090
     acc:0.188345
 le:
     loss:1449.030481
     acc:0.999043
 encoder:
     loss:0.235763
----------------------------


Epoch: [674/1000]:
train:
----------------------------
 tu:
     loss:18285.626067
     acc:0.854511
 tv:
     loss:19818.591365
     acc:0.538623
 lu:
     loss:14037.184457
     acc:0.923027
 lv:
     loss:17049.188829
     acc:0.301025
 le:
     loss:1517.863974
     acc:0.999636
 encoder:
     loss:0.549305
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.782813
     acc:0.852392
 tv:
     loss:19105.167676
     acc:0.489848
 lu:
     loss:13414.184961
     acc:0.913923
 lv:
     loss:16748.180566
     acc:0.189625
 le:
     loss:1448.593933
     acc:0.999124
 encoder:
     loss:0.349190
----------------------------


Epoch: [675/1000]:
train:
----------------------------
 tu:
     loss:18286.360851
     acc:0.854367
 tv:
     loss:19818.219261
     acc:0.538830
 lu:
     loss:14037.272654
     acc:0.922909
 lv:
     loss:17052.554506
     acc:0.300616
 le:
     loss:1517.906762
     acc:0.999637
 encoder:
     loss:0.286260
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.754199
     acc:0.852289
 tv:
     loss:19106.867090
     acc:0.489587
 lu:
     loss:13414.388867
     acc:0.913480
 lv:
     loss:16759.591895
     acc:0.187223
 le:
     loss:1449.046301
     acc:0.999060
 encoder:
     loss:0.194641
----------------------------


Epoch: [676/1000]:
train:
----------------------------
 tu:
     loss:18286.113588
     acc:0.854493
 tv:
     loss:19818.894452
     acc:0.538691
 lu:
     loss:14036.828920
     acc:0.923096
 lv:
     loss:17048.294206
     acc:0.301189
 le:
     loss:1517.882675
     acc:0.999638
 encoder:
     loss:0.211861
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.324512
     acc:0.852453
 tv:
     loss:19107.660645
     acc:0.489290
 lu:
     loss:13415.515186
     acc:0.913333
 lv:
     loss:16744.644434
     acc:0.190940
 le:
     loss:1448.654730
     acc:0.999126
 encoder:
     loss:0.342870
----------------------------


Epoch: [677/1000]:
train:
----------------------------
 tu:
     loss:18285.917912
     acc:0.854487
 tv:
     loss:19818.580192
     acc:0.538704
 lu:
     loss:14038.072845
     acc:0.922785
 lv:
     loss:17051.979697
     acc:0.300493
 le:
     loss:1517.892797
     acc:0.999639
 encoder:
     loss:0.459517
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.306055
     acc:0.852469
 tv:
     loss:19113.120312
     acc:0.487872
 lu:
     loss:13413.902246
     acc:0.913896
 lv:
     loss:16754.410547
     acc:0.188081
 le:
     loss:1448.798590
     acc:0.999085
 encoder:
     loss:0.322040
----------------------------


Epoch: [678/1000]:
train:
----------------------------
 tu:
     loss:18286.317610
     acc:0.854312
 tv:
     loss:19818.592762
     acc:0.538733
 lu:
     loss:14036.835563
     acc:0.923045
 lv:
     loss:17050.546171
     acc:0.300994
 le:
     loss:1517.902876
     acc:0.999640
 encoder:
     loss:0.231043
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.941016
     acc:0.852539
 tv:
     loss:19107.277539
     acc:0.489362
 lu:
     loss:13414.542871
     acc:0.913721
 lv:
     loss:16747.788867
     acc:0.189644
 le:
     loss:1448.831317
     acc:0.999084
 encoder:
     loss:0.179792
----------------------------


Epoch: [679/1000]:
train:
----------------------------
 tu:
     loss:18284.749955
     acc:0.854765
 tv:
     loss:19817.403865
     acc:0.538945
 lu:
     loss:14036.876885
     acc:0.923008
 lv:
     loss:17050.206066
     acc:0.300676
 le:
     loss:1517.848475
     acc:0.999644
 encoder:
     loss:0.170473
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.687109
     acc:0.852714
 tv:
     loss:19102.444141
     acc:0.490604
 lu:
     loss:13414.754199
     acc:0.913782
 lv:
     loss:16750.036621
     acc:0.189206
 le:
     loss:1448.727814
     acc:0.999127
 encoder:
     loss:0.152927
----------------------------


Epoch: [680/1000]:
train:
----------------------------
 tu:
     loss:18285.698117
     acc:0.854581
 tv:
     loss:19819.526095
     acc:0.538593
 lu:
     loss:14038.598576
     acc:0.922671
 lv:
     loss:17051.094545
     acc:0.300852
 le:
     loss:1517.850576
     acc:0.999647
 encoder:
     loss:0.174047
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.957910
     acc:0.852601
 tv:
     loss:19103.453906
     acc:0.490291
 lu:
     loss:13413.676416
     acc:0.913962
 lv:
     loss:16754.472461
     acc:0.188499
 le:
     loss:1448.947565
     acc:0.999042
 encoder:
     loss:0.159967
----------------------------


Epoch: [681/1000]:
train:
----------------------------
 tu:
     loss:18286.440952
     acc:0.854346
 tv:
     loss:19818.381155
     acc:0.538657
 lu:
     loss:14036.892941
     acc:0.922959
 lv:
     loss:17052.188942
     acc:0.300330
 le:
     loss:1517.827347
     acc:0.999660
 encoder:
     loss:0.190456
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.484766
     acc:0.852532
 tv:
     loss:19106.770313
     acc:0.489380
 lu:
     loss:13412.425195
     acc:0.914298
 lv:
     loss:16742.294043
     acc:0.192184
 le:
     loss:1448.707452
     acc:0.999107
 encoder:
     loss:0.213180
----------------------------


Epoch: [682/1000]:
train:
----------------------------
 tu:
     loss:18285.397552
     acc:0.854564
 tv:
     loss:19819.197550
     acc:0.538564
 lu:
     loss:14036.734591
     acc:0.923072
 lv:
     loss:17048.504917
     acc:0.300912
 le:
     loss:1517.840769
     acc:0.999651
 encoder:
     loss:0.269123
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.204297
     acc:0.852229
 tv:
     loss:19108.633301
     acc:0.489041
 lu:
     loss:13413.304492
     acc:0.913953
 lv:
     loss:16755.113281
     acc:0.187715
 le:
     loss:1449.366547
     acc:0.998958
 encoder:
     loss:0.217981
----------------------------


Epoch: [683/1000]:
train:
----------------------------
 tu:
     loss:18285.587947
     acc:0.854472
 tv:
     loss:19819.080283
     acc:0.538615
 lu:
     loss:14035.967512
     acc:0.923146
 lv:
     loss:17049.781012
     acc:0.300915
 le:
     loss:1518.027777
     acc:0.999612
 encoder:
     loss:0.226548
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.597656
     acc:0.852514
 tv:
     loss:19107.351758
     acc:0.489075
 lu:
     loss:13413.650537
     acc:0.913837
 lv:
     loss:16748.187012
     acc:0.190345
 le:
     loss:1448.635052
     acc:0.999127
 encoder:
     loss:0.169394
----------------------------


Epoch: [684/1000]:
train:
----------------------------
 tu:
     loss:18285.543809
     acc:0.854593
 tv:
     loss:19818.927428
     acc:0.538417
 lu:
     loss:14037.099110
     acc:0.923053
 lv:
     loss:17048.929903
     acc:0.301273
 le:
     loss:1517.970089
     acc:0.999619
 encoder:
     loss:0.176964
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.508203
     acc:0.852390
 tv:
     loss:19103.377637
     acc:0.490118
 lu:
     loss:13414.468164
     acc:0.913642
 lv:
     loss:16754.180469
     acc:0.188392
 le:
     loss:1449.025525
     acc:0.999043
 encoder:
     loss:0.204161
----------------------------


Epoch: [685/1000]:
train:
----------------------------
 tu:
     loss:18285.539642
     acc:0.854511
 tv:
     loss:19817.636117
     acc:0.538972
 lu:
     loss:14036.411757
     acc:0.923207
 lv:
     loss:17049.982445
     acc:0.300863
 le:
     loss:1517.830111
     acc:0.999653
 encoder:
     loss:0.242659
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.065039
     acc:0.852271
 tv:
     loss:19101.879980
     acc:0.490991
 lu:
     loss:13415.125879
     acc:0.913685
 lv:
     loss:16751.701562
     acc:0.188761
 le:
     loss:1449.180457
     acc:0.999001
 encoder:
     loss:0.237799
----------------------------


Epoch: [686/1000]:
train:
----------------------------
 tu:
     loss:18285.858807
     acc:0.854392
 tv:
     loss:19818.598644
     acc:0.538551
 lu:
     loss:14038.218568
     acc:0.922723
 lv:
     loss:17050.765863
     acc:0.300403
 le:
     loss:1517.932704
     acc:0.999625
 encoder:
     loss:0.199366
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.519336
     acc:0.852478
 tv:
     loss:19104.422168
     acc:0.490005
 lu:
     loss:13415.772754
     acc:0.913497
 lv:
     loss:16751.135449
     acc:0.188430
 le:
     loss:1448.882477
     acc:0.999063
 encoder:
     loss:0.170937
----------------------------


Epoch: [687/1000]:
train:
----------------------------
 tu:
     loss:18285.475075
     acc:0.854562
 tv:
     loss:19817.573015
     acc:0.538813
 lu:
     loss:14036.866688
     acc:0.923059
 lv:
     loss:17051.009709
     acc:0.300655
 le:
     loss:1517.875950
     acc:0.999643
 encoder:
     loss:0.217577
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.385352
     acc:0.852451
 tv:
     loss:19101.780566
     acc:0.490615
 lu:
     loss:13413.419043
     acc:0.914069
 lv:
     loss:16753.014355
     acc:0.188637
 le:
     loss:1448.602271
     acc:0.999103
 encoder:
     loss:0.210439
----------------------------


Epoch: [688/1000]:
train:
----------------------------
 tu:
     loss:18286.479333
     acc:0.854267
 tv:
     loss:19817.707815
     acc:0.538879
 lu:
     loss:14036.412155
     acc:0.923147
 lv:
     loss:17047.003872
     acc:0.301513
 le:
     loss:1517.896152
     acc:0.999638
 encoder:
     loss:0.214772
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.262793
     acc:0.852511
 tv:
     loss:19108.358008
     acc:0.488975
 lu:
     loss:13414.123877
     acc:0.913834
 lv:
     loss:16751.031250
     acc:0.189362
 le:
     loss:1448.890472
     acc:0.999066
 encoder:
     loss:0.182984
----------------------------


Epoch: [689/1000]:
train:
----------------------------
 tu:
     loss:18286.434570
     acc:0.854375
 tv:
     loss:19816.890784
     acc:0.538884
 lu:
     loss:14036.491052
     acc:0.922952
 lv:
     loss:17042.477857
     acc:0.302240
 le:
     loss:1517.941944
     acc:0.999630
 encoder:
     loss:0.196752
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.217383
     acc:0.852539
 tv:
     loss:19105.275977
     acc:0.489574
 lu:
     loss:13414.444043
     acc:0.914191
 lv:
     loss:16752.875293
     acc:0.188859
 le:
     loss:1448.875464
     acc:0.999045
 encoder:
     loss:0.185983
----------------------------


Epoch: [690/1000]:
train:
----------------------------
 tu:
     loss:18285.615723
     acc:0.854552
 tv:
     loss:19816.582542
     acc:0.539158
 lu:
     loss:14037.635265
     acc:0.922902
 lv:
     loss:17046.047999
     acc:0.301789
 le:
     loss:1517.927679
     acc:0.999637
 encoder:
     loss:0.221974
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.811426
     acc:0.852492
 tv:
     loss:19104.234668
     acc:0.489989
 lu:
     loss:13414.346729
     acc:0.913804
 lv:
     loss:16755.724512
     acc:0.188055
 le:
     loss:1448.719324
     acc:0.999128
 encoder:
     loss:0.348798
----------------------------


Epoch: [691/1000]:
train:
----------------------------
 tu:
     loss:18286.448208
     acc:0.854389
 tv:
     loss:19817.863134
     acc:0.538929
 lu:
     loss:14037.368925
     acc:0.922962
 lv:
     loss:17047.720374
     acc:0.301513
 le:
     loss:1518.037730
     acc:0.999604
 encoder:
     loss:0.332664
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.622852
     acc:0.852451
 tv:
     loss:19108.500781
     acc:0.489374
 lu:
     loss:13413.971191
     acc:0.913938
 lv:
     loss:16755.725977
     acc:0.188367
 le:
     loss:1448.914154
     acc:0.999065
 encoder:
     loss:0.248165
----------------------------


Epoch: [692/1000]:
train:
----------------------------
 tu:
     loss:18285.402707
     acc:0.854591
 tv:
     loss:19817.253634
     acc:0.539048
 lu:
     loss:14037.363758
     acc:0.922914
 lv:
     loss:17045.428563
     acc:0.301964
 le:
     loss:1517.913989
     acc:0.999633
 encoder:
     loss:0.229324
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.555371
     acc:0.852456
 tv:
     loss:19109.103223
     acc:0.489060
 lu:
     loss:13415.882520
     acc:0.913446
 lv:
     loss:16755.350781
     acc:0.188157
 le:
     loss:1448.748553
     acc:0.999106
 encoder:
     loss:0.184116
----------------------------


Epoch: [693/1000]:
train:
----------------------------
 tu:
     loss:18285.799033
     acc:0.854558
 tv:
     loss:19816.302507
     acc:0.539207
 lu:
     loss:14036.426690
     acc:0.923115
 lv:
     loss:17044.626783
     acc:0.302247
 le:
     loss:1517.839100
     acc:0.999652
 encoder:
     loss:0.184525
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.737402
     acc:0.852415
 tv:
     loss:19106.787305
     acc:0.489730
 lu:
     loss:13414.722705
     acc:0.913867
 lv:
     loss:16754.636133
     acc:0.188704
 le:
     loss:1449.282019
     acc:0.998981
 encoder:
     loss:0.343813
----------------------------


Epoch: [694/1000]:
train:
----------------------------
 tu:
     loss:18285.786746
     acc:0.854492
 tv:
     loss:19816.253180
     acc:0.539165
 lu:
     loss:14035.962891
     acc:0.923270
 lv:
     loss:17044.373751
     acc:0.301786
 le:
     loss:1517.908522
     acc:0.999631
 encoder:
     loss:0.303346
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.521191
     acc:0.852517
 tv:
     loss:19101.254688
     acc:0.490650
 lu:
     loss:13413.969336
     acc:0.914000
 lv:
     loss:16752.560449
     acc:0.188941
 le:
     loss:1448.479681
     acc:0.999147
 encoder:
     loss:0.171992
----------------------------


Epoch: [695/1000]:
train:
----------------------------
 tu:
     loss:18285.353470
     acc:0.854460
 tv:
     loss:19817.993368
     acc:0.538911
 lu:
     loss:14037.058071
     acc:0.922994
 lv:
     loss:17047.782261
     acc:0.301568
 le:
     loss:1517.972174
     acc:0.999624
 encoder:
     loss:0.169125
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.382617
     acc:0.852428
 tv:
     loss:19106.099609
     acc:0.489951
 lu:
     loss:13414.519482
     acc:0.913885
 lv:
     loss:16751.091895
     acc:0.188968
 le:
     loss:1449.200879
     acc:0.999000
 encoder:
     loss:0.174249
----------------------------


Epoch: [696/1000]:
train:
----------------------------
 tu:
     loss:18286.183537
     acc:0.854504
 tv:
     loss:19820.298930
     acc:0.538472
 lu:
     loss:14037.052189
     acc:0.922472
 lv:
     loss:17047.181198
     acc:0.301151
 le:
     loss:1517.929242
     acc:0.999630
 encoder:
     loss:0.168256
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.030371
     acc:0.852660
 tv:
     loss:19106.824023
     acc:0.489624
 lu:
     loss:13415.286914
     acc:0.913828
 lv:
     loss:16752.274805
     acc:0.188811
 le:
     loss:1449.055670
     acc:0.999004
 encoder:
     loss:0.142593
----------------------------


Epoch: [697/1000]:
train:
----------------------------
 tu:
     loss:18285.765580
     acc:0.854490
 tv:
     loss:19815.686319
     acc:0.539285
 lu:
     loss:14036.363452
     acc:0.923105
 lv:
     loss:17042.310513
     acc:0.302618
 le:
     loss:1517.853941
     acc:0.999645
 encoder:
     loss:0.160915
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.429590
     acc:0.852531
 tv:
     loss:19104.347461
     acc:0.490049
 lu:
     loss:13415.655713
     acc:0.913733
 lv:
     loss:16750.635547
     acc:0.188814
 le:
     loss:1449.046820
     acc:0.999023
 encoder:
     loss:0.163779
----------------------------


Epoch: [698/1000]:
train:
----------------------------
 tu:
     loss:18285.695017
     acc:0.854497
 tv:
     loss:19816.896689
     acc:0.538961
 lu:
     loss:14036.473610
     acc:0.923062
 lv:
     loss:17044.828727
     acc:0.301920
 le:
     loss:1517.847343
     acc:0.999646
 encoder:
     loss:0.172579
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.514258
     acc:0.852514
 tv:
     loss:19103.861328
     acc:0.490055
 lu:
     loss:13416.585986
     acc:0.913222
 lv:
     loss:16755.444043
     acc:0.188107
 le:
     loss:1449.055914
     acc:0.999022
 encoder:
     loss:0.167753
----------------------------


Epoch: [699/1000]:
train:
----------------------------
 tu:
     loss:18286.534475
     acc:0.854336
 tv:
     loss:19816.662825
     acc:0.539352
 lu:
     loss:14036.823219
     acc:0.923105
 lv:
     loss:17044.483932
     acc:0.302123
 le:
     loss:1517.919136
     acc:0.999630
 encoder:
     loss:0.161629
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.745215
     acc:0.852694
 tv:
     loss:19108.758398
     acc:0.488826
 lu:
     loss:13415.579590
     acc:0.913668
 lv:
     loss:16750.818262
     acc:0.189217
 le:
     loss:1448.445081
     acc:0.999167
 encoder:
     loss:0.158390
----------------------------


Epoch: [700/1000]:
train:
----------------------------
 tu:
     loss:18285.898108
     acc:0.854509
 tv:
     loss:19818.190157
     acc:0.538646
 lu:
     loss:14036.510810
     acc:0.923006
 lv:
     loss:17044.241007
     acc:0.301896
 le:
     loss:1517.904264
     acc:0.999638
 encoder:
     loss:0.174583
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.057617
     acc:0.852593
 tv:
     loss:19104.330469
     acc:0.490093
 lu:
     loss:13415.714697
     acc:0.913577
 lv:
     loss:16745.877539
     acc:0.190154
 le:
     loss:1448.441229
     acc:0.999167
 encoder:
     loss:0.439364
----------------------------


Epoch: [701/1000]:
train:
----------------------------
 tu:
     loss:18286.095294
     acc:0.854400
 tv:
     loss:19817.064816
     acc:0.539096
 lu:
     loss:14036.750216
     acc:0.923103
 lv:
     loss:17044.982740
     acc:0.301832
 le:
     loss:1517.957676
     acc:0.999621
 encoder:
     loss:0.446805
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.352930
     acc:0.852592
 tv:
     loss:19105.148535
     acc:0.489686
 lu:
     loss:13414.456641
     acc:0.913875
 lv:
     loss:16749.764844
     acc:0.189495
 le:
     loss:1448.649896
     acc:0.999104
 encoder:
     loss:0.269414
----------------------------


Epoch: [702/1000]:
train:
----------------------------
 tu:
     loss:18285.590866
     acc:0.854487
 tv:
     loss:19815.770179
     acc:0.539190
 lu:
     loss:14035.916231
     acc:0.923257
 lv:
     loss:17040.506189
     acc:0.302902
 le:
     loss:1517.924382
     acc:0.999635
 encoder:
     loss:0.211291
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.845410
     acc:0.852432
 tv:
     loss:19105.867871
     acc:0.489747
 lu:
     loss:13414.702197
     acc:0.914032
 lv:
     loss:16750.943262
     acc:0.189191
 le:
     loss:1448.851727
     acc:0.999084
 encoder:
     loss:0.174106
----------------------------


Epoch: [703/1000]:
train:
----------------------------
 tu:
     loss:18284.923090
     acc:0.854564
 tv:
     loss:19815.931800
     acc:0.539241
 lu:
     loss:14036.869845
     acc:0.923034
 lv:
     loss:17042.835199
     acc:0.302768
 le:
     loss:1517.888557
     acc:0.999641
 encoder:
     loss:0.257612
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.905762
     acc:0.852327
 tv:
     loss:19107.249707
     acc:0.489410
 lu:
     loss:13416.091504
     acc:0.913462
 lv:
     loss:16753.532617
     acc:0.188514
 le:
     loss:1448.942914
     acc:0.999063
 encoder:
     loss:0.216590
----------------------------


Epoch: [704/1000]:
train:
----------------------------
 tu:
     loss:18286.245140
     acc:0.854419
 tv:
     loss:19815.034600
     acc:0.539159
 lu:
     loss:14036.630644
     acc:0.923092
 lv:
     loss:17038.854867
     acc:0.302955
 le:
     loss:1517.997718
     acc:0.999616
 encoder:
     loss:0.194812
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.544141
     acc:0.852476
 tv:
     loss:19107.993262
     acc:0.489105
 lu:
     loss:13416.780225
     acc:0.913345
 lv:
     loss:16751.053613
     acc:0.189204
 le:
     loss:1448.840552
     acc:0.999041
 encoder:
     loss:0.210745
----------------------------


Epoch: [705/1000]:
train:
----------------------------
 tu:
     loss:18286.440816
     acc:0.854346
 tv:
     loss:19816.914369
     acc:0.538904
 lu:
     loss:14036.853686
     acc:0.922883
 lv:
     loss:17044.181027
     acc:0.302066
 le:
     loss:1517.966618
     acc:0.999632
 encoder:
     loss:0.213901
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.496777
     acc:0.852405
 tv:
     loss:19105.004297
     acc:0.489865
 lu:
     loss:13414.800293
     acc:0.913715
 lv:
     loss:16749.844629
     acc:0.189307
 le:
     loss:1448.583624
     acc:0.999125
 encoder:
     loss:0.184062
----------------------------


Epoch: [706/1000]:
train:
----------------------------
 tu:
     loss:18285.444722
     acc:0.854611
 tv:
     loss:19815.359137
     acc:0.539240
 lu:
     loss:14035.991642
     acc:0.923194
 lv:
     loss:17040.889126
     acc:0.302571
 le:
     loss:1517.856241
     acc:0.999652
 encoder:
     loss:0.200308
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.129102
     acc:0.852581
 tv:
     loss:19107.799609
     acc:0.489157
 lu:
     loss:13416.462451
     acc:0.913207
 lv:
     loss:16752.309668
     acc:0.189208
 le:
     loss:1448.656531
     acc:0.999125
 encoder:
     loss:0.199785
----------------------------


Epoch: [707/1000]:
train:
----------------------------
 tu:
     loss:18286.155137
     acc:0.854480
 tv:
     loss:19815.959586
     acc:0.539115
 lu:
     loss:14036.158283
     acc:0.923253
 lv:
     loss:17043.945222
     acc:0.302174
 le:
     loss:1517.853107
     acc:0.999650
 encoder:
     loss:0.236403
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.318848
     acc:0.852353
 tv:
     loss:19103.805664
     acc:0.490154
 lu:
     loss:13414.898486
     acc:0.913679
 lv:
     loss:16755.135645
     acc:0.187967
 le:
     loss:1449.075317
     acc:0.999023
 encoder:
     loss:0.206373
----------------------------


Epoch: [708/1000]:
train:
----------------------------
 tu:
     loss:18285.558923
     acc:0.854578
 tv:
     loss:19818.890103
     acc:0.538692
 lu:
     loss:14036.984954
     acc:0.923020
 lv:
     loss:17049.178450
     acc:0.301162
 le:
     loss:1517.909076
     acc:0.999641
 encoder:
     loss:0.193274
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.025000
     acc:0.852576
 tv:
     loss:19102.729395
     acc:0.490747
 lu:
     loss:13414.504883
     acc:0.913714
 lv:
     loss:16755.604590
     acc:0.188461
 le:
     loss:1448.776404
     acc:0.999105
 encoder:
     loss:0.211768
----------------------------


Epoch: [709/1000]:
train:
----------------------------
 tu:
     loss:18285.891125
     acc:0.854494
 tv:
     loss:19819.198662
     acc:0.538673
 lu:
     loss:14035.529058
     acc:0.923399
 lv:
     loss:17045.637900
     acc:0.301776
 le:
     loss:1517.866875
     acc:0.999637
 encoder:
     loss:0.193656
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.047070
     acc:0.852270
 tv:
     loss:19106.337207
     acc:0.489885
 lu:
     loss:13414.413428
     acc:0.913829
 lv:
     loss:16752.657715
     acc:0.188685
 le:
     loss:1449.255756
     acc:0.998981
 encoder:
     loss:0.202573
----------------------------


Epoch: [710/1000]:
train:
----------------------------
 tu:
     loss:18286.826603
     acc:0.854190
 tv:
     loss:19817.594681
     acc:0.538843
 lu:
     loss:14036.521121
     acc:0.923065
 lv:
     loss:17042.793366
     acc:0.302348
 le:
     loss:1518.002586
     acc:0.999614
 encoder:
     loss:0.171927
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.431348
     acc:0.852472
 tv:
     loss:19103.693945
     acc:0.490180
 lu:
     loss:13416.832813
     acc:0.913101
 lv:
     loss:16759.056543
     acc:0.186793
 le:
     loss:1449.197961
     acc:0.999000
 encoder:
     loss:0.142384
----------------------------


Epoch: [711/1000]:
train:
----------------------------
 tu:
     loss:18286.009084
     acc:0.854439
 tv:
     loss:19815.335847
     acc:0.539319
 lu:
     loss:14036.543650
     acc:0.923179
 lv:
     loss:17044.021609
     acc:0.302229
 le:
     loss:1517.955013
     acc:0.999627
 encoder:
     loss:0.188018
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.893164
     acc:0.852677
 tv:
     loss:19105.433789
     acc:0.490004
 lu:
     loss:13413.340625
     acc:0.914004
 lv:
     loss:16756.931250
     acc:0.187392
 le:
     loss:1448.538586
     acc:0.999147
 encoder:
     loss:0.183935
----------------------------


Epoch: [712/1000]:
train:
----------------------------
 tu:
     loss:18285.015137
     acc:0.854647
 tv:
     loss:19817.027412
     acc:0.539117
 lu:
     loss:14036.659123
     acc:0.923016
 lv:
     loss:17040.665357
     acc:0.302842
 le:
     loss:1517.907830
     acc:0.999631
 encoder:
     loss:0.194187
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.838184
     acc:0.852715
 tv:
     loss:19107.703711
     acc:0.489344
 lu:
     loss:13415.729395
     acc:0.913439
 lv:
     loss:16755.186035
     acc:0.187673
 le:
     loss:1449.103558
     acc:0.999040
 encoder:
     loss:0.153989
----------------------------


Epoch: [713/1000]:
train:
----------------------------
 tu:
     loss:18285.443234
     acc:0.854572
 tv:
     loss:19815.939476
     acc:0.539151
 lu:
     loss:14037.181504
     acc:0.923011
 lv:
     loss:17046.411133
     acc:0.301643
 le:
     loss:1517.912607
     acc:0.999641
 encoder:
     loss:0.170025
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.326660
     acc:0.852582
 tv:
     loss:19108.322168
     acc:0.489121
 lu:
     loss:13413.319189
     acc:0.913906
 lv:
     loss:16751.334961
     acc:0.189081
 le:
     loss:1448.421515
     acc:0.999167
 encoder:
     loss:0.167983
----------------------------


Epoch: [714/1000]:
train:
----------------------------
 tu:
     loss:18286.031205
     acc:0.854359
 tv:
     loss:19816.523892
     acc:0.538950
 lu:
     loss:14036.982808
     acc:0.923036
 lv:
     loss:17041.488202
     acc:0.302642
 le:
     loss:1517.949899
     acc:0.999622
 encoder:
     loss:0.204415
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.035547
     acc:0.852644
 tv:
     loss:19107.578418
     acc:0.489434
 lu:
     loss:13414.615576
     acc:0.913708
 lv:
     loss:16752.066309
     acc:0.188669
 le:
     loss:1449.152490
     acc:0.998983
 encoder:
     loss:0.268713
----------------------------


Epoch: [715/1000]:
train:
----------------------------
 tu:
     loss:18286.504519
     acc:0.854287
 tv:
     loss:19818.243300
     acc:0.538700
 lu:
     loss:14036.502918
     acc:0.923084
 lv:
     loss:17041.247366
     acc:0.302769
 le:
     loss:1518.043275
     acc:0.999606
 encoder:
     loss:0.299678
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.502832
     acc:0.852453
 tv:
     loss:19104.142773
     acc:0.490026
 lu:
     loss:13413.410547
     acc:0.913980
 lv:
     loss:16744.456445
     acc:0.190303
 le:
     loss:1449.115430
     acc:0.999023
 encoder:
     loss:0.420880
----------------------------


Epoch: [716/1000]:
train:
----------------------------
 tu:
     loss:18285.992574
     acc:0.854407
 tv:
     loss:19816.754156
     acc:0.539137
 lu:
     loss:14036.759005
     acc:0.923027
 lv:
     loss:17040.651617
     acc:0.302920
 le:
     loss:1517.924952
     acc:0.999635
 encoder:
     loss:0.267189
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.693945
     acc:0.852515
 tv:
     loss:19103.391602
     acc:0.490251
 lu:
     loss:13416.123975
     acc:0.913344
 lv:
     loss:16759.189941
     acc:0.187235
 le:
     loss:1449.075909
     acc:0.999021
 encoder:
     loss:0.182784
----------------------------


Epoch: [717/1000]:
train:
----------------------------
 tu:
     loss:18285.959007
     acc:0.854370
 tv:
     loss:19815.996253
     acc:0.539411
 lu:
     loss:14035.997184
     acc:0.923224
 lv:
     loss:17040.947163
     acc:0.302947
 le:
     loss:1517.955261
     acc:0.999626
 encoder:
     loss:0.213346
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.917187
     acc:0.852202
 tv:
     loss:19105.176660
     acc:0.489772
 lu:
     loss:13416.418457
     acc:0.913290
 lv:
     loss:16751.608203
     acc:0.188527
 le:
     loss:1449.034595
     acc:0.999023
 encoder:
     loss:0.205332
----------------------------


Epoch: [718/1000]:
train:
----------------------------
 tu:
     loss:18285.416356
     acc:0.854566
 tv:
     loss:19815.595124
     acc:0.539414
 lu:
     loss:14035.822902
     acc:0.923263
 lv:
     loss:17040.613293
     acc:0.303162
 le:
     loss:1517.897397
     acc:0.999637
 encoder:
     loss:0.235618
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.501953
     acc:0.852488
 tv:
     loss:19108.429980
     acc:0.489342
 lu:
     loss:13412.702051
     acc:0.914212
 lv:
     loss:16753.013867
     acc:0.189105
 le:
     loss:1448.638696
     acc:0.999125
 encoder:
     loss:0.210707
----------------------------


Epoch: [719/1000]:
train:
----------------------------
 tu:
     loss:18285.998547
     acc:0.854397
 tv:
     loss:19814.909668
     acc:0.539409
 lu:
     loss:14035.231093
     acc:0.923320
 lv:
     loss:17040.855026
     acc:0.302672
 le:
     loss:1517.835094
     acc:0.999648
 encoder:
     loss:0.193619
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.330566
     acc:0.852168
 tv:
     loss:19107.767676
     acc:0.489251
 lu:
     loss:13413.598779
     acc:0.913928
 lv:
     loss:16753.426367
     acc:0.188624
 le:
     loss:1448.714557
     acc:0.999106
 encoder:
     loss:0.176076
----------------------------


Epoch: [720/1000]:
train:
----------------------------
 tu:
     loss:18285.791935
     acc:0.854477
 tv:
     loss:19813.898790
     acc:0.539479
 lu:
     loss:14034.510402
     acc:0.923548
 lv:
     loss:17037.010913
     acc:0.303392
 le:
     loss:1517.821931
     acc:0.999655
 encoder:
     loss:0.239237
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.410938
     acc:0.852131
 tv:
     loss:19102.668457
     acc:0.490422
 lu:
     loss:13412.384863
     acc:0.914319
 lv:
     loss:16755.762012
     acc:0.187685
 le:
     loss:1449.416705
     acc:0.998939
 encoder:
     loss:0.220764
----------------------------


Epoch: [721/1000]:
train:
----------------------------
 tu:
     loss:18285.870106
     acc:0.854398
 tv:
     loss:19814.972815
     acc:0.539455
 lu:
     loss:14034.677246
     acc:0.923515
 lv:
     loss:17040.537178
     acc:0.302992
 le:
     loss:1517.934656
     acc:0.999622
 encoder:
     loss:0.194728
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.325000
     acc:0.852250
 tv:
     loss:19109.503223
     acc:0.489286
 lu:
     loss:13410.979590
     acc:0.914626
 lv:
     loss:16755.881836
     acc:0.188050
 le:
     loss:1448.876758
     acc:0.999084
 encoder:
     loss:0.172918
----------------------------


Epoch: [722/1000]:
train:
----------------------------
 tu:
     loss:18286.789687
     acc:0.854306
 tv:
     loss:19816.746923
     acc:0.539002
 lu:
     loss:14034.379758
     acc:0.923490
 lv:
     loss:17039.858296
     acc:0.302962
 le:
     loss:1517.836642
     acc:0.999648
 encoder:
     loss:0.202239
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.843262
     acc:0.852313
 tv:
     loss:19105.814453
     acc:0.490043
 lu:
     loss:13410.620850
     acc:0.914785
 lv:
     loss:16747.731250
     acc:0.189982
 le:
     loss:1448.371619
     acc:0.999188
 encoder:
     loss:0.185890
----------------------------


Epoch: [723/1000]:
train:
----------------------------
 tu:
     loss:18285.186751
     acc:0.854570
 tv:
     loss:19813.956600
     acc:0.539589
 lu:
     loss:14033.450877
     acc:0.923849
 lv:
     loss:17038.628827
     acc:0.303268
 le:
     loss:1517.969680
     acc:0.999627
 encoder:
     loss:0.449164
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.620996
     acc:0.852536
 tv:
     loss:19107.104785
     acc:0.489582
 lu:
     loss:13411.725195
     acc:0.914274
 lv:
     loss:16757.262988
     acc:0.187598
 le:
     loss:1448.587445
     acc:0.999126
 encoder:
     loss:0.281741
----------------------------


Epoch: [724/1000]:
train:
----------------------------
 tu:
     loss:18285.787416
     acc:0.854490
 tv:
     loss:19814.935149
     acc:0.539325
 lu:
     loss:14034.512582
     acc:0.923591
 lv:
     loss:17038.357490
     acc:0.303520
 le:
     loss:1517.864263
     acc:0.999649
 encoder:
     loss:0.256041
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.570312
     acc:0.852475
 tv:
     loss:19104.098340
     acc:0.490001
 lu:
     loss:13409.348633
     acc:0.914821
 lv:
     loss:16751.044531
     acc:0.188749
 le:
     loss:1448.827826
     acc:0.999062
 encoder:
     loss:0.278673
----------------------------


Epoch: [725/1000]:
train:
----------------------------
 tu:
     loss:18285.661973
     acc:0.854459
 tv:
     loss:19812.670705
     acc:0.540032
 lu:
     loss:14032.565055
     acc:0.923940
 lv:
     loss:17037.342331
     acc:0.303498
 le:
     loss:1517.790359
     acc:0.999667
 encoder:
     loss:0.277755
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.893945
     acc:0.852581
 tv:
     loss:19102.052734
     acc:0.490340
 lu:
     loss:13411.152441
     acc:0.914564
 lv:
     loss:16760.549902
     acc:0.187189
 le:
     loss:1449.040161
     acc:0.999020
 encoder:
     loss:0.219616
----------------------------


Epoch: [726/1000]:
train:
----------------------------
 tu:
     loss:18285.263127
     acc:0.854556
 tv:
     loss:19814.080498
     acc:0.539676
 lu:
     loss:14033.008221
     acc:0.923787
 lv:
     loss:17038.229174
     acc:0.303268
 le:
     loss:1517.859771
     acc:0.999642
 encoder:
     loss:0.194881
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.845996
     acc:0.852741
 tv:
     loss:19106.453906
     acc:0.489913
 lu:
     loss:13409.387402
     acc:0.914687
 lv:
     loss:16755.507812
     acc:0.187779
 le:
     loss:1449.103638
     acc:0.998981
 encoder:
     loss:0.163134
----------------------------


Epoch: [727/1000]:
train:
----------------------------
 tu:
     loss:18286.406852
     acc:0.854294
 tv:
     loss:19815.438965
     acc:0.539169
 lu:
     loss:14033.144906
     acc:0.923749
 lv:
     loss:17041.388161
     acc:0.302664
 le:
     loss:1517.869450
     acc:0.999649
 encoder:
     loss:0.217465
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.462891
     acc:0.852228
 tv:
     loss:19104.493555
     acc:0.490316
 lu:
     loss:13409.602051
     acc:0.914940
 lv:
     loss:16750.889648
     acc:0.189460
 le:
     loss:1449.061914
     acc:0.999041
 encoder:
     loss:0.232648
----------------------------


Epoch: [728/1000]:
train:
----------------------------
 tu:
     loss:18285.624421
     acc:0.854465
 tv:
     loss:19816.175656
     acc:0.539197
 lu:
     loss:14033.018441
     acc:0.923907
 lv:
     loss:17039.042832
     acc:0.303017
 le:
     loss:1517.786966
     acc:0.999660
 encoder:
     loss:0.197275
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.486035
     acc:0.852419
 tv:
     loss:19105.368555
     acc:0.489992
 lu:
     loss:13410.897607
     acc:0.914390
 lv:
     loss:16754.458887
     acc:0.188669
 le:
     loss:1448.845911
     acc:0.999042
 encoder:
     loss:0.182387
----------------------------


Epoch: [729/1000]:
train:
----------------------------
 tu:
     loss:18286.213913
     acc:0.854459
 tv:
     loss:19815.869765
     acc:0.539399
 lu:
     loss:14033.502248
     acc:0.923620
 lv:
     loss:17036.930312
     acc:0.303731
 le:
     loss:1517.870919
     acc:0.999642
 encoder:
     loss:0.222920
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.634668
     acc:0.852554
 tv:
     loss:19106.129883
     acc:0.489897
 lu:
     loss:13411.686475
     acc:0.914286
 lv:
     loss:16758.935645
     acc:0.186661
 le:
     loss:1449.212408
     acc:0.998979
 encoder:
     loss:0.267345
----------------------------


Epoch: [730/1000]:
train:
----------------------------
 tu:
     loss:18285.531034
     acc:0.854519
 tv:
     loss:19814.887525
     acc:0.539514
 lu:
     loss:14031.871241
     acc:0.924167
 lv:
     loss:17037.108024
     acc:0.303675
 le:
     loss:1517.858970
     acc:0.999653
 encoder:
     loss:0.264306
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.237402
     acc:0.852670
 tv:
     loss:19107.537793
     acc:0.489478
 lu:
     loss:13409.117432
     acc:0.914936
 lv:
     loss:16751.328516
     acc:0.188440
 le:
     loss:1448.615045
     acc:0.999126
 encoder:
     loss:0.255405
----------------------------


Epoch: [731/1000]:
train:
----------------------------
 tu:
     loss:18285.150243
     acc:0.854602
 tv:
     loss:19814.101381
     acc:0.539372
 lu:
     loss:14031.689078
     acc:0.924080
 lv:
     loss:17039.457497
     acc:0.302870
 le:
     loss:1517.947642
     acc:0.999632
 encoder:
     loss:0.442706
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.758691
     acc:0.852455
 tv:
     loss:19105.600586
     acc:0.489780
 lu:
     loss:13410.782373
     acc:0.914588
 lv:
     loss:16747.515039
     acc:0.189492
 le:
     loss:1448.992682
     acc:0.999064
 encoder:
     loss:0.730732
----------------------------


Epoch: [732/1000]:
train:
----------------------------
 tu:
     loss:18284.256711
     acc:0.854722
 tv:
     loss:19814.832168
     acc:0.539492
 lu:
     loss:14032.198185
     acc:0.924100
 lv:
     loss:17038.535758
     acc:0.303524
 le:
     loss:1517.929489
     acc:0.999622
 encoder:
     loss:0.413658
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.457715
     acc:0.852492
 tv:
     loss:19108.200879
     acc:0.489442
 lu:
     loss:13407.650293
     acc:0.915518
 lv:
     loss:16746.248926
     acc:0.190195
 le:
     loss:1448.789203
     acc:0.999084
 encoder:
     loss:0.247242
----------------------------


Epoch: [733/1000]:
train:
----------------------------
 tu:
     loss:18286.057470
     acc:0.854317
 tv:
     loss:19814.955237
     acc:0.539387
 lu:
     loss:14033.102482
     acc:0.923874
 lv:
     loss:17037.496310
     acc:0.303311
 le:
     loss:1517.908924
     acc:0.999638
 encoder:
     loss:0.247325
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.888867
     acc:0.852696
 tv:
     loss:19109.561816
     acc:0.488930
 lu:
     loss:13407.503174
     acc:0.915282
 lv:
     loss:16750.455273
     acc:0.188943
 le:
     loss:1448.414362
     acc:0.999146
 encoder:
     loss:0.244101
----------------------------


Epoch: [734/1000]:
train:
----------------------------
 tu:
     loss:18284.420864
     acc:0.854699
 tv:
     loss:19814.492903
     acc:0.539446
 lu:
     loss:14032.249444
     acc:0.923916
 lv:
     loss:17038.912723
     acc:0.303204
 le:
     loss:1517.947748
     acc:0.999625
 encoder:
     loss:0.195700
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.831641
     acc:0.852681
 tv:
     loss:19104.132227
     acc:0.490053
 lu:
     loss:13409.649316
     acc:0.914664
 lv:
     loss:16747.612500
     acc:0.189944
 le:
     loss:1448.496326
     acc:0.999169
 encoder:
     loss:0.162685
----------------------------


Epoch: [735/1000]:
train:
----------------------------
 tu:
     loss:18285.880564
     acc:0.854448
 tv:
     loss:19812.171160
     acc:0.540025
 lu:
     loss:14032.378588
     acc:0.923969
 lv:
     loss:17034.517340
     acc:0.304163
 le:
     loss:1517.995490
     acc:0.999615
 encoder:
     loss:0.199174
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.714941
     acc:0.852616
 tv:
     loss:19105.122070
     acc:0.490135
 lu:
     loss:13408.530908
     acc:0.915225
 lv:
     loss:16757.451855
     acc:0.187573
 le:
     loss:1448.241370
     acc:0.999209
 encoder:
     loss:0.179327
----------------------------


Epoch: [736/1000]:
train:
----------------------------
 tu:
     loss:18285.794195
     acc:0.854463
 tv:
     loss:19815.115155
     acc:0.539336
 lu:
     loss:14031.746468
     acc:0.924084
 lv:
     loss:17034.214401
     acc:0.304251
 le:
     loss:1517.912593
     acc:0.999634
 encoder:
     loss:0.192252
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.918066
     acc:0.852393
 tv:
     loss:19105.956445
     acc:0.489745
 lu:
     loss:13409.980762
     acc:0.914515
 lv:
     loss:16753.748242
     acc:0.188814
 le:
     loss:1448.909906
     acc:0.999064
 encoder:
     loss:0.168899
----------------------------


Epoch: [737/1000]:
train:
----------------------------
 tu:
     loss:18284.562114
     acc:0.854676
 tv:
     loss:19814.477562
     acc:0.539538
 lu:
     loss:14032.145042
     acc:0.923931
 lv:
     loss:17035.779308
     acc:0.303452
 le:
     loss:1517.898711
     acc:0.999640
 encoder:
     loss:0.185580
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.920508
     acc:0.852582
 tv:
     loss:19107.437012
     acc:0.489631
 lu:
     loss:13407.554785
     acc:0.915389
 lv:
     loss:16749.431543
     acc:0.189661
 le:
     loss:1448.573553
     acc:0.999127
 encoder:
     loss:0.187656
----------------------------


Epoch: [738/1000]:
train:
----------------------------
 tu:
     loss:18285.242835
     acc:0.854564
 tv:
     loss:19814.019066
     acc:0.539656
 lu:
     loss:14031.544331
     acc:0.924080
 lv:
     loss:17033.585529
     acc:0.304606
 le:
     loss:1517.878472
     acc:0.999643
 encoder:
     loss:0.168446
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.707422
     acc:0.852514
 tv:
     loss:19106.136035
     acc:0.489633
 lu:
     loss:13409.591797
     acc:0.914708
 lv:
     loss:16757.776562
     acc:0.187306
 le:
     loss:1448.898999
     acc:0.999065
 encoder:
     loss:0.156718
----------------------------


Epoch: [739/1000]:
train:
----------------------------
 tu:
     loss:18285.449219
     acc:0.854553
 tv:
     loss:19815.029955
     acc:0.539468
 lu:
     loss:14030.957372
     acc:0.924282
 lv:
     loss:17034.318348
     acc:0.304207
 le:
     loss:1517.826744
     acc:0.999661
 encoder:
     loss:0.190727
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.464062
     acc:0.852528
 tv:
     loss:19104.726953
     acc:0.489865
 lu:
     loss:13411.521924
     acc:0.914292
 lv:
     loss:16757.397168
     acc:0.187781
 le:
     loss:1448.823737
     acc:0.999064
 encoder:
     loss:0.177519
----------------------------


Epoch: [740/1000]:
train:
----------------------------
 tu:
     loss:18285.228675
     acc:0.854565
 tv:
     loss:19815.635129
     acc:0.539207
 lu:
     loss:14031.743845
     acc:0.924185
 lv:
     loss:17037.563795
     acc:0.303530
 le:
     loss:1517.852663
     acc:0.999652
 encoder:
     loss:0.189465
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.879785
     acc:0.852753
 tv:
     loss:19104.603809
     acc:0.490016
 lu:
     loss:13408.596094
     acc:0.915353
 lv:
     loss:16755.648730
     acc:0.188410
 le:
     loss:1449.182141
     acc:0.999000
 encoder:
     loss:0.176477
----------------------------


Epoch: [741/1000]:
train:
----------------------------
 tu:
     loss:18285.811796
     acc:0.854338
 tv:
     loss:19814.376567
     acc:0.539457
 lu:
     loss:14032.564782
     acc:0.924031
 lv:
     loss:17036.356650
     acc:0.303820
 le:
     loss:1517.910586
     acc:0.999643
 encoder:
     loss:0.185747
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.388379
     acc:0.852516
 tv:
     loss:19104.105078
     acc:0.490171
 lu:
     loss:13408.730859
     acc:0.915093
 lv:
     loss:16759.324902
     acc:0.186763
 le:
     loss:1448.557983
     acc:0.999147
 encoder:
     loss:0.170978
----------------------------


Epoch: [742/1000]:
train:
----------------------------
 tu:
     loss:18286.490223
     acc:0.854212
 tv:
     loss:19813.968364
     acc:0.539455
 lu:
     loss:14030.932220
     acc:0.924174
 lv:
     loss:17035.886458
     acc:0.303685
 le:
     loss:1517.893533
     acc:0.999638
 encoder:
     loss:0.161520
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.785059
     acc:0.852307
 tv:
     loss:19108.522559
     acc:0.488961
 lu:
     loss:13409.106396
     acc:0.914847
 lv:
     loss:16761.853320
     acc:0.185763
 le:
     loss:1448.757080
     acc:0.999084
 encoder:
     loss:0.135131
----------------------------


Epoch: [743/1000]:
train:
----------------------------
 tu:
     loss:18285.474882
     acc:0.854482
 tv:
     loss:19812.945596
     acc:0.539902
 lu:
     loss:14030.573583
     acc:0.924431
 lv:
     loss:17035.217047
     acc:0.304034
 le:
     loss:1517.809992
     acc:0.999651
 encoder:
     loss:0.252905
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.363672
     acc:0.852438
 tv:
     loss:19108.106055
     acc:0.489098
 lu:
     loss:13408.460400
     acc:0.915301
 lv:
     loss:16756.897852
     acc:0.187787
 le:
     loss:1448.886694
     acc:0.999065
 encoder:
     loss:0.186796
----------------------------


Epoch: [744/1000]:
train:
----------------------------
 tu:
     loss:18285.840741
     acc:0.854469
 tv:
     loss:19813.489996
     acc:0.539646
 lu:
     loss:14030.841218
     acc:0.924314
 lv:
     loss:17032.791243
     acc:0.304421
 le:
     loss:1517.806025
     acc:0.999660
 encoder:
     loss:0.292068
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.236133
     acc:0.852629
 tv:
     loss:19105.736035
     acc:0.489490
 lu:
     loss:13408.328711
     acc:0.915068
 lv:
     loss:16756.913184
     acc:0.188100
 le:
     loss:1449.225616
     acc:0.998980
 encoder:
     loss:0.234797
----------------------------


Epoch: [745/1000]:
train:
----------------------------
 tu:
     loss:18285.414290
     acc:0.854520
 tv:
     loss:19813.889103
     acc:0.539868
 lu:
     loss:14031.609102
     acc:0.924183
 lv:
     loss:17034.190884
     acc:0.304514
 le:
     loss:1517.928135
     acc:0.999624
 encoder:
     loss:0.467117
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.488086
     acc:0.852657
 tv:
     loss:19104.431152
     acc:0.490342
 lu:
     loss:13408.754053
     acc:0.915066
 lv:
     loss:16758.991211
     acc:0.186833
 le:
     loss:1448.607428
     acc:0.999125
 encoder:
     loss:0.680966
----------------------------


Epoch: [746/1000]:
train:
----------------------------
 tu:
     loss:18285.176054
     acc:0.854555
 tv:
     loss:19814.526980
     acc:0.539473
 lu:
     loss:14031.608864
     acc:0.924100
 lv:
     loss:17031.470828
     acc:0.304861
 le:
     loss:1517.838846
     acc:0.999646
 encoder:
     loss:0.490718
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.298730
     acc:0.852631
 tv:
     loss:19108.455762
     acc:0.489361
 lu:
     loss:13407.315430
     acc:0.915127
 lv:
     loss:16753.428418
     acc:0.188765
 le:
     loss:1448.803174
     acc:0.999065
 encoder:
     loss:0.404307
----------------------------


Epoch: [747/1000]:
train:
----------------------------
 tu:
     loss:18285.728845
     acc:0.854426
 tv:
     loss:19813.411791
     acc:0.539548
 lu:
     loss:14031.702807
     acc:0.924192
 lv:
     loss:17032.018373
     acc:0.304684
 le:
     loss:1518.065833
     acc:0.999600
 encoder:
     loss:0.275076
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.215332
     acc:0.852554
 tv:
     loss:19104.554297
     acc:0.490437
 lu:
     loss:13408.190479
     acc:0.915143
 lv:
     loss:16748.945020
     acc:0.189579
 le:
     loss:1448.810284
     acc:0.999084
 encoder:
     loss:0.171016
----------------------------


Epoch: [748/1000]:
train:
----------------------------
 tu:
     loss:18285.774335
     acc:0.854443
 tv:
     loss:19813.802053
     acc:0.539654
 lu:
     loss:14030.920160
     acc:0.924333
 lv:
     loss:17031.038688
     acc:0.304969
 le:
     loss:1517.862141
     acc:0.999648
 encoder:
     loss:0.211432
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.197070
     acc:0.852596
 tv:
     loss:19110.752930
     acc:0.488264
 lu:
     loss:13408.087695
     acc:0.915279
 lv:
     loss:16751.458496
     acc:0.189033
 le:
     loss:1448.762915
     acc:0.999084
 encoder:
     loss:0.176339
----------------------------


Epoch: [749/1000]:
train:
----------------------------
 tu:
     loss:18285.587107
     acc:0.854449
 tv:
     loss:19813.489065
     acc:0.539840
 lu:
     loss:14030.086607
     acc:0.924452
 lv:
     loss:17031.467921
     acc:0.305041
 le:
     loss:1517.873986
     acc:0.999634
 encoder:
     loss:0.195070
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.828027
     acc:0.852306
 tv:
     loss:19108.089062
     acc:0.489280
 lu:
     loss:13410.736719
     acc:0.914726
 lv:
     loss:16753.536133
     acc:0.188291
 le:
     loss:1448.732410
     acc:0.999085
 encoder:
     loss:0.201048
----------------------------


Epoch: [750/1000]:
train:
----------------------------
 tu:
     loss:18285.398812
     acc:0.854590
 tv:
     loss:19814.760458
     acc:0.539350
 lu:
     loss:14030.402548
     acc:0.924477
 lv:
     loss:17036.483819
     acc:0.303686
 le:
     loss:1517.799482
     acc:0.999658
 encoder:
     loss:0.218281
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.021094
     acc:0.852597
 tv:
     loss:19105.469043
     acc:0.489888
 lu:
     loss:13408.239453
     acc:0.915282
 lv:
     loss:16752.196582
     acc:0.188583
 le:
     loss:1448.729749
     acc:0.999107
 encoder:
     loss:0.184026
----------------------------


Epoch: [751/1000]:
train:
----------------------------
 tu:
     loss:18285.087232
     acc:0.854574
 tv:
     loss:19814.078341
     acc:0.539671
 lu:
     loss:14030.743879
     acc:0.924358
 lv:
     loss:17033.339287
     acc:0.304266
 le:
     loss:1517.906954
     acc:0.999635
 encoder:
     loss:0.217158
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.161719
     acc:0.852550
 tv:
     loss:19106.389355
     acc:0.489692
 lu:
     loss:13407.893018
     acc:0.915145
 lv:
     loss:16753.897852
     acc:0.188812
 le:
     loss:1448.765289
     acc:0.999084
 encoder:
     loss:0.366858
----------------------------


Epoch: [752/1000]:
train:
----------------------------
 tu:
     loss:18284.734307
     acc:0.854515
 tv:
     loss:19811.769157
     acc:0.539917
 lu:
     loss:14031.140977
     acc:0.924169
 lv:
     loss:17030.937114
     acc:0.304879
 le:
     loss:1517.841206
     acc:0.999658
 encoder:
     loss:0.311535
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.702930
     acc:0.852348
 tv:
     loss:19108.007812
     acc:0.489168
 lu:
     loss:13408.936963
     acc:0.914783
 lv:
     loss:16755.265723
     acc:0.188175
 le:
     loss:1448.720508
     acc:0.999141
 encoder:
     loss:0.183903
----------------------------


Epoch: [753/1000]:
train:
----------------------------
 tu:
     loss:18285.372331
     acc:0.854459
 tv:
     loss:19812.330941
     acc:0.539890
 lu:
     loss:14031.649232
     acc:0.924104
 lv:
     loss:17031.177723
     acc:0.304658
 le:
     loss:1517.867876
     acc:0.999639
 encoder:
     loss:0.198428
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.310547
     acc:0.852548
 tv:
     loss:19103.875586
     acc:0.490397
 lu:
     loss:13407.745557
     acc:0.915185
 lv:
     loss:16757.080273
     acc:0.188012
 le:
     loss:1448.900885
     acc:0.999065
 encoder:
     loss:0.231112
----------------------------


Epoch: [754/1000]:
train:
----------------------------
 tu:
     loss:18285.235079
     acc:0.854533
 tv:
     loss:19813.104242
     acc:0.539764
 lu:
     loss:14031.185377
     acc:0.924179
 lv:
     loss:17034.773256
     acc:0.303962
 le:
     loss:1517.751343
     acc:0.999671
 encoder:
     loss:0.229660
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.483203
     acc:0.852453
 tv:
     loss:19108.231934
     acc:0.488934
 lu:
     loss:13409.732324
     acc:0.914683
 lv:
     loss:16761.104883
     acc:0.186848
 le:
     loss:1449.164008
     acc:0.999003
 encoder:
     loss:0.188032
----------------------------


Epoch: [755/1000]:
train:
----------------------------
 tu:
     loss:18285.896984
     acc:0.854463
 tv:
     loss:19813.682401
     acc:0.539703
 lu:
     loss:14030.856718
     acc:0.924211
 lv:
     loss:17032.782908
     acc:0.304591
 le:
     loss:1518.020557
     acc:0.999611
 encoder:
     loss:0.378000
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.942188
     acc:0.852653
 tv:
     loss:19109.623828
     acc:0.488957
 lu:
     loss:13409.693555
     acc:0.914867
 lv:
     loss:16762.860449
     acc:0.185851
 le:
     loss:1448.695715
     acc:0.999085
 encoder:
     loss:0.389999
----------------------------


Epoch: [756/1000]:
train:
----------------------------
 tu:
     loss:18285.245946
     acc:0.854541
 tv:
     loss:19813.369686
     acc:0.539511
 lu:
     loss:14030.529195
     acc:0.924444
 lv:
     loss:17027.279297
     acc:0.305707
 le:
     loss:1517.802463
     acc:0.999666
 encoder:
     loss:0.353844
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.371484
     acc:0.852595
 tv:
     loss:19108.723926
     acc:0.489350
 lu:
     loss:13407.421191
     acc:0.915332
 lv:
     loss:16753.488184
     acc:0.188945
 le:
     loss:1448.573499
     acc:0.999148
 encoder:
     loss:0.306346
----------------------------


Epoch: [757/1000]:
train:
----------------------------
 tu:
     loss:18285.725359
     acc:0.854365
 tv:
     loss:19812.850586
     acc:0.539795
 lu:
     loss:14031.745730
     acc:0.924001
 lv:
     loss:17033.437534
     acc:0.304386
 le:
     loss:1517.870989
     acc:0.999644
 encoder:
     loss:0.263423
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.417676
     acc:0.852875
 tv:
     loss:19105.617480
     acc:0.489973
 lu:
     loss:13408.302148
     acc:0.915186
 lv:
     loss:16754.357324
     acc:0.187952
 le:
     loss:1448.639929
     acc:0.999107
 encoder:
     loss:0.204175
----------------------------


Epoch: [758/1000]:
train:
----------------------------
 tu:
     loss:18284.476562
     acc:0.854736
 tv:
     loss:19813.731286
     acc:0.539625
 lu:
     loss:14030.952705
     acc:0.924255
 lv:
     loss:17032.092444
     acc:0.304469
 le:
     loss:1517.882151
     acc:0.999641
 encoder:
     loss:0.352428
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.845313
     acc:0.852397
 tv:
     loss:19109.419824
     acc:0.488841
 lu:
     loss:13410.256738
     acc:0.914602
 lv:
     loss:16752.257617
     acc:0.188614
 le:
     loss:1449.132733
     acc:0.999005
 encoder:
     loss:0.360131
----------------------------


Epoch: [759/1000]:
train:
----------------------------
 tu:
     loss:18285.187591
     acc:0.854584
 tv:
     loss:19812.634595
     acc:0.539921
 lu:
     loss:14030.915243
     acc:0.924129
 lv:
     loss:17029.049441
     acc:0.305386
 le:
     loss:1517.907674
     acc:0.999638
 encoder:
     loss:0.264760
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.487988
     acc:0.852372
 tv:
     loss:19101.867090
     acc:0.490755
 lu:
     loss:13408.479834
     acc:0.915065
 lv:
     loss:16755.172363
     acc:0.188340
 le:
     loss:1448.529395
     acc:0.999148
 encoder:
     loss:0.257581
----------------------------


Epoch: [760/1000]:
train:
----------------------------
 tu:
     loss:18286.246219
     acc:0.854304
 tv:
     loss:19814.728958
     acc:0.539468
 lu:
     loss:14031.147359
     acc:0.924228
 lv:
     loss:17031.371537
     acc:0.304804
 le:
     loss:1517.812781
     acc:0.999657
 encoder:
     loss:0.319661
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.959375
     acc:0.852633
 tv:
     loss:19108.560254
     acc:0.489237
 lu:
     loss:13409.106348
     acc:0.915091
 lv:
     loss:16757.922168
     acc:0.187427
 le:
     loss:1448.628735
     acc:0.999105
 encoder:
     loss:0.292988
----------------------------


Epoch: [761/1000]:
train:
----------------------------
 tu:
     loss:18285.398301
     acc:0.854587
 tv:
     loss:19812.345783
     acc:0.539906
 lu:
     loss:14030.701376
     acc:0.924140
 lv:
     loss:17030.161156
     acc:0.304899
 le:
     loss:1517.861684
     acc:0.999643
 encoder:
     loss:0.329659
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.537695
     acc:0.852455
 tv:
     loss:19108.287207
     acc:0.489058
 lu:
     loss:13407.935449
     acc:0.915192
 lv:
     loss:16749.319824
     acc:0.189441
 le:
     loss:1448.861896
     acc:0.999082
 encoder:
     loss:0.282771
----------------------------


Epoch: [762/1000]:
train:
----------------------------
 tu:
     loss:18284.765727
     acc:0.854728
 tv:
     loss:19812.610987
     acc:0.539777
 lu:
     loss:14029.627248
     acc:0.924512
 lv:
     loss:17030.311205
     acc:0.304673
 le:
     loss:1517.932904
     acc:0.999627
 encoder:
     loss:0.255767
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.234570
     acc:0.852390
 tv:
     loss:19110.144727
     acc:0.488824
 lu:
     loss:13408.760156
     acc:0.914968
 lv:
     loss:16754.404883
     acc:0.188418
 le:
     loss:1448.702960
     acc:0.999105
 encoder:
     loss:0.201466
----------------------------


Epoch: [763/1000]:
train:
----------------------------
 tu:
     loss:18286.477357
     acc:0.854187
 tv:
     loss:19813.284827
     acc:0.539615
 lu:
     loss:14031.122672
     acc:0.924229
 lv:
     loss:17032.474246
     acc:0.304767
 le:
     loss:1517.804854
     acc:0.999659
 encoder:
     loss:0.229279
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.667773
     acc:0.852780
 tv:
     loss:19109.493164
     acc:0.488747
 lu:
     loss:13410.047754
     acc:0.914466
 lv:
     loss:16754.704980
     acc:0.187735
 le:
     loss:1448.628326
     acc:0.999126
 encoder:
     loss:0.200061
----------------------------


Epoch: [764/1000]:
train:
----------------------------
 tu:
     loss:18285.370174
     acc:0.854437
 tv:
     loss:19813.680062
     acc:0.539672
 lu:
     loss:14031.699957
     acc:0.924059
 lv:
     loss:17031.247956
     acc:0.304828
 le:
     loss:1518.020461
     acc:0.999611
 encoder:
     loss:0.242433
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.049316
     acc:0.852655
 tv:
     loss:19104.981445
     acc:0.489741
 lu:
     loss:13408.362842
     acc:0.915130
 lv:
     loss:16755.168555
     acc:0.187892
 le:
     loss:1448.713147
     acc:0.999086
 encoder:
     loss:0.566103
----------------------------


Epoch: [765/1000]:
train:
----------------------------
 tu:
     loss:18285.360295
     acc:0.854532
 tv:
     loss:19814.218716
     acc:0.539370
 lu:
     loss:14030.045115
     acc:0.924448
 lv:
     loss:17030.871423
     acc:0.304691
 le:
     loss:1517.827231
     acc:0.999648
 encoder:
     loss:0.393530
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.400391
     acc:0.852573
 tv:
     loss:19106.869824
     acc:0.489466
 lu:
     loss:13407.523047
     acc:0.915255
 lv:
     loss:16754.696680
     acc:0.188094
 le:
     loss:1448.803119
     acc:0.999082
 encoder:
     loss:0.209659
----------------------------


Epoch: [766/1000]:
train:
----------------------------
 tu:
     loss:18284.582542
     acc:0.854603
 tv:
     loss:19813.703920
     acc:0.539583
 lu:
     loss:14030.624614
     acc:0.924289
 lv:
     loss:17031.857376
     acc:0.304389
 le:
     loss:1517.754886
     acc:0.999671
 encoder:
     loss:0.203597
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.455469
     acc:0.852431
 tv:
     loss:19104.547656
     acc:0.489930
 lu:
     loss:13405.033936
     acc:0.915843
 lv:
     loss:16749.599121
     acc:0.189220
 le:
     loss:1448.893152
     acc:0.999065
 encoder:
     loss:0.200568
----------------------------


Epoch: [767/1000]:
train:
----------------------------
 tu:
     loss:18285.063181
     acc:0.854684
 tv:
     loss:19814.265591
     acc:0.539528
 lu:
     loss:14029.370174
     acc:0.924592
 lv:
     loss:17030.057345
     acc:0.305082
 le:
     loss:1517.764912
     acc:0.999670
 encoder:
     loss:0.223667
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.868945
     acc:0.852427
 tv:
     loss:19105.384863
     acc:0.490010
 lu:
     loss:13408.868555
     acc:0.914910
 lv:
     loss:16749.053711
     acc:0.189336
 le:
     loss:1448.865540
     acc:0.999118
 encoder:
     loss:0.269085
----------------------------


Epoch: [768/1000]:
train:
----------------------------
 tu:
     loss:18285.117097
     acc:0.854637
 tv:
     loss:19814.340014
     acc:0.539452
 lu:
     loss:14031.932515
     acc:0.924002
 lv:
     loss:17031.717035
     acc:0.304401
 le:
     loss:1517.897092
     acc:0.999634
 encoder:
     loss:0.237238
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.136719
     acc:0.852554
 tv:
     loss:19106.568652
     acc:0.489579
 lu:
     loss:13407.187939
     acc:0.915192
 lv:
     loss:16747.067676
     acc:0.189675
 le:
     loss:1448.779474
     acc:0.999083
 encoder:
     loss:0.170719
----------------------------


Epoch: [769/1000]:
train:
----------------------------
 tu:
     loss:18284.884380
     acc:0.854584
 tv:
     loss:19813.954635
     acc:0.539371
 lu:
     loss:14029.993868
     acc:0.924559
 lv:
     loss:17031.278036
     acc:0.304695
 le:
     loss:1517.870891
     acc:0.999647
 encoder:
     loss:0.188400
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.755664
     acc:0.852350
 tv:
     loss:19104.531348
     acc:0.490035
 lu:
     loss:13408.840381
     acc:0.914893
 lv:
     loss:16753.575293
     acc:0.188044
 le:
     loss:1448.981464
     acc:0.999099
 encoder:
     loss:0.196851
----------------------------


Epoch: [770/1000]:
train:
----------------------------
 tu:
     loss:18285.036905
     acc:0.854536
 tv:
     loss:19812.779161
     acc:0.540071
 lu:
     loss:14030.685286
     acc:0.924316
 lv:
     loss:17028.966933
     acc:0.305503
 le:
     loss:1517.901892
     acc:0.999637
 encoder:
     loss:0.183140
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.198047
     acc:0.852591
 tv:
     loss:19112.866504
     acc:0.488308
 lu:
     loss:13409.084473
     acc:0.914811
 lv:
     loss:16750.833789
     acc:0.188632
 le:
     loss:1448.977014
     acc:0.999044
 encoder:
     loss:0.174649
----------------------------


Epoch: [771/1000]:
train:
----------------------------
 tu:
     loss:18285.680937
     acc:0.854469
 tv:
     loss:19812.745435
     acc:0.539971
 lu:
     loss:14030.346680
     acc:0.924352
 lv:
     loss:17029.990995
     acc:0.305172
 le:
     loss:1517.970812
     acc:0.999618
 encoder:
     loss:0.301895
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.844727
     acc:0.852408
 tv:
     loss:19102.557715
     acc:0.489996
 lu:
     loss:13409.473096
     acc:0.914781
 lv:
     loss:16745.970801
     acc:0.189976
 le:
     loss:1449.342102
     acc:0.998998
 encoder:
     loss:0.297022
----------------------------


Epoch: [772/1000]:
train:
----------------------------
 tu:
     loss:18285.720022
     acc:0.854476
 tv:
     loss:19814.419331
     acc:0.539420
 lu:
     loss:14030.647245
     acc:0.924345
 lv:
     loss:17030.981559
     acc:0.304651
 le:
     loss:1517.836094
     acc:0.999653
 encoder:
     loss:0.695540
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.989160
     acc:0.852403
 tv:
     loss:19108.662012
     acc:0.489291
 lu:
     loss:13409.158252
     acc:0.914932
 lv:
     loss:16759.708008
     acc:0.186644
 le:
     loss:1448.858856
     acc:0.999086
 encoder:
     loss:1.750810
----------------------------


Epoch: [773/1000]:
train:
----------------------------
 tu:
     loss:18285.593795
     acc:0.854454
 tv:
     loss:19813.879565
     acc:0.539314
 lu:
     loss:14030.136696
     acc:0.924411
 lv:
     loss:17029.694699
     acc:0.305027
 le:
     loss:1517.849071
     acc:0.999648
 encoder:
     loss:1.275374
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.478125
     acc:0.852472
 tv:
     loss:19110.546777
     acc:0.488554
 lu:
     loss:13408.644922
     acc:0.915111
 lv:
     loss:16753.774805
     acc:0.188160
 le:
     loss:1448.875940
     acc:0.999065
 encoder:
     loss:0.775371
----------------------------


Epoch: [774/1000]:
train:
----------------------------
 tu:
     loss:18285.117494
     acc:0.854535
 tv:
     loss:19813.869981
     acc:0.539409
 lu:
     loss:14031.458769
     acc:0.924099
 lv:
     loss:17027.952864
     acc:0.305217
 le:
     loss:1517.920972
     acc:0.999637
 encoder:
     loss:0.629943
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.574023
     acc:0.852511
 tv:
     loss:19109.158398
     acc:0.488852
 lu:
     loss:13410.331299
     acc:0.914730
 lv:
     loss:16750.281641
     acc:0.188924
 le:
     loss:1448.742828
     acc:0.999106
 encoder:
     loss:0.413913
----------------------------


Epoch: [775/1000]:
train:
----------------------------
 tu:
     loss:18285.609296
     acc:0.854494
 tv:
     loss:19811.845408
     acc:0.540130
 lu:
     loss:14030.447039
     acc:0.924158
 lv:
     loss:17027.520326
     acc:0.305330
 le:
     loss:1517.878209
     acc:0.999638
 encoder:
     loss:0.402550
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.876465
     acc:0.852676
 tv:
     loss:19104.387598
     acc:0.490107
 lu:
     loss:13410.438574
     acc:0.914585
 lv:
     loss:16748.734473
     acc:0.189109
 le:
     loss:1448.575421
     acc:0.999107
 encoder:
     loss:0.333266
----------------------------


Epoch: [776/1000]:
train:
----------------------------
 tu:
     loss:18284.724337
     acc:0.854695
 tv:
     loss:19812.371491
     acc:0.539809
 lu:
     loss:14029.981570
     acc:0.924325
 lv:
     loss:17026.013842
     acc:0.305689
 le:
     loss:1517.834308
     acc:0.999654
 encoder:
     loss:0.316329
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.626758
     acc:0.852698
 tv:
     loss:19109.267578
     acc:0.489005
 lu:
     loss:13407.644238
     acc:0.915327
 lv:
     loss:16755.600293
     acc:0.187543
 le:
     loss:1448.784674
     acc:0.999102
 encoder:
     loss:0.222491
----------------------------


Epoch: [777/1000]:
train:
----------------------------
 tu:
     loss:18286.048113
     acc:0.854240
 tv:
     loss:19812.076967
     acc:0.540220
 lu:
     loss:14030.362134
     acc:0.924353
 lv:
     loss:17029.011548
     acc:0.305321
 le:
     loss:1517.860997
     acc:0.999641
 encoder:
     loss:0.273702
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.392383
     acc:0.852490
 tv:
     loss:19111.977148
     acc:0.488620
 lu:
     loss:13410.310547
     acc:0.914605
 lv:
     loss:16753.961621
     acc:0.189060
 le:
     loss:1448.606384
     acc:0.999126
 encoder:
     loss:0.308683
----------------------------


Epoch: [778/1000]:
train:
----------------------------
 tu:
     loss:18286.559457
     acc:0.854222
 tv:
     loss:19812.947402
     acc:0.539759
 lu:
     loss:14029.722293
     acc:0.924554
 lv:
     loss:17025.361044
     acc:0.306121
 le:
     loss:1517.902828
     acc:0.999631
 encoder:
     loss:0.316650
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.689648
     acc:0.852425
 tv:
     loss:19106.670508
     acc:0.489499
 lu:
     loss:13406.433203
     acc:0.915640
 lv:
     loss:16747.132617
     acc:0.189828
 le:
     loss:1448.630658
     acc:0.999144
 encoder:
     loss:0.281777
----------------------------


Epoch: [779/1000]:
train:
----------------------------
 tu:
     loss:18285.415766
     acc:0.854470
 tv:
     loss:19813.422829
     acc:0.539849
 lu:
     loss:14031.719568
     acc:0.924135
 lv:
     loss:17034.288393
     acc:0.304237
 le:
     loss:1517.948221
     acc:0.999628
 encoder:
     loss:0.262380
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.010059
     acc:0.852583
 tv:
     loss:19102.435254
     acc:0.490150
 lu:
     loss:13407.963916
     acc:0.915234
 lv:
     loss:16753.083984
     acc:0.188816
 le:
     loss:1448.511694
     acc:0.999146
 encoder:
     loss:0.203609
----------------------------


Epoch: [780/1000]:
train:
----------------------------
 tu:
     loss:18285.230378
     acc:0.854485
 tv:
     loss:19812.553927
     acc:0.540140
 lu:
     loss:14029.978675
     acc:0.924451
 lv:
     loss:17029.773142
     acc:0.304898
 le:
     loss:1517.706652
     acc:0.999677
 encoder:
     loss:0.221484
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.213672
     acc:0.852228
 tv:
     loss:19104.710645
     acc:0.490115
 lu:
     loss:13407.387939
     acc:0.915301
 lv:
     loss:16757.815234
     acc:0.187569
 le:
     loss:1448.779633
     acc:0.999124
 encoder:
     loss:0.223127
----------------------------


Epoch: [781/1000]:
train:
----------------------------
 tu:
     loss:18286.544468
     acc:0.854286
 tv:
     loss:19812.724246
     acc:0.540001
 lu:
     loss:14030.558492
     acc:0.924251
 lv:
     loss:17031.075184
     acc:0.304659
 le:
     loss:1517.947124
     acc:0.999627
 encoder:
     loss:0.236262
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.483789
     acc:0.852544
 tv:
     loss:19110.156836
     acc:0.488877
 lu:
     loss:13406.760059
     acc:0.915484
 lv:
     loss:16753.643750
     acc:0.188436
 le:
     loss:1448.614886
     acc:0.999162
 encoder:
     loss:0.295320
----------------------------


Epoch: [782/1000]:
train:
----------------------------
 tu:
     loss:18285.342524
     acc:0.854540
 tv:
     loss:19814.555721
     acc:0.539437
 lu:
     loss:14031.572152
     acc:0.923979
 lv:
     loss:17032.046296
     acc:0.304239
 le:
     loss:1517.819726
     acc:0.999651
 encoder:
     loss:0.239694
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.614258
     acc:0.852313
 tv:
     loss:19104.288672
     acc:0.490096
 lu:
     loss:13409.330176
     acc:0.914667
 lv:
     loss:16753.105859
     acc:0.188378
 le:
     loss:1448.927802
     acc:0.999042
 encoder:
     loss:0.210208
----------------------------


Epoch: [783/1000]:
train:
----------------------------
 tu:
     loss:18285.511230
     acc:0.854453
 tv:
     loss:19812.646905
     acc:0.539811
 lu:
     loss:14031.008017
     acc:0.924168
 lv:
     loss:17030.302326
     acc:0.304670
 le:
     loss:1517.858126
     acc:0.999641
 encoder:
     loss:0.193169
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.731543
     acc:0.852657
 tv:
     loss:19104.791309
     acc:0.489895
 lu:
     loss:13407.062305
     acc:0.915391
 lv:
     loss:16754.287891
     acc:0.188518
 le:
     loss:1448.580151
     acc:0.999128
 encoder:
     loss:0.187606
----------------------------


Epoch: [784/1000]:
train:
----------------------------
 tu:
     loss:18286.519054
     acc:0.854146
 tv:
     loss:19814.702046
     acc:0.539296
 lu:
     loss:14031.594920
     acc:0.924105
 lv:
     loss:17029.985942
     acc:0.305055
 le:
     loss:1517.848809
     acc:0.999639
 encoder:
     loss:0.226037
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.010840
     acc:0.852350
 tv:
     loss:19108.176270
     acc:0.489255
 lu:
     loss:13407.695752
     acc:0.915065
 lv:
     loss:16754.393848
     acc:0.188554
 le:
     loss:1448.957050
     acc:0.999044
 encoder:
     loss:0.235503
----------------------------


Epoch: [785/1000]:
train:
----------------------------
 tu:
     loss:18286.313579
     acc:0.854285
 tv:
     loss:19812.163994
     acc:0.539739
 lu:
     loss:14029.901413
     acc:0.924461
 lv:
     loss:17025.476960
     acc:0.305801
 le:
     loss:1517.822642
     acc:0.999657
 encoder:
     loss:0.261954
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.625195
     acc:0.852469
 tv:
     loss:19106.413672
     acc:0.489349
 lu:
     loss:13406.789551
     acc:0.915393
 lv:
     loss:16751.198340
     acc:0.188705
 le:
     loss:1448.972638
     acc:0.999042
 encoder:
     loss:0.480294
----------------------------


Epoch: [786/1000]:
train:
----------------------------
 tu:
     loss:18286.241438
     acc:0.854322
 tv:
     loss:19811.753509
     acc:0.539866
 lu:
     loss:14030.520905
     acc:0.924455
 lv:
     loss:17028.240200
     acc:0.305159
 le:
     loss:1517.869589
     acc:0.999642
 encoder:
     loss:0.406630
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.430371
     acc:0.852456
 tv:
     loss:19108.428320
     acc:0.489190
 lu:
     loss:13407.509180
     acc:0.915464
 lv:
     loss:16764.103418
     acc:0.185480
 le:
     loss:1449.100482
     acc:0.999004
 encoder:
     loss:0.265946
----------------------------


Epoch: [787/1000]:
train:
----------------------------
 tu:
     loss:18285.106025
     acc:0.854503
 tv:
     loss:19815.461562
     acc:0.539321
 lu:
     loss:14030.153036
     acc:0.924462
 lv:
     loss:17027.648494
     acc:0.305491
 le:
     loss:1518.033819
     acc:0.999606
 encoder:
     loss:0.259871
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.269434
     acc:0.852411
 tv:
     loss:19103.627051
     acc:0.490306
 lu:
     loss:13406.992383
     acc:0.915462
 lv:
     loss:16747.185156
     acc:0.189957
 le:
     loss:1448.481073
     acc:0.999169
 encoder:
     loss:0.210115
----------------------------


Epoch: [788/1000]:
train:
----------------------------
 tu:
     loss:18284.734375
     acc:0.854707
 tv:
     loss:19812.506473
     acc:0.539541
 lu:
     loss:14030.951070
     acc:0.924271
 lv:
     loss:17026.128884
     acc:0.305492
 le:
     loss:1517.797174
     acc:0.999657
 encoder:
     loss:0.217981
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.484375
     acc:0.852511
 tv:
     loss:19110.330176
     acc:0.488908
 lu:
     loss:13406.516357
     acc:0.915597
 lv:
     loss:16754.586230
     acc:0.188198
 le:
     loss:1448.978278
     acc:0.999063
 encoder:
     loss:0.185664
----------------------------


Epoch: [789/1000]:
train:
----------------------------
 tu:
     loss:18285.163915
     acc:0.854522
 tv:
     loss:19812.673476
     acc:0.539733
 lu:
     loss:14031.767510
     acc:0.924003
 lv:
     loss:17028.944540
     acc:0.305328
 le:
     loss:1517.947758
     acc:0.999625
 encoder:
     loss:0.402000
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.867676
     acc:0.852597
 tv:
     loss:19103.525586
     acc:0.490716
 lu:
     loss:13407.265039
     acc:0.915531
 lv:
     loss:16751.570117
     acc:0.188815
 le:
     loss:1449.019690
     acc:0.999044
 encoder:
     loss:0.576521
----------------------------


Epoch: [790/1000]:
train:
----------------------------
 tu:
     loss:18285.824571
     acc:0.854479
 tv:
     loss:19811.877748
     acc:0.539972
 lu:
     loss:14031.873251
     acc:0.924036
 lv:
     loss:17029.563567
     acc:0.304930
 le:
     loss:1518.037277
     acc:0.999612
 encoder:
     loss:0.509199
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.398242
     acc:0.852570
 tv:
     loss:19107.061914
     acc:0.489701
 lu:
     loss:13405.660400
     acc:0.915739
 lv:
     loss:16754.496973
     acc:0.188135
 le:
     loss:1448.839362
     acc:0.999084
 encoder:
     loss:0.275846
----------------------------


Epoch: [791/1000]:
train:
----------------------------
 tu:
     loss:18286.531341
     acc:0.854273
 tv:
     loss:19813.428995
     acc:0.539705
 lu:
     loss:14030.773381
     acc:0.924216
 lv:
     loss:17028.050645
     acc:0.305466
 le:
     loss:1518.072378
     acc:0.999596
 encoder:
     loss:0.368222
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.893066
     acc:0.852334
 tv:
     loss:19108.401660
     acc:0.489222
 lu:
     loss:13405.697461
     acc:0.915738
 lv:
     loss:16759.093457
     acc:0.187304
 le:
     loss:1448.696515
     acc:0.999107
 encoder:
     loss:0.357129
----------------------------


Epoch: [792/1000]:
train:
----------------------------
 tu:
     loss:18286.981343
     acc:0.854139
 tv:
     loss:19812.469216
     acc:0.539846
 lu:
     loss:14031.229174
     acc:0.924173
 lv:
     loss:17031.153581
     acc:0.304753
 le:
     loss:1517.981968
     acc:0.999621
 encoder:
     loss:0.365375
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.293945
     acc:0.852555
 tv:
     loss:19101.994531
     acc:0.490475
 lu:
     loss:13408.179932
     acc:0.915124
 lv:
     loss:16754.194141
     acc:0.188266
 le:
     loss:1448.980316
     acc:0.999065
 encoder:
     loss:0.298891
----------------------------


Epoch: [793/1000]:
train:
----------------------------
 tu:
     loss:18285.129894
     acc:0.854609
 tv:
     loss:19812.057481
     acc:0.539741
 lu:
     loss:14030.415209
     acc:0.924290
 lv:
     loss:17029.191429
     acc:0.305097
 le:
     loss:1517.907405
     acc:0.999635
 encoder:
     loss:0.270887
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.183301
     acc:0.852495
 tv:
     loss:19106.034473
     acc:0.489729
 lu:
     loss:13406.530957
     acc:0.915630
 lv:
     loss:16752.037891
     acc:0.188415
 le:
     loss:1448.634375
     acc:0.999126
 encoder:
     loss:0.211565
----------------------------


Epoch: [794/1000]:
train:
----------------------------
 tu:
     loss:18285.990291
     acc:0.854369
 tv:
     loss:19810.383153
     acc:0.540350
 lu:
     loss:14030.192065
     acc:0.924489
 lv:
     loss:17021.847077
     acc:0.306863
 le:
     loss:1517.876621
     acc:0.999650
 encoder:
     loss:0.222129
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.166504
     acc:0.852355
 tv:
     loss:19103.531445
     acc:0.490470
 lu:
     loss:13405.745947
     acc:0.915652
 lv:
     loss:16749.876367
     acc:0.189407
 le:
     loss:1448.967090
     acc:0.999062
 encoder:
     loss:0.215836
----------------------------


Epoch: [795/1000]:
train:
----------------------------
 tu:
     loss:18285.830862
     acc:0.854457
 tv:
     loss:19811.838958
     acc:0.540116
 lu:
     loss:14030.897949
     acc:0.924269
 lv:
     loss:17033.036576
     acc:0.304458
 le:
     loss:1517.810859
     acc:0.999659
 encoder:
     loss:0.224307
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.402539
     acc:0.852454
 tv:
     loss:19105.414453
     acc:0.489746
 lu:
     loss:13406.004639
     acc:0.915704
 lv:
     loss:16754.298730
     acc:0.188010
 le:
     loss:1448.530078
     acc:0.999166
 encoder:
     loss:0.255210
----------------------------


Epoch: [796/1000]:
train:
----------------------------
 tu:
     loss:18285.048374
     acc:0.854501
 tv:
     loss:19812.252328
     acc:0.539693
 lu:
     loss:14031.114712
     acc:0.924158
 lv:
     loss:17029.255791
     acc:0.304876
 le:
     loss:1517.892043
     acc:0.999636
 encoder:
     loss:0.242442
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.731445
     acc:0.852528
 tv:
     loss:19106.692676
     acc:0.489630
 lu:
     loss:13406.578662
     acc:0.915322
 lv:
     loss:16753.950098
     acc:0.188958
 le:
     loss:1448.691986
     acc:0.999124
 encoder:
     loss:0.228233
----------------------------


Epoch: [797/1000]:
train:
----------------------------
 tu:
     loss:18284.975881
     acc:0.854533
 tv:
     loss:19812.090718
     acc:0.539998
 lu:
     loss:14029.957088
     acc:0.924531
 lv:
     loss:17023.390716
     acc:0.306626
 le:
     loss:1517.853063
     acc:0.999651
 encoder:
     loss:0.277870
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.048438
     acc:0.852346
 tv:
     loss:19106.996191
     acc:0.489331
 lu:
     loss:13406.318555
     acc:0.915634
 lv:
     loss:16752.130566
     acc:0.188477
 le:
     loss:1448.829309
     acc:0.999083
 encoder:
     loss:0.290537
----------------------------


Epoch: [798/1000]:
train:
----------------------------
 tu:
     loss:18284.928983
     acc:0.854529
 tv:
     loss:19809.741290
     acc:0.540410
 lu:
     loss:14029.332168
     acc:0.924461
 lv:
     loss:17027.340979
     acc:0.305394
 le:
     loss:1517.892550
     acc:0.999635
 encoder:
     loss:0.276741
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.513574
     acc:0.852474
 tv:
     loss:19108.731836
     acc:0.489193
 lu:
     loss:13408.855811
     acc:0.914928
 lv:
     loss:16760.174902
     acc:0.186876
 le:
     loss:1448.824542
     acc:0.999065
 encoder:
     loss:0.251777
----------------------------


Epoch: [799/1000]:
train:
----------------------------
 tu:
     loss:18285.451410
     acc:0.854448
 tv:
     loss:19810.355060
     acc:0.540439
 lu:
     loss:14029.265795
     acc:0.924592
 lv:
     loss:17021.964276
     acc:0.306840
 le:
     loss:1517.877810
     acc:0.999642
 encoder:
     loss:0.213705
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.497266
     acc:0.852576
 tv:
     loss:19107.489746
     acc:0.489570
 lu:
     loss:13406.717041
     acc:0.915464
 lv:
     loss:16756.747266
     acc:0.187607
 le:
     loss:1448.583020
     acc:0.999125
 encoder:
     loss:0.207855
----------------------------


Epoch: [800/1000]:
train:
----------------------------
 tu:
     loss:18284.999273
     acc:0.854492
 tv:
     loss:19810.506722
     acc:0.540303
 lu:
     loss:14029.610783
     acc:0.924550
 lv:
     loss:17020.927189
     acc:0.306997
 le:
     loss:1517.810240
     acc:0.999655
 encoder:
     loss:0.220390
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.436035
     acc:0.852533
 tv:
     loss:19106.132812
     acc:0.489732
 lu:
     loss:13406.058154
     acc:0.915588
 lv:
     loss:16753.088965
     acc:0.188644
 le:
     loss:1448.828723
     acc:0.999085
 encoder:
     loss:0.223551
----------------------------


Epoch: [801/1000]:
train:
----------------------------
 tu:
     loss:18285.056220
     acc:0.854504
 tv:
     loss:19811.219182
     acc:0.539973
 lu:
     loss:14029.606457
     acc:0.924646
 lv:
     loss:17021.412950
     acc:0.306793
 le:
     loss:1517.878435
     acc:0.999642
 encoder:
     loss:0.204773
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.528809
     acc:0.852406
 tv:
     loss:19107.095410
     acc:0.489456
 lu:
     loss:13406.956641
     acc:0.915480
 lv:
     loss:16749.024316
     acc:0.189593
 le:
     loss:1448.755969
     acc:0.999086
 encoder:
     loss:0.180336
----------------------------


Epoch: [802/1000]:
train:
----------------------------
 tu:
     loss:18285.638604
     acc:0.854392
 tv:
     loss:19810.472270
     acc:0.540501
 lu:
     loss:14029.761923
     acc:0.924501
 lv:
     loss:17020.981309
     acc:0.307111
 le:
     loss:1517.773489
     acc:0.999664
 encoder:
     loss:0.189671
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.561328
     acc:0.852434
 tv:
     loss:19109.987500
     acc:0.488922
 lu:
     loss:13406.321143
     acc:0.915606
 lv:
     loss:16752.462500
     acc:0.188221
 le:
     loss:1448.878333
     acc:0.999062
 encoder:
     loss:0.198714
----------------------------


Epoch: [803/1000]:
train:
----------------------------
 tu:
     loss:18284.978516
     acc:0.854595
 tv:
     loss:19811.678870
     acc:0.539836
 lu:
     loss:14030.720442
     acc:0.924203
 lv:
     loss:17025.206747
     acc:0.305946
 le:
     loss:1517.805051
     acc:0.999658
 encoder:
     loss:0.199160
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.532813
     acc:0.852354
 tv:
     loss:19111.543164
     acc:0.488607
 lu:
     loss:13408.911914
     acc:0.915003
 lv:
     loss:16759.638770
     acc:0.187535
 le:
     loss:1449.034637
     acc:0.999042
 encoder:
     loss:0.242797
----------------------------


Epoch: [804/1000]:
train:
----------------------------
 tu:
     loss:18284.926792
     acc:0.854550
 tv:
     loss:19809.756541
     acc:0.540460
 lu:
     loss:14028.518430
     acc:0.924756
 lv:
     loss:17022.704181
     acc:0.306337
 le:
     loss:1517.720600
     acc:0.999683
 encoder:
     loss:0.414278
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.199219
     acc:0.852557
 tv:
     loss:19104.924805
     acc:0.490077
 lu:
     loss:13406.739502
     acc:0.915407
 lv:
     loss:16755.389258
     acc:0.187591
 le:
     loss:1448.522791
     acc:0.999165
 encoder:
     loss:0.287499
----------------------------


Epoch: [805/1000]:
train:
----------------------------
 tu:
     loss:18285.947266
     acc:0.854353
 tv:
     loss:19809.197061
     acc:0.540578
 lu:
     loss:14029.299453
     acc:0.924448
 lv:
     loss:17021.288597
     acc:0.306604
 le:
     loss:1517.922968
     acc:0.999638
 encoder:
     loss:0.269587
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.737109
     acc:0.852619
 tv:
     loss:19108.333398
     acc:0.489288
 lu:
     loss:13406.640039
     acc:0.915513
 lv:
     loss:16758.228027
     acc:0.187470
 le:
     loss:1448.696338
     acc:0.999083
 encoder:
     loss:0.329059
----------------------------


Epoch: [806/1000]:
train:
----------------------------
 tu:
     loss:18285.262343
     acc:0.854505
 tv:
     loss:19810.480105
     acc:0.540377
 lu:
     loss:14029.768430
     acc:0.924459
 lv:
     loss:17022.837277
     acc:0.306472
 le:
     loss:1517.903637
     acc:0.999636
 encoder:
     loss:0.328869
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.087402
     acc:0.852337
 tv:
     loss:19105.365039
     acc:0.489986
 lu:
     loss:13409.280273
     acc:0.914890
 lv:
     loss:16758.437695
     acc:0.186832
 le:
     loss:1448.935455
     acc:0.999081
 encoder:
     loss:0.468353
----------------------------


Epoch: [807/1000]:
train:
----------------------------
 tu:
     loss:18285.143634
     acc:0.854502
 tv:
     loss:19808.791527
     acc:0.540583
 lu:
     loss:14030.219727
     acc:0.924422
 lv:
     loss:17018.264251
     acc:0.307807
 le:
     loss:1517.806636
     acc:0.999661
 encoder:
     loss:0.509456
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.106641
     acc:0.852474
 tv:
     loss:19105.257422
     acc:0.490028
 lu:
     loss:13406.851318
     acc:0.915470
 lv:
     loss:16759.517871
     acc:0.187394
 le:
     loss:1448.898718
     acc:0.999081
 encoder:
     loss:0.479584
----------------------------


Epoch: [808/1000]:
train:
----------------------------
 tu:
     loss:18285.203034
     acc:0.854493
 tv:
     loss:19810.282772
     acc:0.540294
 lu:
     loss:14030.444007
     acc:0.924405
 lv:
     loss:17020.765352
     acc:0.306996
 le:
     loss:1517.950285
     acc:0.999619
 encoder:
     loss:0.508758
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.276953
     acc:0.852495
 tv:
     loss:19108.978906
     acc:0.489069
 lu:
     loss:13408.723193
     acc:0.915030
 lv:
     loss:16753.659668
     acc:0.188652
 le:
     loss:1448.773578
     acc:0.999103
 encoder:
     loss:0.392561
----------------------------


Epoch: [809/1000]:
train:
----------------------------
 tu:
     loss:18285.328931
     acc:0.854404
 tv:
     loss:19811.396621
     acc:0.540177
 lu:
     loss:14029.144009
     acc:0.924555
 lv:
     loss:17020.595635
     acc:0.306905
 le:
     loss:1517.922847
     acc:0.999634
 encoder:
     loss:0.283388
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.041797
     acc:0.852333
 tv:
     loss:19102.079102
     acc:0.490732
 lu:
     loss:13406.962842
     acc:0.915561
 lv:
     loss:16755.902148
     acc:0.188203
 le:
     loss:1448.749567
     acc:0.999084
 encoder:
     loss:0.258293
----------------------------


Epoch: [810/1000]:
train:
----------------------------
 tu:
     loss:18285.135140
     acc:0.854471
 tv:
     loss:19810.667310
     acc:0.540182
 lu:
     loss:14029.667004
     acc:0.924493
 lv:
     loss:17018.077512
     acc:0.307882
 le:
     loss:1517.764715
     acc:0.999669
 encoder:
     loss:0.307016
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.119727
     acc:0.852355
 tv:
     loss:19105.156445
     acc:0.490039
 lu:
     loss:13407.279688
     acc:0.915484
 lv:
     loss:16755.116992
     acc:0.187754
 le:
     loss:1448.590527
     acc:0.999108
 encoder:
     loss:0.261462
----------------------------


Epoch: [811/1000]:
train:
----------------------------
 tu:
     loss:18285.669956
     acc:0.854465
 tv:
     loss:19810.393010
     acc:0.540266
 lu:
     loss:14030.326819
     acc:0.924372
 lv:
     loss:17018.723474
     acc:0.307389
 le:
     loss:1517.839547
     acc:0.999642
 encoder:
     loss:0.246291
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.778223
     acc:0.852397
 tv:
     loss:19111.896680
     acc:0.488598
 lu:
     loss:13407.526709
     acc:0.915300
 lv:
     loss:16744.113477
     acc:0.190803
 le:
     loss:1448.659729
     acc:0.999105
 encoder:
     loss:0.186630
----------------------------


Epoch: [812/1000]:
train:
----------------------------
 tu:
     loss:18284.862850
     acc:0.854511
 tv:
     loss:19810.636344
     acc:0.540147
 lu:
     loss:14031.577671
     acc:0.924088
 lv:
     loss:17020.099757
     acc:0.307102
 le:
     loss:1517.932908
     acc:0.999631
 encoder:
     loss:0.210420
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.392480
     acc:0.852454
 tv:
     loss:19107.521094
     acc:0.489384
 lu:
     loss:13408.370703
     acc:0.915349
 lv:
     loss:16756.017285
     acc:0.188158
 le:
     loss:1448.747522
     acc:0.999088
 encoder:
     loss:0.209511
----------------------------


Epoch: [813/1000]:
train:
----------------------------
 tu:
     loss:18285.215003
     acc:0.854459
 tv:
     loss:19808.714594
     acc:0.540511
 lu:
     loss:14028.959098
     acc:0.924589
 lv:
     loss:17017.563215
     acc:0.307360
 le:
     loss:1517.780603
     acc:0.999662
 encoder:
     loss:0.196765
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.035547
     acc:0.852275
 tv:
     loss:19100.715527
     acc:0.490834
 lu:
     loss:13407.104102
     acc:0.915528
 lv:
     loss:16753.958008
     acc:0.188909
 le:
     loss:1448.542590
     acc:0.999147
 encoder:
     loss:0.186338
----------------------------


Epoch: [814/1000]:
train:
----------------------------
 tu:
     loss:18285.784452
     acc:0.854346
 tv:
     loss:19808.695403
     acc:0.540549
 lu:
     loss:14028.510822
     acc:0.924632
 lv:
     loss:17019.474882
     acc:0.306939
 le:
     loss:1517.895580
     acc:0.999635
 encoder:
     loss:0.193242
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.608984
     acc:0.852554
 tv:
     loss:19108.114355
     acc:0.489306
 lu:
     loss:13409.134961
     acc:0.915082
 lv:
     loss:16748.114063
     acc:0.189614
 le:
     loss:1448.520801
     acc:0.999165
 encoder:
     loss:0.191504
----------------------------


Epoch: [815/1000]:
train:
----------------------------
 tu:
     loss:18284.675225
     acc:0.854595
 tv:
     loss:19809.158101
     acc:0.540637
 lu:
     loss:14030.602528
     acc:0.924167
 lv:
     loss:17019.592944
     acc:0.307329
 le:
     loss:1517.885834
     acc:0.999631
 encoder:
     loss:0.188763
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.617676
     acc:0.852454
 tv:
     loss:19107.244824
     acc:0.489148
 lu:
     loss:13407.982715
     acc:0.915317
 lv:
     loss:16755.581152
     acc:0.187476
 le:
     loss:1449.154242
     acc:0.999020
 encoder:
     loss:0.167034
----------------------------


Epoch: [816/1000]:
train:
----------------------------
 tu:
     loss:18284.887684
     acc:0.854535
 tv:
     loss:19809.194722
     acc:0.540645
 lu:
     loss:14029.353152
     acc:0.924559
 lv:
     loss:17019.042185
     acc:0.307180
 le:
     loss:1517.934745
     acc:0.999624
 encoder:
     loss:0.200567
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.079199
     acc:0.852514
 tv:
     loss:19107.123340
     acc:0.489480
 lu:
     loss:13406.397852
     acc:0.915850
 lv:
     loss:16746.255762
     acc:0.190385
 le:
     loss:1448.776312
     acc:0.999085
 encoder:
     loss:0.234389
----------------------------


Epoch: [817/1000]:
train:
----------------------------
 tu:
     loss:18284.875931
     acc:0.854531
 tv:
     loss:19808.348565
     acc:0.540809
 lu:
     loss:14029.255655
     acc:0.924403
 lv:
     loss:17014.941860
     acc:0.308091
 le:
     loss:1517.723827
     acc:0.999670
 encoder:
     loss:0.241603
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.096191
     acc:0.852438
 tv:
     loss:19111.163965
     acc:0.488521
 lu:
     loss:13407.490723
     acc:0.915464
 lv:
     loss:16755.371094
     acc:0.188086
 le:
     loss:1448.531708
     acc:0.999144
 encoder:
     loss:0.222933
----------------------------


Epoch: [818/1000]:
train:
----------------------------
 tu:
     loss:18284.903968
     acc:0.854562
 tv:
     loss:19809.648824
     acc:0.540444
 lu:
     loss:14029.322890
     acc:0.924546
 lv:
     loss:17020.527117
     acc:0.306944
 le:
     loss:1517.918620
     acc:0.999627
 encoder:
     loss:0.197658
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.939063
     acc:0.852595
 tv:
     loss:19106.625684
     acc:0.489605
 lu:
     loss:13409.418262
     acc:0.914897
 lv:
     loss:16759.722070
     acc:0.187444
 le:
     loss:1448.647369
     acc:0.999107
 encoder:
     loss:0.180636
----------------------------


Epoch: [819/1000]:
train:
----------------------------
 tu:
     loss:18284.417321
     acc:0.854566
 tv:
     loss:19807.138910
     acc:0.541093
 lu:
     loss:14029.026095
     acc:0.924707
 lv:
     loss:17016.304086
     acc:0.307926
 le:
     loss:1517.861899
     acc:0.999635
 encoder:
     loss:0.184815
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.263770
     acc:0.852480
 tv:
     loss:19108.299609
     acc:0.489318
 lu:
     loss:13409.377930
     acc:0.914806
 lv:
     loss:16759.469824
     acc:0.187331
 le:
     loss:1448.698645
     acc:0.999085
 encoder:
     loss:0.167327
----------------------------


Epoch: [820/1000]:
train:
----------------------------
 tu:
     loss:18285.330532
     acc:0.854411
 tv:
     loss:19811.245878
     acc:0.540249
 lu:
     loss:14030.614371
     acc:0.924308
 lv:
     loss:17017.725756
     acc:0.307666
 le:
     loss:1517.808397
     acc:0.999649
 encoder:
     loss:0.179960
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.576758
     acc:0.852516
 tv:
     loss:19113.549609
     acc:0.488441
 lu:
     loss:13409.247998
     acc:0.914883
 lv:
     loss:16750.054883
     acc:0.189752
 le:
     loss:1448.565460
     acc:0.999148
 encoder:
     loss:0.188053
----------------------------


Epoch: [821/1000]:
train:
----------------------------
 tu:
     loss:18285.368459
     acc:0.854460
 tv:
     loss:19809.665459
     acc:0.540560
 lu:
     loss:14028.260856
     acc:0.924911
 lv:
     loss:17017.557742
     acc:0.307763
 le:
     loss:1517.844390
     acc:0.999649
 encoder:
     loss:0.238001
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.666504
     acc:0.852475
 tv:
     loss:19107.165430
     acc:0.489654
 lu:
     loss:13407.293115
     acc:0.915471
 lv:
     loss:16749.576074
     acc:0.189704
 le:
     loss:1448.889258
     acc:0.999086
 encoder:
     loss:0.208685
----------------------------


Epoch: [822/1000]:
train:
----------------------------
 tu:
     loss:18285.194120
     acc:0.854448
 tv:
     loss:19810.303711
     acc:0.540197
 lu:
     loss:14029.628804
     acc:0.924452
 lv:
     loss:17019.906261
     acc:0.307065
 le:
     loss:1517.823830
     acc:0.999653
 encoder:
     loss:0.197401
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.226953
     acc:0.852620
 tv:
     loss:19106.800391
     acc:0.489781
 lu:
     loss:13406.622168
     acc:0.915608
 lv:
     loss:16754.533691
     acc:0.188070
 le:
     loss:1448.474780
     acc:0.999185
 encoder:
     loss:0.171745
----------------------------


Epoch: [823/1000]:
train:
----------------------------
 tu:
     loss:18285.928700
     acc:0.854357
 tv:
     loss:19811.030614
     acc:0.540247
 lu:
     loss:14029.547057
     acc:0.924489
 lv:
     loss:17019.344375
     acc:0.307092
 le:
     loss:1518.011932
     acc:0.999608
 encoder:
     loss:0.180838
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.280957
     acc:0.852459
 tv:
     loss:19111.432812
     acc:0.488477
 lu:
     loss:13408.461084
     acc:0.915005
 lv:
     loss:16761.166504
     acc:0.186330
 le:
     loss:1448.440594
     acc:0.999187
 encoder:
     loss:0.166517
----------------------------


Epoch: [824/1000]:
train:
----------------------------
 tu:
     loss:18285.834143
     acc:0.854317
 tv:
     loss:19809.809207
     acc:0.540490
 lu:
     loss:14028.161587
     acc:0.924858
 lv:
     loss:17019.912745
     acc:0.307274
 le:
     loss:1517.998121
     acc:0.999615
 encoder:
     loss:0.195343
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.423340
     acc:0.852569
 tv:
     loss:19110.523730
     acc:0.488912
 lu:
     loss:13409.100098
     acc:0.914864
 lv:
     loss:16754.760547
     acc:0.188413
 le:
     loss:1448.401044
     acc:0.999187
 encoder:
     loss:0.185359
----------------------------


Epoch: [825/1000]:
train:
----------------------------
 tu:
     loss:18286.362600
     acc:0.854226
 tv:
     loss:19811.273222
     acc:0.540041
 lu:
     loss:14030.147404
     acc:0.924446
 lv:
     loss:17018.830600
     acc:0.307224
 le:
     loss:1517.902843
     acc:0.999638
 encoder:
     loss:0.201745
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.346289
     acc:0.852469
 tv:
     loss:19104.622266
     acc:0.490382
 lu:
     loss:13409.908789
     acc:0.914618
 lv:
     loss:16747.618066
     acc:0.189308
 le:
     loss:1448.698444
     acc:0.999085
 encoder:
     loss:0.182912
----------------------------


Epoch: [826/1000]:
train:
----------------------------
 tu:
     loss:18283.537745
     acc:0.854954
 tv:
     loss:19809.012400
     acc:0.540432
 lu:
     loss:14028.730457
     acc:0.924507
 lv:
     loss:17021.061047
     acc:0.306672
 le:
     loss:1517.823894
     acc:0.999658
 encoder:
     loss:0.188580
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.648535
     acc:0.852495
 tv:
     loss:19109.614258
     acc:0.488844
 lu:
     loss:13410.736670
     acc:0.914729
 lv:
     loss:16753.293945
     acc:0.189486
 le:
     loss:1448.679871
     acc:0.999086
 encoder:
     loss:0.159885
----------------------------


Epoch: [827/1000]:
train:
----------------------------
 tu:
     loss:18285.161678
     acc:0.854566
 tv:
     loss:19807.711778
     acc:0.540787
 lu:
     loss:14029.967194
     acc:0.924371
 lv:
     loss:17022.013013
     acc:0.306396
 le:
     loss:1517.872625
     acc:0.999652
 encoder:
     loss:0.189619
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.875195
     acc:0.852683
 tv:
     loss:19110.565918
     acc:0.488933
 lu:
     loss:13406.531445
     acc:0.915571
 lv:
     loss:16751.211133
     acc:0.189097
 le:
     loss:1448.612238
     acc:0.999125
 encoder:
     loss:0.332823
----------------------------


Epoch: [828/1000]:
train:
----------------------------
 tu:
     loss:18284.673317
     acc:0.854668
 tv:
     loss:19809.462266
     acc:0.540401
 lu:
     loss:14028.940850
     acc:0.924571
 lv:
     loss:17017.414403
     acc:0.307424
 le:
     loss:1517.942599
     acc:0.999630
 encoder:
     loss:0.271521
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.825391
     acc:0.852566
 tv:
     loss:19104.738672
     acc:0.490268
 lu:
     loss:13410.043164
     acc:0.914735
 lv:
     loss:16752.221191
     acc:0.188627
 le:
     loss:1448.572125
     acc:0.999127
 encoder:
     loss:0.353857
----------------------------


Epoch: [829/1000]:
train:
----------------------------
 tu:
     loss:18285.134391
     acc:0.854526
 tv:
     loss:19809.751885
     acc:0.540332
 lu:
     loss:14028.104197
     acc:0.924718
 lv:
     loss:17017.731616
     acc:0.307479
 le:
     loss:1517.987533
     acc:0.999610
 encoder:
     loss:0.305446
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.501367
     acc:0.852602
 tv:
     loss:19107.854004
     acc:0.489476
 lu:
     loss:13408.849414
     acc:0.914937
 lv:
     loss:16754.770703
     acc:0.188445
 le:
     loss:1448.664740
     acc:0.999126
 encoder:
     loss:0.207415
----------------------------


Epoch: [830/1000]:
train:
----------------------------
 tu:
     loss:18285.280114
     acc:0.854502
 tv:
     loss:19810.495492
     acc:0.539967
 lu:
     loss:14029.583598
     acc:0.924440
 lv:
     loss:17018.566974
     acc:0.307171
 le:
     loss:1517.844860
     acc:0.999654
 encoder:
     loss:0.663296
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.617773
     acc:0.852453
 tv:
     loss:19110.966797
     acc:0.488510
 lu:
     loss:13409.358594
     acc:0.914802
 lv:
     loss:16755.197168
     acc:0.188195
 le:
     loss:1448.844824
     acc:0.999101
 encoder:
     loss:0.632095
----------------------------


Epoch: [831/1000]:
train:
----------------------------
 tu:
     loss:18285.083439
     acc:0.854454
 tv:
     loss:19809.417991
     acc:0.540379
 lu:
     loss:14029.196698
     acc:0.924622
 lv:
     loss:17018.056516
     acc:0.307638
 le:
     loss:1517.891998
     acc:0.999634
 encoder:
     loss:0.436527
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.235938
     acc:0.852545
 tv:
     loss:19107.508984
     acc:0.489818
 lu:
     loss:13407.927832
     acc:0.915155
 lv:
     loss:16746.383105
     acc:0.190684
 le:
     loss:1448.551404
     acc:0.999147
 encoder:
     loss:0.284087
----------------------------


Epoch: [832/1000]:
train:
----------------------------
 tu:
     loss:18285.011832
     acc:0.854512
 tv:
     loss:19809.928109
     acc:0.540549
 lu:
     loss:14028.683810
     acc:0.924722
 lv:
     loss:17019.011685
     acc:0.307711
 le:
     loss:1517.936394
     acc:0.999633
 encoder:
     loss:0.265961
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.363184
     acc:0.852472
 tv:
     loss:19109.599023
     acc:0.488794
 lu:
     loss:13406.547119
     acc:0.915488
 lv:
     loss:16759.306152
     acc:0.187632
 le:
     loss:1448.740405
     acc:0.999086
 encoder:
     loss:0.255994
----------------------------


Epoch: [833/1000]:
train:
----------------------------
 tu:
     loss:18284.677098
     acc:0.854698
 tv:
     loss:19809.255144
     acc:0.540440
 lu:
     loss:14029.300668
     acc:0.924638
 lv:
     loss:17018.048726
     acc:0.307169
 le:
     loss:1517.914551
     acc:0.999640
 encoder:
     loss:0.227323
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.183691
     acc:0.852557
 tv:
     loss:19108.216016
     acc:0.489342
 lu:
     loss:13409.803320
     acc:0.914600
 lv:
     loss:16756.884961
     acc:0.187786
 le:
     loss:1448.476825
     acc:0.999148
 encoder:
     loss:0.195703
----------------------------


Epoch: [834/1000]:
train:
----------------------------
 tu:
     loss:18285.225631
     acc:0.854555
 tv:
     loss:19809.486555
     acc:0.540400
 lu:
     loss:14029.653286
     acc:0.924378
 lv:
     loss:17023.577092
     acc:0.306112
 le:
     loss:1517.902253
     acc:0.999641
 encoder:
     loss:0.589002
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.305664
     acc:0.852514
 tv:
     loss:19106.754883
     acc:0.489749
 lu:
     loss:13410.350586
     acc:0.914698
 lv:
     loss:16751.521191
     acc:0.189866
 le:
     loss:1448.431018
     acc:0.999187
 encoder:
     loss:0.686651
----------------------------


Epoch: [835/1000]:
train:
----------------------------
 tu:
     loss:18285.545864
     acc:0.854380
 tv:
     loss:19810.639183
     acc:0.540272
 lu:
     loss:14029.611078
     acc:0.924483
 lv:
     loss:17016.791368
     acc:0.307714
 le:
     loss:1518.007915
     acc:0.999606
 encoder:
     loss:0.465853
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.049805
     acc:0.852360
 tv:
     loss:19107.973242
     acc:0.489208
 lu:
     loss:13406.478955
     acc:0.915709
 lv:
     loss:16752.293945
     acc:0.188748
 le:
     loss:1448.488403
     acc:0.999166
 encoder:
     loss:0.316960
----------------------------


Epoch: [836/1000]:
train:
----------------------------
 tu:
     loss:18284.849791
     acc:0.854583
 tv:
     loss:19811.153332
     acc:0.539991
 lu:
     loss:14030.073753
     acc:0.924442
 lv:
     loss:17017.760799
     acc:0.307504
 le:
     loss:1517.977954
     acc:0.999617
 encoder:
     loss:0.282038
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.518066
     acc:0.852536
 tv:
     loss:19106.841602
     acc:0.489686
 lu:
     loss:13407.414014
     acc:0.915245
 lv:
     loss:16761.193457
     acc:0.186467
 le:
     loss:1448.524066
     acc:0.999166
 encoder:
     loss:0.240968
----------------------------


Epoch: [837/1000]:
train:
----------------------------
 tu:
     loss:18285.123433
     acc:0.854568
 tv:
     loss:19809.407567
     acc:0.540572
 lu:
     loss:14028.480662
     acc:0.924416
 lv:
     loss:17019.147733
     acc:0.306822
 le:
     loss:1517.953158
     acc:0.999623
 encoder:
     loss:0.214443
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.884375
     acc:0.852620
 tv:
     loss:19108.181055
     acc:0.489209
 lu:
     loss:13407.442773
     acc:0.915137
 lv:
     loss:16757.653418
     acc:0.186560
 le:
     loss:1448.900745
     acc:0.999082
 encoder:
     loss:0.181122
----------------------------


Epoch: [838/1000]:
train:
----------------------------
 tu:
     loss:18285.445789
     acc:0.854483
 tv:
     loss:19810.288063
     acc:0.540120
 lu:
     loss:14029.469295
     acc:0.924542
 lv:
     loss:17015.866620
     acc:0.307703
 le:
     loss:1517.842204
     acc:0.999646
 encoder:
     loss:0.205322
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.018848
     acc:0.852518
 tv:
     loss:19109.126367
     acc:0.489557
 lu:
     loss:13408.241895
     acc:0.915071
 lv:
     loss:16757.069922
     acc:0.187864
 le:
     loss:1448.895544
     acc:0.999066
 encoder:
     loss:0.218886
----------------------------


Epoch: [839/1000]:
train:
----------------------------
 tu:
     loss:18285.162325
     acc:0.854485
 tv:
     loss:19808.603425
     acc:0.540755
 lu:
     loss:14029.890773
     acc:0.924377
 lv:
     loss:17020.395769
     acc:0.307083
 le:
     loss:1517.893900
     acc:0.999638
 encoder:
     loss:0.214278
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.025684
     acc:0.852643
 tv:
     loss:19108.175488
     acc:0.489081
 lu:
     loss:13407.449854
     acc:0.915406
 lv:
     loss:16760.294434
     acc:0.186611
 le:
     loss:1448.311499
     acc:0.999207
 encoder:
     loss:0.177503
----------------------------


Epoch: [840/1000]:
train:
----------------------------
 tu:
     loss:18285.316281
     acc:0.854468
 tv:
     loss:19809.300361
     acc:0.540267
 lu:
     loss:14028.222225
     acc:0.924758
 lv:
     loss:17017.488883
     acc:0.307554
 le:
     loss:1517.891526
     acc:0.999633
 encoder:
     loss:0.193230
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.678223
     acc:0.852359
 tv:
     loss:19104.829590
     acc:0.490174
 lu:
     loss:13408.656299
     acc:0.914837
 lv:
     loss:16755.821094
     acc:0.187832
 le:
     loss:1448.678613
     acc:0.999126
 encoder:
     loss:0.199546
----------------------------


Epoch: [841/1000]:
train:
----------------------------
 tu:
     loss:18284.414994
     acc:0.854642
 tv:
     loss:19808.411428
     acc:0.540589
 lu:
     loss:14029.677973
     acc:0.924496
 lv:
     loss:17020.531318
     acc:0.306716
 le:
     loss:1517.879900
     acc:0.999643
 encoder:
     loss:0.228876
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.313281
     acc:0.852554
 tv:
     loss:19103.797266
     acc:0.490241
 lu:
     loss:13407.524756
     acc:0.915124
 lv:
     loss:16766.365430
     acc:0.185478
 le:
     loss:1448.624500
     acc:0.999127
 encoder:
     loss:0.191082
----------------------------


Epoch: [842/1000]:
train:
----------------------------
 tu:
     loss:18285.524925
     acc:0.854422
 tv:
     loss:19809.124534
     acc:0.540418
 lu:
     loss:14029.809582
     acc:0.924422
 lv:
     loss:17019.796841
     acc:0.306699
 le:
     loss:1517.762609
     acc:0.999673
 encoder:
     loss:0.205209
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.806543
     acc:0.852664
 tv:
     loss:19106.564453
     acc:0.489703
 lu:
     loss:13406.560645
     acc:0.915532
 lv:
     loss:16755.755762
     acc:0.187859
 le:
     loss:1448.873657
     acc:0.999084
 encoder:
     loss:0.247868
----------------------------


Epoch: [843/1000]:
train:
----------------------------
 tu:
     loss:18285.135947
     acc:0.854539
 tv:
     loss:19808.224030
     acc:0.540578
 lu:
     loss:14030.060115
     acc:0.924265
 lv:
     loss:17018.260367
     acc:0.307352
 le:
     loss:1517.815848
     acc:0.999656
 encoder:
     loss:0.246877
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.554297
     acc:0.852685
 tv:
     loss:19107.976953
     acc:0.489268
 lu:
     loss:13406.730225
     acc:0.915483
 lv:
     loss:16765.283691
     acc:0.186217
 le:
     loss:1449.087866
     acc:0.999021
 encoder:
     loss:0.241390
----------------------------


Epoch: [844/1000]:
train:
----------------------------
 tu:
     loss:18284.741926
     acc:0.854554
 tv:
     loss:19809.538199
     acc:0.540535
 lu:
     loss:14028.157295
     acc:0.924750
 lv:
     loss:17019.789074
     acc:0.307057
 le:
     loss:1517.884472
     acc:0.999633
 encoder:
     loss:0.206132
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.078223
     acc:0.852573
 tv:
     loss:19110.852637
     acc:0.488424
 lu:
     loss:13409.436133
     acc:0.914813
 lv:
     loss:16755.501953
     acc:0.188908
 le:
     loss:1448.726331
     acc:0.999123
 encoder:
     loss:0.158499
----------------------------


Epoch: [845/1000]:
train:
----------------------------
 tu:
     loss:18284.663586
     acc:0.854567
 tv:
     loss:19810.129213
     acc:0.540328
 lu:
     loss:14029.254724
     acc:0.924539
 lv:
     loss:17014.813181
     acc:0.308024
 le:
     loss:1517.830713
     acc:0.999649
 encoder:
     loss:0.158553
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.530957
     acc:0.852474
 tv:
     loss:19106.626953
     acc:0.489792
 lu:
     loss:13407.944922
     acc:0.915293
 lv:
     loss:16756.839062
     acc:0.187888
 le:
     loss:1448.758307
     acc:0.999125
 encoder:
     loss:0.152256
----------------------------


Epoch: [846/1000]:
train:
----------------------------
 tu:
     loss:18284.800361
     acc:0.854589
 tv:
     loss:19807.290879
     acc:0.540769
 lu:
     loss:14028.569336
     acc:0.924755
 lv:
     loss:17013.173204
     acc:0.308493
 le:
     loss:1517.871599
     acc:0.999634
 encoder:
     loss:0.185718
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.396582
     acc:0.852153
 tv:
     loss:19108.312500
     acc:0.489470
 lu:
     loss:13407.153955
     acc:0.915384
 lv:
     loss:16748.693555
     acc:0.189672
 le:
     loss:1448.895081
     acc:0.999062
 encoder:
     loss:0.164212
----------------------------


Epoch: [847/1000]:
train:
----------------------------
 tu:
     loss:18284.873603
     acc:0.854574
 tv:
     loss:19806.465945
     acc:0.541114
 lu:
     loss:14029.148506
     acc:0.924452
 lv:
     loss:17016.459473
     acc:0.307706
 le:
     loss:1517.844025
     acc:0.999647
 encoder:
     loss:0.191390
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.755176
     acc:0.852430
 tv:
     loss:19106.938574
     acc:0.489571
 lu:
     loss:13405.334717
     acc:0.915795
 lv:
     loss:16759.369531
     acc:0.186882
 le:
     loss:1448.763818
     acc:0.999104
 encoder:
     loss:0.204091
----------------------------


Epoch: [848/1000]:
train:
----------------------------
 tu:
     loss:18285.490995
     acc:0.854388
 tv:
     loss:19807.843421
     acc:0.540690
 lu:
     loss:14028.283578
     acc:0.924730
 lv:
     loss:17014.125500
     acc:0.308000
 le:
     loss:1517.845137
     acc:0.999644
 encoder:
     loss:0.239668
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.147461
     acc:0.852500
 tv:
     loss:19106.579980
     acc:0.489659
 lu:
     loss:13407.945898
     acc:0.915273
 lv:
     loss:16753.063574
     acc:0.188472
 le:
     loss:1448.949323
     acc:0.999062
 encoder:
     loss:0.305080
----------------------------


Epoch: [849/1000]:
train:
----------------------------
 tu:
     loss:18284.816338
     acc:0.854512
 tv:
     loss:19808.814374
     acc:0.540512
 lu:
     loss:14029.957610
     acc:0.924377
 lv:
     loss:17016.708053
     acc:0.307648
 le:
     loss:1517.861974
     acc:0.999635
 encoder:
     loss:0.266671
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17431.083301
     acc:0.852354
 tv:
     loss:19105.256836
     acc:0.489793
 lu:
     loss:13407.481982
     acc:0.915402
 lv:
     loss:16762.427246
     acc:0.186172
 le:
     loss:1449.137640
     acc:0.999042
 encoder:
     loss:0.230230
----------------------------


Epoch: [850/1000]:
train:
----------------------------
 tu:
     loss:18284.707417
     acc:0.854646
 tv:
     loss:19809.136582
     acc:0.540475
 lu:
     loss:14030.026526
     acc:0.924405
 lv:
     loss:17017.849802
     acc:0.307478
 le:
     loss:1517.891902
     acc:0.999640
 encoder:
     loss:0.256907
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.335645
     acc:0.852539
 tv:
     loss:19109.830273
     acc:0.489120
 lu:
     loss:13407.654053
     acc:0.915212
 lv:
     loss:16757.913086
     acc:0.187702
 le:
     loss:1448.682483
     acc:0.999122
 encoder:
     loss:0.204658
----------------------------


Epoch: [851/1000]:
train:
----------------------------
 tu:
     loss:18285.840843
     acc:0.854315
 tv:
     loss:19809.802621
     acc:0.540382
 lu:
     loss:14028.316895
     acc:0.924727
 lv:
     loss:17015.724223
     acc:0.307690
 le:
     loss:1517.849588
     acc:0.999644
 encoder:
     loss:0.220557
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.982715
     acc:0.852394
 tv:
     loss:19109.089746
     acc:0.488885
 lu:
     loss:13408.614209
     acc:0.915169
 lv:
     loss:16755.773242
     acc:0.188242
 le:
     loss:1449.001617
     acc:0.999079
 encoder:
     loss:0.201913
----------------------------


Epoch: [852/1000]:
train:
----------------------------
 tu:
     loss:18284.535315
     acc:0.854621
 tv:
     loss:19808.018055
     acc:0.540901
 lu:
     loss:14029.345397
     acc:0.924497
 lv:
     loss:17016.078965
     acc:0.307831
 le:
     loss:1517.748319
     acc:0.999662
 encoder:
     loss:0.259040
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.297168
     acc:0.852515
 tv:
     loss:19107.423828
     acc:0.489449
 lu:
     loss:13409.108301
     acc:0.915121
 lv:
     loss:16756.625586
     acc:0.187768
 le:
     loss:1449.021375
     acc:0.999041
 encoder:
     loss:0.414267
----------------------------


Epoch: [853/1000]:
train:
----------------------------
 tu:
     loss:18285.262491
     acc:0.854358
 tv:
     loss:19809.564260
     acc:0.540547
 lu:
     loss:14028.913506
     acc:0.924531
 lv:
     loss:17013.056130
     acc:0.308322
 le:
     loss:1517.816198
     acc:0.999659
 encoder:
     loss:0.364805
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.217383
     acc:0.852499
 tv:
     loss:19107.282227
     acc:0.489552
 lu:
     loss:13407.517920
     acc:0.915498
 lv:
     loss:16750.427051
     acc:0.189309
 le:
     loss:1448.710706
     acc:0.999142
 encoder:
     loss:0.261930
----------------------------


Epoch: [854/1000]:
train:
----------------------------
 tu:
     loss:18284.517431
     acc:0.854454
 tv:
     loss:19807.782840
     acc:0.540968
 lu:
     loss:14029.598531
     acc:0.924534
 lv:
     loss:17015.174101
     acc:0.308461
 le:
     loss:1517.714050
     acc:0.999675
 encoder:
     loss:0.230137
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.327344
     acc:0.852535
 tv:
     loss:19107.626367
     acc:0.489652
 lu:
     loss:13406.385791
     acc:0.915616
 lv:
     loss:16763.478711
     acc:0.185960
 le:
     loss:1448.498608
     acc:0.999185
 encoder:
     loss:0.187218
----------------------------


Epoch: [855/1000]:
train:
----------------------------
 tu:
     loss:18283.947504
     acc:0.854625
 tv:
     loss:19809.013377
     acc:0.540626
 lu:
     loss:14028.493925
     acc:0.924703
 lv:
     loss:17014.882460
     acc:0.308273
 le:
     loss:1517.813027
     acc:0.999651
 encoder:
     loss:0.193236
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.829980
     acc:0.852679
 tv:
     loss:19111.111230
     acc:0.488400
 lu:
     loss:13409.155029
     acc:0.914885
 lv:
     loss:16754.496289
     acc:0.188644
 le:
     loss:1448.745862
     acc:0.999123
 encoder:
     loss:0.177714
----------------------------


Epoch: [856/1000]:
train:
----------------------------
 tu:
     loss:18284.310093
     acc:0.854667
 tv:
     loss:19808.052314
     acc:0.540511
 lu:
     loss:14028.505053
     acc:0.924601
 lv:
     loss:17012.395553
     acc:0.308671
 le:
     loss:1517.786269
     acc:0.999662
 encoder:
     loss:0.197458
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.978711
     acc:0.852651
 tv:
     loss:19105.955469
     acc:0.489733
 lu:
     loss:13407.936719
     acc:0.915125
 lv:
     loss:16754.194434
     acc:0.188443
 le:
     loss:1448.819397
     acc:0.999124
 encoder:
     loss:0.199252
----------------------------


Epoch: [857/1000]:
train:
----------------------------
 tu:
     loss:18284.974394
     acc:0.854478
 tv:
     loss:19806.734273
     acc:0.541051
 lu:
     loss:14028.876215
     acc:0.924664
 lv:
     loss:17013.989712
     acc:0.308553
 le:
     loss:1517.966722
     acc:0.999626
 encoder:
     loss:0.213992
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.432812
     acc:0.852718
 tv:
     loss:19109.916406
     acc:0.488834
 lu:
     loss:13408.423730
     acc:0.914844
 lv:
     loss:16755.827344
     acc:0.188431
 le:
     loss:1448.939227
     acc:0.999084
 encoder:
     loss:0.183740
----------------------------


Epoch: [858/1000]:
train:
----------------------------
 tu:
     loss:18285.055925
     acc:0.854545
 tv:
     loss:19807.190895
     acc:0.540786
 lu:
     loss:14028.950298
     acc:0.924580
 lv:
     loss:17013.978107
     acc:0.308290
 le:
     loss:1517.848472
     acc:0.999649
 encoder:
     loss:0.216784
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.664258
     acc:0.852798
 tv:
     loss:19110.716406
     acc:0.488493
 lu:
     loss:13409.693604
     acc:0.914828
 lv:
     loss:16760.087695
     acc:0.187399
 le:
     loss:1448.883130
     acc:0.999083
 encoder:
     loss:0.229266
----------------------------


Epoch: [859/1000]:
train:
----------------------------
 tu:
     loss:18284.644384
     acc:0.854641
 tv:
     loss:19808.030046
     acc:0.540802
 lu:
     loss:14029.362929
     acc:0.924618
 lv:
     loss:17017.978993
     acc:0.307311
 le:
     loss:1517.731697
     acc:0.999675
 encoder:
     loss:0.212282
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.526172
     acc:0.852695
 tv:
     loss:19108.845605
     acc:0.489010
 lu:
     loss:13408.455762
     acc:0.915008
 lv:
     loss:16753.974805
     acc:0.188573
 le:
     loss:1448.436249
     acc:0.999187
 encoder:
     loss:0.199800
----------------------------


Epoch: [860/1000]:
train:
----------------------------
 tu:
     loss:18284.966502
     acc:0.854545
 tv:
     loss:19809.900243
     acc:0.540285
 lu:
     loss:14029.693359
     acc:0.924537
 lv:
     loss:17017.178155
     acc:0.307618
 le:
     loss:1517.834186
     acc:0.999655
 encoder:
     loss:0.236523
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.811719
     acc:0.852775
 tv:
     loss:19106.432227
     acc:0.489559
 lu:
     loss:13406.918408
     acc:0.915576
 lv:
     loss:16755.445703
     acc:0.188294
 le:
     loss:1448.738934
     acc:0.999102
 encoder:
     loss:0.188028
----------------------------


Epoch: [861/1000]:
train:
----------------------------
 tu:
     loss:18284.865382
     acc:0.854522
 tv:
     loss:19809.072175
     acc:0.540702
 lu:
     loss:14028.455385
     acc:0.924730
 lv:
     loss:17013.189430
     acc:0.308703
 le:
     loss:1517.770528
     acc:0.999662
 encoder:
     loss:0.206687
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.566895
     acc:0.852670
 tv:
     loss:19109.127441
     acc:0.488912
 lu:
     loss:13406.780713
     acc:0.915335
 lv:
     loss:16756.957031
     acc:0.187051
 le:
     loss:1448.247418
     acc:0.999228
 encoder:
     loss:0.189080
----------------------------


Epoch: [862/1000]:
train:
----------------------------
 tu:
     loss:18284.989212
     acc:0.854405
 tv:
     loss:19807.682049
     acc:0.540755
 lu:
     loss:14029.376624
     acc:0.924430
 lv:
     loss:17013.381404
     acc:0.308623
 le:
     loss:1517.936528
     acc:0.999625
 encoder:
     loss:0.197449
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.873340
     acc:0.852603
 tv:
     loss:19109.109863
     acc:0.488928
 lu:
     loss:13410.380566
     acc:0.914631
 lv:
     loss:16755.816895
     acc:0.187938
 le:
     loss:1448.856189
     acc:0.999064
 encoder:
     loss:0.195955
----------------------------


Epoch: [863/1000]:
train:
----------------------------
 tu:
     loss:18284.781250
     acc:0.854509
 tv:
     loss:19809.076376
     acc:0.540488
 lu:
     loss:14028.802087
     acc:0.924727
 lv:
     loss:17011.056766
     acc:0.309012
 le:
     loss:1517.703900
     acc:0.999685
 encoder:
     loss:0.192205
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.287891
     acc:0.852633
 tv:
     loss:19109.710742
     acc:0.488831
 lu:
     loss:13408.306641
     acc:0.915187
 lv:
     loss:16760.023047
     acc:0.187099
 le:
     loss:1448.494269
     acc:0.999164
 encoder:
     loss:0.192282
----------------------------


Epoch: [864/1000]:
train:
----------------------------
 tu:
     loss:18284.803279
     acc:0.854508
 tv:
     loss:19810.541436
     acc:0.539966
 lu:
     loss:14029.987123
     acc:0.924284
 lv:
     loss:17014.522961
     acc:0.307919
 le:
     loss:1517.808970
     acc:0.999658
 encoder:
     loss:0.234602
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.951953
     acc:0.852502
 tv:
     loss:19110.112109
     acc:0.488728
 lu:
     loss:13408.687793
     acc:0.914977
 lv:
     loss:16759.207617
     acc:0.187526
 le:
     loss:1448.578424
     acc:0.999165
 encoder:
     loss:0.208821
----------------------------


Epoch: [865/1000]:
train:
----------------------------
 tu:
     loss:18284.903320
     acc:0.854537
 tv:
     loss:19810.483694
     acc:0.540229
 lu:
     loss:14029.328602
     acc:0.924479
 lv:
     loss:17014.317519
     acc:0.308240
 le:
     loss:1517.772289
     acc:0.999662
 encoder:
     loss:0.212156
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.096387
     acc:0.852553
 tv:
     loss:19114.080859
     acc:0.488050
 lu:
     loss:13407.542773
     acc:0.915204
 lv:
     loss:16762.175195
     acc:0.186143
 le:
     loss:1448.284790
     acc:0.999210
 encoder:
     loss:0.193650
----------------------------


Epoch: [866/1000]:
train:
----------------------------
 tu:
     loss:18285.208246
     acc:0.854442
 tv:
     loss:19808.491529
     acc:0.540718
 lu:
     loss:14028.898812
     acc:0.924662
 lv:
     loss:17010.278502
     acc:0.309281
 le:
     loss:1517.759465
     acc:0.999671
 encoder:
     loss:0.221915
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.619922
     acc:0.852699
 tv:
     loss:19111.196582
     acc:0.488814
 lu:
     loss:13407.588916
     acc:0.915249
 lv:
     loss:16756.158398
     acc:0.188059
 le:
     loss:1448.149640
     acc:0.999269
 encoder:
     loss:0.289105
----------------------------


Epoch: [867/1000]:
train:
----------------------------
 tu:
     loss:18284.630394
     acc:0.854569
 tv:
     loss:19807.343466
     acc:0.541074
 lu:
     loss:14028.241211
     acc:0.924805
 lv:
     loss:17013.021428
     acc:0.308639
 le:
     loss:1517.838336
     acc:0.999656
 encoder:
     loss:0.270763
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.871680
     acc:0.852495
 tv:
     loss:19110.531543
     acc:0.489053
 lu:
     loss:13406.901514
     acc:0.915390
 lv:
     loss:16756.596875
     acc:0.188190
 le:
     loss:1448.637238
     acc:0.999106
 encoder:
     loss:0.229774
----------------------------


Epoch: [868/1000]:
train:
----------------------------
 tu:
     loss:18285.234182
     acc:0.854459
 tv:
     loss:19808.177326
     acc:0.540541
 lu:
     loss:14027.855151
     acc:0.924847
 lv:
     loss:17010.949605
     acc:0.308918
 le:
     loss:1517.869968
     acc:0.999638
 encoder:
     loss:0.256338
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.457617
     acc:0.852621
 tv:
     loss:19105.634277
     acc:0.489997
 lu:
     loss:13407.343164
     acc:0.915370
 lv:
     loss:16756.508496
     acc:0.187705
 le:
     loss:1448.309576
     acc:0.999190
 encoder:
     loss:0.200377
----------------------------


Epoch: [869/1000]:
train:
----------------------------
 tu:
     loss:18284.428404
     acc:0.854675
 tv:
     loss:19807.998422
     acc:0.540774
 lu:
     loss:14029.287518
     acc:0.924484
 lv:
     loss:17017.083962
     acc:0.307534
 le:
     loss:1517.829482
     acc:0.999653
 encoder:
     loss:0.208456
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.035254
     acc:0.852597
 tv:
     loss:19103.620898
     acc:0.490492
 lu:
     loss:13408.181934
     acc:0.915195
 lv:
     loss:16753.229492
     acc:0.188884
 le:
     loss:1448.413531
     acc:0.999187
 encoder:
     loss:0.239373
----------------------------


Epoch: [870/1000]:
train:
----------------------------
 tu:
     loss:18284.926224
     acc:0.854654
 tv:
     loss:19808.320471
     acc:0.540399
 lu:
     loss:14029.013910
     acc:0.924660
 lv:
     loss:17013.439533
     acc:0.308406
 le:
     loss:1517.875725
     acc:0.999639
 encoder:
     loss:0.257804
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.902637
     acc:0.852714
 tv:
     loss:19110.743945
     acc:0.488856
 lu:
     loss:13408.199121
     acc:0.915132
 lv:
     loss:16755.148340
     acc:0.188027
 le:
     loss:1448.664172
     acc:0.999108
 encoder:
     loss:0.340540
----------------------------


Epoch: [871/1000]:
train:
----------------------------
 tu:
     loss:18285.373671
     acc:0.854481
 tv:
     loss:19805.892851
     acc:0.541127
 lu:
     loss:14031.057470
     acc:0.924181
 lv:
     loss:17012.334348
     acc:0.308510
 le:
     loss:1517.867704
     acc:0.999637
 encoder:
     loss:0.248353
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.514453
     acc:0.852828
 tv:
     loss:19106.511426
     acc:0.489997
 lu:
     loss:13406.849658
     acc:0.915440
 lv:
     loss:16759.447754
     acc:0.187049
 le:
     loss:1448.682678
     acc:0.999123
 encoder:
     loss:0.199174
----------------------------


Epoch: [872/1000]:
train:
----------------------------
 tu:
     loss:18285.522699
     acc:0.854435
 tv:
     loss:19808.960052
     acc:0.540638
 lu:
     loss:14029.970794
     acc:0.924325
 lv:
     loss:17015.586937
     acc:0.308203
 le:
     loss:1517.951857
     acc:0.999626
 encoder:
     loss:0.345778
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.880469
     acc:0.852697
 tv:
     loss:19112.645215
     acc:0.488268
 lu:
     loss:13409.165527
     acc:0.914884
 lv:
     loss:16760.677051
     acc:0.187652
 le:
     loss:1448.644891
     acc:0.999085
 encoder:
     loss:0.324612
----------------------------


Epoch: [873/1000]:
train:
----------------------------
 tu:
     loss:18285.117823
     acc:0.854570
 tv:
     loss:19808.593739
     acc:0.540537
 lu:
     loss:14029.145326
     acc:0.924612
 lv:
     loss:17016.255734
     acc:0.307745
 le:
     loss:1517.887278
     acc:0.999642
 encoder:
     loss:0.261766
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.387012
     acc:0.852559
 tv:
     loss:19105.091016
     acc:0.490074
 lu:
     loss:13406.692432
     acc:0.915579
 lv:
     loss:16755.895215
     acc:0.188234
 le:
     loss:1448.374408
     acc:0.999207
 encoder:
     loss:0.190569
----------------------------


Epoch: [874/1000]:
train:
----------------------------
 tu:
     loss:18285.264592
     acc:0.854447
 tv:
     loss:19806.860556
     acc:0.540962
 lu:
     loss:14029.277662
     acc:0.924664
 lv:
     loss:17012.683514
     acc:0.308357
 le:
     loss:1517.897854
     acc:0.999639
 encoder:
     loss:0.202567
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.764062
     acc:0.852419
 tv:
     loss:19106.857715
     acc:0.489420
 lu:
     loss:13404.646484
     acc:0.915926
 lv:
     loss:16760.480664
     acc:0.186949
 le:
     loss:1448.460114
     acc:0.999185
 encoder:
     loss:0.189507
----------------------------


Epoch: [875/1000]:
train:
----------------------------
 tu:
     loss:18284.727834
     acc:0.854640
 tv:
     loss:19808.638059
     acc:0.540509
 lu:
     loss:14028.168128
     acc:0.924857
 lv:
     loss:17015.109330
     acc:0.307934
 le:
     loss:1517.949300
     acc:0.999623
 encoder:
     loss:0.204757
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.106543
     acc:0.852636
 tv:
     loss:19110.052344
     acc:0.488964
 lu:
     loss:13407.082568
     acc:0.915299
 lv:
     loss:16750.658691
     acc:0.188760
 le:
     loss:1448.659058
     acc:0.999124
 encoder:
     loss:0.189221
----------------------------


Epoch: [876/1000]:
train:
----------------------------
 tu:
     loss:18286.302927
     acc:0.854273
 tv:
     loss:19809.389262
     acc:0.540518
 lu:
     loss:14029.132915
     acc:0.924450
 lv:
     loss:17015.737918
     acc:0.307733
 le:
     loss:1517.813864
     acc:0.999658
 encoder:
     loss:0.217378
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.300586
     acc:0.852571
 tv:
     loss:19115.321777
     acc:0.487664
 lu:
     loss:13409.914502
     acc:0.914867
 lv:
     loss:16753.063184
     acc:0.188650
 le:
     loss:1448.706622
     acc:0.999106
 encoder:
     loss:0.212842
----------------------------


Epoch: [877/1000]:
train:
----------------------------
 tu:
     loss:18285.857286
     acc:0.854362
 tv:
     loss:19807.705101
     acc:0.540712
 lu:
     loss:14028.788472
     acc:0.924568
 lv:
     loss:17010.807061
     acc:0.308736
 le:
     loss:1517.812784
     acc:0.999648
 encoder:
     loss:0.248815
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.568164
     acc:0.852453
 tv:
     loss:19111.463281
     acc:0.488553
 lu:
     loss:13408.539111
     acc:0.914906
 lv:
     loss:16754.719141
     acc:0.188303
 le:
     loss:1448.566571
     acc:0.999144
 encoder:
     loss:0.259346
----------------------------


Epoch: [878/1000]:
train:
----------------------------
 tu:
     loss:18285.154467
     acc:0.854555
 tv:
     loss:19808.092512
     acc:0.540714
 lu:
     loss:14029.170013
     acc:0.924606
 lv:
     loss:17009.212641
     acc:0.309234
 le:
     loss:1517.879876
     acc:0.999642
 encoder:
     loss:0.231040
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.429883
     acc:0.852716
 tv:
     loss:19108.017383
     acc:0.489303
 lu:
     loss:13405.912939
     acc:0.915582
 lv:
     loss:16749.489453
     acc:0.189484
 le:
     loss:1448.143536
     acc:0.999249
 encoder:
     loss:0.214990
----------------------------


Epoch: [879/1000]:
train:
----------------------------
 tu:
     loss:18285.071948
     acc:0.854432
 tv:
     loss:19807.282840
     acc:0.540845
 lu:
     loss:14027.499023
     acc:0.924870
 lv:
     loss:17013.367664
     acc:0.308575
 le:
     loss:1517.891146
     acc:0.999638
 encoder:
     loss:0.228981
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.279199
     acc:0.852576
 tv:
     loss:19107.421387
     acc:0.489813
 lu:
     loss:13408.096680
     acc:0.915336
 lv:
     loss:16753.618164
     acc:0.188398
 le:
     loss:1448.528461
     acc:0.999166
 encoder:
     loss:0.194415
----------------------------


Epoch: [880/1000]:
train:
----------------------------
 tu:
     loss:18285.370242
     acc:0.854366
 tv:
     loss:19808.256416
     acc:0.540557
 lu:
     loss:14029.844125
     acc:0.924459
 lv:
     loss:17011.408362
     acc:0.308775
 le:
     loss:1517.875836
     acc:0.999635
 encoder:
     loss:0.220424
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.533594
     acc:0.852635
 tv:
     loss:19109.860547
     acc:0.489066
 lu:
     loss:13406.638965
     acc:0.915566
 lv:
     loss:16750.380371
     acc:0.188686
 le:
     loss:1448.477802
     acc:0.999128
 encoder:
     loss:0.247736
----------------------------


Epoch: [881/1000]:
train:
----------------------------
 tu:
     loss:18285.025584
     acc:0.854456
 tv:
     loss:19808.608535
     acc:0.540563
 lu:
     loss:14028.635685
     acc:0.924753
 lv:
     loss:17011.032238
     acc:0.308867
 le:
     loss:1518.002255
     acc:0.999611
 encoder:
     loss:0.415078
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.225879
     acc:0.852728
 tv:
     loss:19114.129980
     acc:0.487808
 lu:
     loss:13408.343701
     acc:0.914870
 lv:
     loss:16755.373340
     acc:0.187449
 le:
     loss:1448.708075
     acc:0.999124
 encoder:
     loss:0.270960
----------------------------


Epoch: [882/1000]:
train:
----------------------------
 tu:
     loss:18285.209791
     acc:0.854466
 tv:
     loss:19807.918661
     acc:0.540677
 lu:
     loss:14029.766829
     acc:0.924507
 lv:
     loss:17010.685751
     acc:0.309233
 le:
     loss:1517.918460
     acc:0.999638
 encoder:
     loss:0.290112
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.631836
     acc:0.852681
 tv:
     loss:19106.591992
     acc:0.489659
 lu:
     loss:13406.780859
     acc:0.915490
 lv:
     loss:16752.048242
     acc:0.189280
 le:
     loss:1448.390503
     acc:0.999187
 encoder:
     loss:0.330262
----------------------------


Epoch: [883/1000]:
train:
----------------------------
 tu:
     loss:18284.487271
     acc:0.854719
 tv:
     loss:19809.158589
     acc:0.540224
 lu:
     loss:14029.286723
     acc:0.924537
 lv:
     loss:17010.209234
     acc:0.308692
 le:
     loss:1517.720074
     acc:0.999677
 encoder:
     loss:0.251991
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.540332
     acc:0.852744
 tv:
     loss:19109.994238
     acc:0.488765
 lu:
     loss:13407.806055
     acc:0.915117
 lv:
     loss:16761.377637
     acc:0.186102
 le:
     loss:1448.752869
     acc:0.999125
 encoder:
     loss:0.270002
----------------------------


Epoch: [884/1000]:
train:
----------------------------
 tu:
     loss:18285.443507
     acc:0.854483
 tv:
     loss:19809.420013
     acc:0.540480
 lu:
     loss:14029.605196
     acc:0.924466
 lv:
     loss:17012.066054
     acc:0.308499
 le:
     loss:1517.809147
     acc:0.999659
 encoder:
     loss:0.265099
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.477637
     acc:0.852763
 tv:
     loss:19110.647461
     acc:0.488732
 lu:
     loss:13407.586523
     acc:0.915145
 lv:
     loss:16754.513281
     acc:0.188286
 le:
     loss:1448.860663
     acc:0.999085
 encoder:
     loss:0.210299
----------------------------


Epoch: [885/1000]:
train:
----------------------------
 tu:
     loss:18284.870776
     acc:0.854456
 tv:
     loss:19810.257971
     acc:0.540220
 lu:
     loss:14027.468898
     acc:0.924858
 lv:
     loss:17010.358285
     acc:0.309337
 le:
     loss:1517.918021
     acc:0.999638
 encoder:
     loss:0.226953
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.006934
     acc:0.852821
 tv:
     loss:19106.970313
     acc:0.489881
 lu:
     loss:13404.615430
     acc:0.915899
 lv:
     loss:16757.830469
     acc:0.188126
 le:
     loss:1448.658325
     acc:0.999146
 encoder:
     loss:0.218098
----------------------------


Epoch: [886/1000]:
train:
----------------------------
 tu:
     loss:18284.839878
     acc:0.854557
 tv:
     loss:19805.938272
     acc:0.541074
 lu:
     loss:14030.009709
     acc:0.924284
 lv:
     loss:17009.988894
     acc:0.308916
 le:
     loss:1517.864001
     acc:0.999647
 encoder:
     loss:0.189832
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.186328
     acc:0.852569
 tv:
     loss:19109.849023
     acc:0.489153
 lu:
     loss:13407.114551
     acc:0.915186
 lv:
     loss:16760.985352
     acc:0.186848
 le:
     loss:1448.648285
     acc:0.999123
 encoder:
     loss:0.182980
----------------------------


Epoch: [887/1000]:
train:
----------------------------
 tu:
     loss:18284.120117
     acc:0.854713
 tv:
     loss:19808.505269
     acc:0.540457
 lu:
     loss:14029.058457
     acc:0.924571
 lv:
     loss:17009.809536
     acc:0.309107
 le:
     loss:1517.933699
     acc:0.999629
 encoder:
     loss:0.216753
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.011230
     acc:0.852908
 tv:
     loss:19106.439258
     acc:0.489642
 lu:
     loss:13409.924316
     acc:0.914815
 lv:
     loss:16747.723438
     acc:0.189760
 le:
     loss:1448.513745
     acc:0.999164
 encoder:
     loss:0.367672
----------------------------


Epoch: [888/1000]:
train:
----------------------------
 tu:
     loss:18284.888615
     acc:0.854516
 tv:
     loss:19807.880269
     acc:0.540794
 lu:
     loss:14028.147961
     acc:0.924787
 lv:
     loss:17008.680891
     acc:0.309414
 le:
     loss:1517.888977
     acc:0.999633
 encoder:
     loss:0.371338
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.511719
     acc:0.852796
 tv:
     loss:19105.401465
     acc:0.489928
 lu:
     loss:13409.145898
     acc:0.914845
 lv:
     loss:16760.347266
     acc:0.186581
 le:
     loss:1448.523999
     acc:0.999164
 encoder:
     loss:0.378252
----------------------------


Epoch: [889/1000]:
train:
----------------------------
 tu:
     loss:18284.348553
     acc:0.854603
 tv:
     loss:19806.086710
     acc:0.540933
 lu:
     loss:14028.306561
     acc:0.924822
 lv:
     loss:17005.875852
     acc:0.310134
 le:
     loss:1517.721037
     acc:0.999673
 encoder:
     loss:0.301050
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.287012
     acc:0.852782
 tv:
     loss:19112.151270
     acc:0.488253
 lu:
     loss:13410.015430
     acc:0.914560
 lv:
     loss:16758.316797
     acc:0.187667
 le:
     loss:1448.595245
     acc:0.999128
 encoder:
     loss:0.204377
----------------------------


Epoch: [890/1000]:
train:
----------------------------
 tu:
     loss:18285.008528
     acc:0.854521
 tv:
     loss:19805.573288
     acc:0.541339
 lu:
     loss:14028.783090
     acc:0.924600
 lv:
     loss:17007.244845
     acc:0.309572
 le:
     loss:1517.833141
     acc:0.999648
 encoder:
     loss:0.187575
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.726172
     acc:0.852679
 tv:
     loss:19111.881445
     acc:0.488580
 lu:
     loss:13408.862646
     acc:0.914862
 lv:
     loss:16759.456445
     acc:0.187636
 le:
     loss:1448.740399
     acc:0.999142
 encoder:
     loss:0.189496
----------------------------


Epoch: [891/1000]:
train:
----------------------------
 tu:
     loss:18285.632778
     acc:0.854349
 tv:
     loss:19807.039335
     acc:0.540689
 lu:
     loss:14028.541879
     acc:0.924669
 lv:
     loss:17010.630837
     acc:0.308757
 le:
     loss:1517.766660
     acc:0.999662
 encoder:
     loss:0.222415
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.090527
     acc:0.852597
 tv:
     loss:19105.866895
     acc:0.489723
 lu:
     loss:13407.974561
     acc:0.915146
 lv:
     loss:16755.457227
     acc:0.188265
 le:
     loss:1448.658392
     acc:0.999123
 encoder:
     loss:0.202133
----------------------------


Epoch: [892/1000]:
train:
----------------------------
 tu:
     loss:18284.232240
     acc:0.854700
 tv:
     loss:19806.685808
     acc:0.541085
 lu:
     loss:14027.750204
     acc:0.924719
 lv:
     loss:17008.003532
     acc:0.309269
 le:
     loss:1517.736457
     acc:0.999664
 encoder:
     loss:0.197565
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.833789
     acc:0.852576
 tv:
     loss:19105.044238
     acc:0.490117
 lu:
     loss:13409.256152
     acc:0.915112
 lv:
     loss:16754.087305
     acc:0.188217
 le:
     loss:1448.385718
     acc:0.999189
 encoder:
     loss:0.180665
----------------------------


Epoch: [893/1000]:
train:
----------------------------
 tu:
     loss:18284.769486
     acc:0.854575
 tv:
     loss:19806.995253
     acc:0.540805
 lu:
     loss:14028.789789
     acc:0.924710
 lv:
     loss:17011.425849
     acc:0.308604
 le:
     loss:1517.825102
     acc:0.999653
 encoder:
     loss:0.174059
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.405176
     acc:0.852736
 tv:
     loss:19108.246191
     acc:0.489043
 lu:
     loss:13408.002344
     acc:0.915159
 lv:
     loss:16751.188477
     acc:0.189452
 le:
     loss:1448.617078
     acc:0.999127
 encoder:
     loss:0.187669
----------------------------


Epoch: [894/1000]:
train:
----------------------------
 tu:
     loss:18284.864439
     acc:0.854531
 tv:
     loss:19805.706668
     acc:0.541111
 lu:
     loss:14027.990348
     acc:0.924697
 lv:
     loss:17007.418707
     acc:0.309506
 le:
     loss:1517.803015
     acc:0.999655
 encoder:
     loss:0.201422
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.134766
     acc:0.852454
 tv:
     loss:19109.269727
     acc:0.489100
 lu:
     loss:13408.190186
     acc:0.915293
 lv:
     loss:16757.377930
     acc:0.188043
 le:
     loss:1448.475574
     acc:0.999167
 encoder:
     loss:0.190959
----------------------------


Epoch: [895/1000]:
train:
----------------------------
 tu:
     loss:18284.429154
     acc:0.854565
 tv:
     loss:19805.568496
     acc:0.541138
 lu:
     loss:14028.605128
     acc:0.924676
 lv:
     loss:17005.772756
     acc:0.309817
 le:
     loss:1517.855266
     acc:0.999641
 encoder:
     loss:0.199408
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.500781
     acc:0.852740
 tv:
     loss:19110.125488
     acc:0.488962
 lu:
     loss:13408.447949
     acc:0.915082
 lv:
     loss:16759.978809
     acc:0.186981
 le:
     loss:1448.367285
     acc:0.999167
 encoder:
     loss:0.198176
----------------------------


Epoch: [896/1000]:
train:
----------------------------
 tu:
     loss:18284.722372
     acc:0.854633
 tv:
     loss:19805.166016
     acc:0.541379
 lu:
     loss:14029.044831
     acc:0.924600
 lv:
     loss:17007.420910
     acc:0.309680
 le:
     loss:1517.802612
     acc:0.999649
 encoder:
     loss:0.251817
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.150391
     acc:0.852613
 tv:
     loss:19112.770898
     acc:0.488477
 lu:
     loss:13408.792480
     acc:0.914965
 lv:
     loss:16764.940625
     acc:0.185909
 le:
     loss:1448.326086
     acc:0.999207
 encoder:
     loss:0.229070
----------------------------


Epoch: [897/1000]:
train:
----------------------------
 tu:
     loss:18284.283544
     acc:0.854705
 tv:
     loss:19806.464912
     acc:0.540973
 lu:
     loss:14027.817451
     acc:0.924802
 lv:
     loss:17007.512854
     acc:0.309429
 le:
     loss:1517.828386
     acc:0.999655
 encoder:
     loss:0.219665
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.466016
     acc:0.852453
 tv:
     loss:19113.520410
     acc:0.488064
 lu:
     loss:13408.360840
     acc:0.915211
 lv:
     loss:16759.392285
     acc:0.187220
 le:
     loss:1448.438257
     acc:0.999148
 encoder:
     loss:0.249719
----------------------------


Epoch: [898/1000]:
train:
----------------------------
 tu:
     loss:18284.224178
     acc:0.854672
 tv:
     loss:19805.799793
     acc:0.541384
 lu:
     loss:14027.971123
     acc:0.924760
 lv:
     loss:17006.254292
     acc:0.309959
 le:
     loss:1517.781466
     acc:0.999661
 encoder:
     loss:0.242312
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.223340
     acc:0.852561
 tv:
     loss:19106.244141
     acc:0.489722
 lu:
     loss:13406.991016
     acc:0.915282
 lv:
     loss:16750.900195
     acc:0.189327
 le:
     loss:1448.368835
     acc:0.999207
 encoder:
     loss:0.204366
----------------------------


Epoch: [899/1000]:
train:
----------------------------
 tu:
     loss:18285.684866
     acc:0.854365
 tv:
     loss:19806.229674
     acc:0.541126
 lu:
     loss:14027.987782
     acc:0.924755
 lv:
     loss:17007.805880
     acc:0.309488
 le:
     loss:1517.811663
     acc:0.999661
 encoder:
     loss:0.207046
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.194629
     acc:0.852556
 tv:
     loss:19105.988770
     acc:0.489700
 lu:
     loss:13407.756787
     acc:0.915340
 lv:
     loss:16751.947754
     acc:0.188731
 le:
     loss:1448.328375
     acc:0.999203
 encoder:
     loss:0.201808
----------------------------


Epoch: [900/1000]:
train:
----------------------------
 tu:
     loss:18284.876590
     acc:0.854559
 tv:
     loss:19805.285202
     acc:0.541322
 lu:
     loss:14027.969988
     acc:0.924856
 lv:
     loss:17008.464128
     acc:0.309540
 le:
     loss:1517.813936
     acc:0.999657
 encoder:
     loss:0.207776
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.625586
     acc:0.852564
 tv:
     loss:19110.890234
     acc:0.488622
 lu:
     loss:13406.443555
     acc:0.915517
 lv:
     loss:16753.264062
     acc:0.189038
 le:
     loss:1448.359161
     acc:0.999207
 encoder:
     loss:0.221890
----------------------------


Epoch: [901/1000]:
train:
----------------------------
 tu:
     loss:18284.781091
     acc:0.854522
 tv:
     loss:19807.394724
     acc:0.540942
 lu:
     loss:14028.700888
     acc:0.924679
 lv:
     loss:17010.573083
     acc:0.309220
 le:
     loss:1517.831844
     acc:0.999654
 encoder:
     loss:0.275987
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.839551
     acc:0.852328
 tv:
     loss:19110.732520
     acc:0.488731
 lu:
     loss:13407.215576
     acc:0.915362
 lv:
     loss:16761.402344
     acc:0.186440
 le:
     loss:1448.560919
     acc:0.999129
 encoder:
     loss:0.293049
----------------------------


Epoch: [902/1000]:
train:
----------------------------
 tu:
     loss:18284.343114
     acc:0.854741
 tv:
     loss:19806.834654
     acc:0.540835
 lu:
     loss:14028.113690
     acc:0.924821
 lv:
     loss:17005.009209
     acc:0.310143
 le:
     loss:1517.842088
     acc:0.999653
 encoder:
     loss:0.260463
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.695117
     acc:0.852707
 tv:
     loss:19102.465039
     acc:0.490501
 lu:
     loss:13406.264502
     acc:0.915624
 lv:
     loss:16757.341211
     acc:0.187546
 le:
     loss:1448.590997
     acc:0.999127
 encoder:
     loss:0.205919
----------------------------


Epoch: [903/1000]:
train:
----------------------------
 tu:
     loss:18284.718500
     acc:0.854652
 tv:
     loss:19808.084268
     acc:0.540595
 lu:
     loss:14027.957429
     acc:0.924830
 lv:
     loss:17008.373569
     acc:0.309283
 le:
     loss:1517.817268
     acc:0.999645
 encoder:
     loss:0.233662
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.465820
     acc:0.852618
 tv:
     loss:19110.839941
     acc:0.488874
 lu:
     loss:13406.617334
     acc:0.915470
 lv:
     loss:16754.622461
     acc:0.188004
 le:
     loss:1448.638947
     acc:0.999104
 encoder:
     loss:0.212370
----------------------------


Epoch: [904/1000]:
train:
----------------------------
 tu:
     loss:18285.017578
     acc:0.854473
 tv:
     loss:19809.271110
     acc:0.540437
 lu:
     loss:14027.886776
     acc:0.924899
 lv:
     loss:17010.004043
     acc:0.309005
 le:
     loss:1517.862625
     acc:0.999647
 encoder:
     loss:0.221446
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.028809
     acc:0.852555
 tv:
     loss:19106.962891
     acc:0.489806
 lu:
     loss:13406.618652
     acc:0.915393
 lv:
     loss:16756.279199
     acc:0.187988
 le:
     loss:1448.433868
     acc:0.999224
 encoder:
     loss:0.208944
----------------------------


Epoch: [905/1000]:
train:
----------------------------
 tu:
     loss:18284.721089
     acc:0.854501
 tv:
     loss:19806.535940
     acc:0.541137
 lu:
     loss:14030.079363
     acc:0.924283
 lv:
     loss:17007.470930
     acc:0.309530
 le:
     loss:1517.806328
     acc:0.999658
 encoder:
     loss:0.220243
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.633691
     acc:0.852433
 tv:
     loss:19103.822754
     acc:0.490441
 lu:
     loss:13406.726465
     acc:0.915478
 lv:
     loss:16753.014941
     acc:0.188815
 le:
     loss:1448.228741
     acc:0.999227
 encoder:
     loss:0.219637
----------------------------


Epoch: [906/1000]:
train:
----------------------------
 tu:
     loss:18284.086948
     acc:0.854660
 tv:
     loss:19807.473360
     acc:0.540913
 lu:
     loss:14027.117233
     acc:0.924917
 lv:
     loss:17007.139376
     acc:0.309890
 le:
     loss:1517.675837
     acc:0.999682
 encoder:
     loss:0.221976
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.973730
     acc:0.852462
 tv:
     loss:19103.575098
     acc:0.490055
 lu:
     loss:13406.818164
     acc:0.915406
 lv:
     loss:16759.118652
     acc:0.187954
 le:
     loss:1448.424231
     acc:0.999185
 encoder:
     loss:0.207694
----------------------------


Epoch: [907/1000]:
train:
----------------------------
 tu:
     loss:18284.636242
     acc:0.854510
 tv:
     loss:19807.236351
     acc:0.540736
 lu:
     loss:14028.878111
     acc:0.924629
 lv:
     loss:17006.044161
     acc:0.309752
 le:
     loss:1517.860293
     acc:0.999640
 encoder:
     loss:0.325732
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.415137
     acc:0.852498
 tv:
     loss:19109.191504
     acc:0.488928
 lu:
     loss:13406.728174
     acc:0.915260
 lv:
     loss:16760.301855
     acc:0.187323
 le:
     loss:1448.330090
     acc:0.999185
 encoder:
     loss:0.319633
----------------------------


Epoch: [908/1000]:
train:
----------------------------
 tu:
     loss:18284.357842
     acc:0.854614
 tv:
     loss:19807.851188
     acc:0.540601
 lu:
     loss:14028.826070
     acc:0.924633
 lv:
     loss:17005.072606
     acc:0.310100
 le:
     loss:1517.810202
     acc:0.999660
 encoder:
     loss:0.231471
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.977539
     acc:0.852634
 tv:
     loss:19110.873828
     acc:0.488521
 lu:
     loss:13407.371777
     acc:0.915198
 lv:
     loss:16764.528711
     acc:0.186134
 le:
     loss:1448.132159
     acc:0.999267
 encoder:
     loss:0.220537
----------------------------


Epoch: [909/1000]:
train:
----------------------------
 tu:
     loss:18284.606718
     acc:0.854606
 tv:
     loss:19807.714696
     acc:0.540603
 lu:
     loss:14028.626317
     acc:0.924745
 lv:
     loss:17007.539835
     acc:0.309501
 le:
     loss:1517.897785
     acc:0.999636
 encoder:
     loss:0.269010
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.816016
     acc:0.852676
 tv:
     loss:19110.042676
     acc:0.488881
 lu:
     loss:13407.403711
     acc:0.915465
 lv:
     loss:16759.830371
     acc:0.187404
 le:
     loss:1448.523334
     acc:0.999166
 encoder:
     loss:0.261927
----------------------------


Epoch: [910/1000]:
train:
----------------------------
 tu:
     loss:18284.494129
     acc:0.854663
 tv:
     loss:19808.138626
     acc:0.540413
 lu:
     loss:14028.778536
     acc:0.924566
 lv:
     loss:17007.543934
     acc:0.309432
 le:
     loss:1517.908193
     acc:0.999638
 encoder:
     loss:0.242108
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.334766
     acc:0.852497
 tv:
     loss:19110.163672
     acc:0.488923
 lu:
     loss:13407.288867
     acc:0.915395
 lv:
     loss:16763.547266
     acc:0.186348
 le:
     loss:1448.294934
     acc:0.999188
 encoder:
     loss:0.320712
----------------------------


Epoch: [911/1000]:
train:
----------------------------
 tu:
     loss:18285.474700
     acc:0.854405
 tv:
     loss:19808.208973
     acc:0.540605
 lu:
     loss:14027.679756
     acc:0.924749
 lv:
     loss:17009.046296
     acc:0.309206
 le:
     loss:1517.866625
     acc:0.999649
 encoder:
     loss:0.342492
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.391895
     acc:0.852434
 tv:
     loss:19109.994922
     acc:0.488978
 lu:
     loss:13406.281934
     acc:0.915742
 lv:
     loss:16759.453711
     acc:0.187215
 le:
     loss:1448.576074
     acc:0.999127
 encoder:
     loss:0.214412
----------------------------


Epoch: [912/1000]:
train:
----------------------------
 tu:
     loss:18284.653616
     acc:0.854586
 tv:
     loss:19806.334893
     acc:0.541087
 lu:
     loss:14029.216343
     acc:0.924519
 lv:
     loss:17005.912904
     acc:0.309895
 le:
     loss:1517.689615
     acc:0.999683
 encoder:
     loss:0.222494
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.452344
     acc:0.852769
 tv:
     loss:19111.225586
     acc:0.488652
 lu:
     loss:13409.287012
     acc:0.914850
 lv:
     loss:16759.908203
     acc:0.187162
 le:
     loss:1448.490021
     acc:0.999145
 encoder:
     loss:0.218275
----------------------------


Epoch: [913/1000]:
train:
----------------------------
 tu:
     loss:18285.510208
     acc:0.854314
 tv:
     loss:19808.807083
     acc:0.540585
 lu:
     loss:14028.766545
     acc:0.924746
 lv:
     loss:17006.414698
     acc:0.309887
 le:
     loss:1517.772644
     acc:0.999664
 encoder:
     loss:0.252658
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.387891
     acc:0.852476
 tv:
     loss:19107.032715
     acc:0.489599
 lu:
     loss:13408.553320
     acc:0.915036
 lv:
     loss:16756.105469
     acc:0.187775
 le:
     loss:1448.325446
     acc:0.999186
 encoder:
     loss:0.226021
----------------------------


Epoch: [914/1000]:
train:
----------------------------
 tu:
     loss:18284.083246
     acc:0.854738
 tv:
     loss:19806.388581
     acc:0.540874
 lu:
     loss:14029.612634
     acc:0.924464
 lv:
     loss:17003.921341
     acc:0.310317
 le:
     loss:1517.793730
     acc:0.999658
 encoder:
     loss:0.238609
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.283887
     acc:0.852460
 tv:
     loss:19112.775879
     acc:0.488369
 lu:
     loss:13408.277295
     acc:0.914978
 lv:
     loss:16754.669141
     acc:0.188930
 le:
     loss:1448.712634
     acc:0.999103
 encoder:
     loss:0.249054
----------------------------


Epoch: [915/1000]:
train:
----------------------------
 tu:
     loss:18285.056357
     acc:0.854487
 tv:
     loss:19808.129372
     acc:0.540651
 lu:
     loss:14027.420263
     acc:0.924782
 lv:
     loss:17002.693791
     acc:0.310404
 le:
     loss:1517.813782
     acc:0.999661
 encoder:
     loss:0.237853
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.928125
     acc:0.852603
 tv:
     loss:19105.347363
     acc:0.490127
 lu:
     loss:13405.584668
     acc:0.915700
 lv:
     loss:16760.768945
     acc:0.187320
 le:
     loss:1448.392706
     acc:0.999169
 encoder:
     loss:0.213760
----------------------------


Epoch: [916/1000]:
train:
----------------------------
 tu:
     loss:18284.145565
     acc:0.854637
 tv:
     loss:19805.007688
     acc:0.541493
 lu:
     loss:14028.351165
     acc:0.924713
 lv:
     loss:17002.478232
     acc:0.310845
 le:
     loss:1517.910227
     acc:0.999640
 encoder:
     loss:0.264108
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.455469
     acc:0.852433
 tv:
     loss:19103.347754
     acc:0.490390
 lu:
     loss:13407.495020
     acc:0.915266
 lv:
     loss:16754.610742
     acc:0.188836
 le:
     loss:1448.155005
     acc:0.999230
 encoder:
     loss:0.204186
----------------------------


Epoch: [917/1000]:
train:
----------------------------
 tu:
     loss:18284.973860
     acc:0.854597
 tv:
     loss:19806.859341
     acc:0.540784
 lu:
     loss:14028.990700
     acc:0.924476
 lv:
     loss:17005.199707
     acc:0.309991
 le:
     loss:1517.837204
     acc:0.999648
 encoder:
     loss:0.186885
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.804395
     acc:0.852600
 tv:
     loss:19108.918750
     acc:0.489114
 lu:
     loss:13408.582764
     acc:0.914954
 lv:
     loss:16758.849023
     acc:0.186843
 le:
     loss:1448.346027
     acc:0.999203
 encoder:
     loss:0.184382
----------------------------


Epoch: [918/1000]:
train:
----------------------------
 tu:
     loss:18284.196369
     acc:0.854593
 tv:
     loss:19805.026447
     acc:0.541203
 lu:
     loss:14028.934797
     acc:0.924551
 lv:
     loss:17004.119016
     acc:0.310372
 le:
     loss:1517.838193
     acc:0.999646
 encoder:
     loss:0.192896
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.685352
     acc:0.852380
 tv:
     loss:19109.660645
     acc:0.489118
 lu:
     loss:13408.573437
     acc:0.915029
 lv:
     loss:16759.000781
     acc:0.187290
 le:
     loss:1448.221942
     acc:0.999245
 encoder:
     loss:0.232884
----------------------------


Epoch: [919/1000]:
train:
----------------------------
 tu:
     loss:18284.548578
     acc:0.854655
 tv:
     loss:19808.364928
     acc:0.540562
 lu:
     loss:14028.441565
     acc:0.924686
 lv:
     loss:17003.977550
     acc:0.309964
 le:
     loss:1517.927740
     acc:0.999625
 encoder:
     loss:0.268228
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.930273
     acc:0.852540
 tv:
     loss:19107.029980
     acc:0.489532
 lu:
     loss:13406.354297
     acc:0.915449
 lv:
     loss:16754.637207
     acc:0.188348
 le:
     loss:1448.574091
     acc:0.999128
 encoder:
     loss:0.234430
----------------------------


Epoch: [920/1000]:
train:
----------------------------
 tu:
     loss:18285.280273
     acc:0.854444
 tv:
     loss:19807.448356
     acc:0.540821
 lu:
     loss:14029.064930
     acc:0.924532
 lv:
     loss:17008.407647
     acc:0.309167
 le:
     loss:1517.996535
     acc:0.999610
 encoder:
     loss:0.271788
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.678027
     acc:0.852351
 tv:
     loss:19107.094141
     acc:0.489696
 lu:
     loss:13406.402002
     acc:0.915470
 lv:
     loss:16758.684570
     acc:0.187149
 le:
     loss:1448.542834
     acc:0.999145
 encoder:
     loss:0.267786
----------------------------


Epoch: [921/1000]:
train:
----------------------------
 tu:
     loss:18285.366404
     acc:0.854386
 tv:
     loss:19808.651731
     acc:0.540656
 lu:
     loss:14029.441656
     acc:0.924492
 lv:
     loss:17009.562568
     acc:0.309227
 le:
     loss:1517.912236
     acc:0.999634
 encoder:
     loss:0.385604
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.046387
     acc:0.852511
 tv:
     loss:19109.790234
     acc:0.489177
 lu:
     loss:13407.835645
     acc:0.915095
 lv:
     loss:16762.625586
     acc:0.186840
 le:
     loss:1448.304791
     acc:0.999188
 encoder:
     loss:0.482113
----------------------------


Epoch: [922/1000]:
train:
----------------------------
 tu:
     loss:18285.161780
     acc:0.854489
 tv:
     loss:19806.006064
     acc:0.541277
 lu:
     loss:14029.353675
     acc:0.924433
 lv:
     loss:17008.476369
     acc:0.309472
 le:
     loss:1517.983810
     acc:0.999613
 encoder:
     loss:0.348120
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.706641
     acc:0.852635
 tv:
     loss:19111.356836
     acc:0.488486
 lu:
     loss:13407.648291
     acc:0.915289
 lv:
     loss:16756.355469
     acc:0.187937
 le:
     loss:1448.389233
     acc:0.999167
 encoder:
     loss:0.292541
----------------------------


Epoch: [923/1000]:
train:
----------------------------
 tu:
     loss:18284.781193
     acc:0.854532
 tv:
     loss:19806.482876
     acc:0.541031
 lu:
     loss:14029.138388
     acc:0.924487
 lv:
     loss:17008.757267
     acc:0.309361
 le:
     loss:1517.851375
     acc:0.999643
 encoder:
     loss:0.443803
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.673242
     acc:0.852659
 tv:
     loss:19110.384668
     acc:0.488741
 lu:
     loss:13406.959668
     acc:0.915383
 lv:
     loss:16748.749609
     acc:0.189555
 le:
     loss:1448.716516
     acc:0.999124
 encoder:
     loss:0.506442
----------------------------


Epoch: [924/1000]:
train:
----------------------------
 tu:
     loss:18284.616892
     acc:0.854604
 tv:
     loss:19806.416106
     acc:0.541043
 lu:
     loss:14029.554949
     acc:0.924498
 lv:
     loss:17007.903173
     acc:0.309266
 le:
     loss:1517.970159
     acc:0.999625
 encoder:
     loss:0.452025
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.639746
     acc:0.852676
 tv:
     loss:19107.291699
     acc:0.489427
 lu:
     loss:13408.731494
     acc:0.914911
 lv:
     loss:16752.359570
     acc:0.189332
 le:
     loss:1448.513501
     acc:0.999148
 encoder:
     loss:0.381566
----------------------------


Epoch: [925/1000]:
train:
----------------------------
 tu:
     loss:18285.493278
     acc:0.854481
 tv:
     loss:19810.097997
     acc:0.540239
 lu:
     loss:14028.384527
     acc:0.924672
 lv:
     loss:17014.489939
     acc:0.308129
 le:
     loss:1517.800162
     acc:0.999657
 encoder:
     loss:0.351812
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.775781
     acc:0.852352
 tv:
     loss:19106.771387
     acc:0.489347
 lu:
     loss:13406.694092
     acc:0.915537
 lv:
     loss:16752.571582
     acc:0.188566
 le:
     loss:1448.691022
     acc:0.999107
 encoder:
     loss:0.249166
----------------------------


Epoch: [926/1000]:
train:
----------------------------
 tu:
     loss:18285.268816
     acc:0.854473
 tv:
     loss:19808.221907
     acc:0.540667
 lu:
     loss:14028.907545
     acc:0.924416
 lv:
     loss:17010.685581
     acc:0.308795
 le:
     loss:1517.682052
     acc:0.999686
 encoder:
     loss:0.281603
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.347656
     acc:0.852442
 tv:
     loss:19109.519727
     acc:0.489121
 lu:
     loss:13409.447266
     acc:0.914707
 lv:
     loss:16750.880371
     acc:0.188687
 le:
     loss:1448.632819
     acc:0.999106
 encoder:
     loss:0.361297
----------------------------


Epoch: [927/1000]:
train:
----------------------------
 tu:
     loss:18283.293979
     acc:0.854885
 tv:
     loss:19808.147291
     acc:0.540812
 lu:
     loss:14029.383028
     acc:0.924542
 lv:
     loss:17009.946017
     acc:0.309106
 le:
     loss:1517.895259
     acc:0.999632
 encoder:
     loss:0.326284
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.652246
     acc:0.852453
 tv:
     loss:19109.066113
     acc:0.489097
 lu:
     loss:13408.826855
     acc:0.914911
 lv:
     loss:16753.849414
     acc:0.188547
 le:
     loss:1449.117212
     acc:0.999041
 encoder:
     loss:0.277510
----------------------------


Epoch: [928/1000]:
train:
----------------------------
 tu:
     loss:18284.859852
     acc:0.854548
 tv:
     loss:19806.301190
     acc:0.541106
 lu:
     loss:14028.570767
     acc:0.924606
 lv:
     loss:17005.720533
     acc:0.310074
 le:
     loss:1517.943731
     acc:0.999635
 encoder:
     loss:0.251977
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.395605
     acc:0.852479
 tv:
     loss:19112.273047
     acc:0.488281
 lu:
     loss:13405.601904
     acc:0.915544
 lv:
     loss:16745.455859
     acc:0.190356
 le:
     loss:1448.172559
     acc:0.999209
 encoder:
     loss:0.270653
----------------------------


Epoch: [929/1000]:
train:
----------------------------
 tu:
     loss:18284.537359
     acc:0.854623
 tv:
     loss:19806.363508
     acc:0.540833
 lu:
     loss:14028.995446
     acc:0.924684
 lv:
     loss:17004.646643
     acc:0.310361
 le:
     loss:1517.829915
     acc:0.999651
 encoder:
     loss:0.293909
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.590430
     acc:0.852311
 tv:
     loss:19108.763379
     acc:0.489286
 lu:
     loss:13406.493018
     acc:0.915545
 lv:
     loss:16748.978418
     acc:0.189145
 le:
     loss:1448.679181
     acc:0.999102
 encoder:
     loss:0.232873
----------------------------


Epoch: [930/1000]:
train:
----------------------------
 tu:
     loss:18284.220896
     acc:0.854593
 tv:
     loss:19806.563136
     acc:0.540827
 lu:
     loss:14027.890727
     acc:0.924787
 lv:
     loss:17002.419025
     acc:0.310493
 le:
     loss:1517.916551
     acc:0.999639
 encoder:
     loss:0.256745
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.481543
     acc:0.852375
 tv:
     loss:19107.713184
     acc:0.489582
 lu:
     loss:13408.608887
     acc:0.914919
 lv:
     loss:16756.980566
     acc:0.187678
 le:
     loss:1448.782837
     acc:0.999107
 encoder:
     loss:0.253465
----------------------------


Epoch: [931/1000]:
train:
----------------------------
 tu:
     loss:18284.993085
     acc:0.854430
 tv:
     loss:19805.640307
     acc:0.541034
 lu:
     loss:14028.493471
     acc:0.924698
 lv:
     loss:17001.517737
     acc:0.310518
 le:
     loss:1517.872534
     acc:0.999638
 encoder:
     loss:0.270136
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.201660
     acc:0.852558
 tv:
     loss:19106.007129
     acc:0.489970
 lu:
     loss:13409.576270
     acc:0.914783
 lv:
     loss:16756.492480
     acc:0.188073
 le:
     loss:1448.680005
     acc:0.999127
 encoder:
     loss:0.281208
----------------------------


Epoch: [932/1000]:
train:
----------------------------
 tu:
     loss:18285.181255
     acc:0.854434
 tv:
     loss:19804.371957
     acc:0.541314
 lu:
     loss:14028.174373
     acc:0.924727
 lv:
     loss:17003.363156
     acc:0.310397
 le:
     loss:1517.861357
     acc:0.999644
 encoder:
     loss:0.326526
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.325000
     acc:0.852454
 tv:
     loss:19110.231348
     acc:0.489041
 lu:
     loss:13409.446533
     acc:0.914641
 lv:
     loss:16753.604492
     acc:0.188613
 le:
     loss:1448.805402
     acc:0.999123
 encoder:
     loss:0.304255
----------------------------


Epoch: [933/1000]:
train:
----------------------------
 tu:
     loss:18284.653820
     acc:0.854456
 tv:
     loss:19804.770848
     acc:0.541334
 lu:
     loss:14027.847384
     acc:0.924783
 lv:
     loss:17000.691384
     acc:0.311337
 le:
     loss:1517.724546
     acc:0.999676
 encoder:
     loss:0.298646
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.964844
     acc:0.852623
 tv:
     loss:19108.010352
     acc:0.489500
 lu:
     loss:13407.120508
     acc:0.915330
 lv:
     loss:16758.320898
     acc:0.187659
 le:
     loss:1448.625623
     acc:0.999125
 encoder:
     loss:0.347478
----------------------------


Epoch: [934/1000]:
train:
----------------------------
 tu:
     loss:18284.906625
     acc:0.854492
 tv:
     loss:19805.366506
     acc:0.541296
 lu:
     loss:14027.312795
     acc:0.924815
 lv:
     loss:17002.872377
     acc:0.310290
 le:
     loss:1517.819796
     acc:0.999652
 encoder:
     loss:0.412998
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.543555
     acc:0.852439
 tv:
     loss:19108.750781
     acc:0.489066
 lu:
     loss:13408.569629
     acc:0.915122
 lv:
     loss:16766.784277
     acc:0.185010
 le:
     loss:1448.628986
     acc:0.999127
 encoder:
     loss:0.287565
----------------------------


Epoch: [935/1000]:
train:
----------------------------
 tu:
     loss:18283.535701
     acc:0.854888
 tv:
     loss:19805.607751
     acc:0.540947
 lu:
     loss:14029.343705
     acc:0.924411
 lv:
     loss:17004.116290
     acc:0.309919
 le:
     loss:1517.696905
     acc:0.999686
 encoder:
     loss:0.257706
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.491406
     acc:0.852514
 tv:
     loss:19109.392285
     acc:0.489174
 lu:
     loss:13407.723828
     acc:0.915246
 lv:
     loss:16756.275293
     acc:0.187939
 le:
     loss:1448.647809
     acc:0.999108
 encoder:
     loss:0.239083
----------------------------


Epoch: [936/1000]:
train:
----------------------------
 tu:
     loss:18285.412813
     acc:0.854352
 tv:
     loss:19806.421307
     acc:0.541121
 lu:
     loss:14027.636208
     acc:0.924810
 lv:
     loss:17002.268555
     acc:0.310832
 le:
     loss:1517.700843
     acc:0.999674
 encoder:
     loss:0.260684
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.122559
     acc:0.852557
 tv:
     loss:19108.385937
     acc:0.489115
 lu:
     loss:13409.104248
     acc:0.914960
 lv:
     loss:16762.096387
     acc:0.187034
 le:
     loss:1448.497119
     acc:0.999107
 encoder:
     loss:0.197702
----------------------------


Epoch: [937/1000]:
train:
----------------------------
 tu:
     loss:18284.996991
     acc:0.854458
 tv:
     loss:19806.236498
     acc:0.540995
 lu:
     loss:14026.851551
     acc:0.924922
 lv:
     loss:17002.549566
     acc:0.310455
 le:
     loss:1517.817671
     acc:0.999656
 encoder:
     loss:0.499641
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.504590
     acc:0.852501
 tv:
     loss:19108.724805
     acc:0.489208
 lu:
     loss:13407.025146
     acc:0.915352
 lv:
     loss:16755.015234
     acc:0.188279
 le:
     loss:1448.360321
     acc:0.999205
 encoder:
     loss:0.510535
----------------------------


Epoch: [938/1000]:
train:
----------------------------
 tu:
     loss:18284.188590
     acc:0.854662
 tv:
     loss:19803.530966
     acc:0.541672
 lu:
     loss:14028.210551
     acc:0.924775
 lv:
     loss:16999.649153
     acc:0.310986
 le:
     loss:1517.730673
     acc:0.999672
 encoder:
     loss:0.403104
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.061719
     acc:0.852657
 tv:
     loss:19110.085449
     acc:0.488530
 lu:
     loss:13408.016260
     acc:0.915338
 lv:
     loss:16751.686621
     acc:0.189349
 le:
     loss:1448.180865
     acc:0.999247
 encoder:
     loss:0.274161
----------------------------


Epoch: [939/1000]:
train:
----------------------------
 tu:
     loss:18284.320574
     acc:0.854592
 tv:
     loss:19806.647927
     acc:0.540950
 lu:
     loss:14027.878202
     acc:0.924847
 lv:
     loss:17002.342240
     acc:0.310583
 le:
     loss:1517.806379
     acc:0.999652
 encoder:
     loss:0.287051
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.821094
     acc:0.852656
 tv:
     loss:19113.245312
     acc:0.487851
 lu:
     loss:13408.075928
     acc:0.915019
 lv:
     loss:16753.877637
     acc:0.188426
 le:
     loss:1448.352539
     acc:0.999189
 encoder:
     loss:0.262032
----------------------------


Epoch: [940/1000]:
train:
----------------------------
 tu:
     loss:18284.264319
     acc:0.854638
 tv:
     loss:19806.470908
     acc:0.540995
 lu:
     loss:14029.736521
     acc:0.924382
 lv:
     loss:17003.345181
     acc:0.310368
 le:
     loss:1517.768685
     acc:0.999671
 encoder:
     loss:0.314956
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.876953
     acc:0.852634
 tv:
     loss:19111.479590
     acc:0.488654
 lu:
     loss:13407.818799
     acc:0.915286
 lv:
     loss:16745.988867
     acc:0.190206
 le:
     loss:1448.515503
     acc:0.999161
 encoder:
     loss:0.274951
----------------------------


Epoch: [941/1000]:
train:
----------------------------
 tu:
     loss:18283.623944
     acc:0.854786
 tv:
     loss:19804.553745
     acc:0.541317
 lu:
     loss:14028.869618
     acc:0.924623
 lv:
     loss:17001.290970
     acc:0.310855
 le:
     loss:1517.832583
     acc:0.999652
 encoder:
     loss:0.255644
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.392480
     acc:0.852680
 tv:
     loss:19108.333789
     acc:0.489483
 lu:
     loss:13408.184180
     acc:0.915064
 lv:
     loss:16757.837109
     acc:0.187433
 le:
     loss:1448.523126
     acc:0.999142
 encoder:
     loss:0.292019
----------------------------


Epoch: [942/1000]:
train:
----------------------------
 tu:
     loss:18284.961948
     acc:0.854519
 tv:
     loss:19805.513467
     acc:0.541145
 lu:
     loss:14028.074877
     acc:0.924750
 lv:
     loss:17003.186512
     acc:0.310328
 le:
     loss:1517.684390
     acc:0.999680
 encoder:
     loss:0.272573
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.970117
     acc:0.852594
 tv:
     loss:19111.258789
     acc:0.488780
 lu:
     loss:13409.121045
     acc:0.914825
 lv:
     loss:16762.932617
     acc:0.186569
 le:
     loss:1448.241705
     acc:0.999205
 encoder:
     loss:0.251604
----------------------------


Epoch: [943/1000]:
train:
----------------------------
 tu:
     loss:18283.794377
     acc:0.854779
 tv:
     loss:19807.186592
     acc:0.540765
 lu:
     loss:14027.336562
     acc:0.924886
 lv:
     loss:17001.716536
     acc:0.310722
 le:
     loss:1517.825742
     acc:0.999655
 encoder:
     loss:0.221207
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.601172
     acc:0.852697
 tv:
     loss:19108.746484
     acc:0.489443
 lu:
     loss:13407.091992
     acc:0.915287
 lv:
     loss:16754.848047
     acc:0.188749
 le:
     loss:1448.181610
     acc:0.999206
 encoder:
     loss:0.207871
----------------------------


Epoch: [944/1000]:
train:
----------------------------
 tu:
     loss:18284.504701
     acc:0.854545
 tv:
     loss:19805.964810
     acc:0.541069
 lu:
     loss:14028.018339
     acc:0.924693
 lv:
     loss:16998.524516
     acc:0.311351
 le:
     loss:1517.752206
     acc:0.999668
 encoder:
     loss:0.261067
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.416504
     acc:0.852672
 tv:
     loss:19115.748730
     acc:0.487606
 lu:
     loss:13406.516650
     acc:0.915597
 lv:
     loss:16763.173828
     acc:0.186030
 le:
     loss:1448.464703
     acc:0.999169
 encoder:
     loss:0.256473
----------------------------


Epoch: [945/1000]:
train:
----------------------------
 tu:
     loss:18284.741438
     acc:0.854442
 tv:
     loss:19805.926542
     acc:0.541047
 lu:
     loss:14028.714015
     acc:0.924556
 lv:
     loss:16999.570619
     acc:0.311125
 le:
     loss:1517.867525
     acc:0.999634
 encoder:
     loss:0.234117
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.538672
     acc:0.852471
 tv:
     loss:19111.301270
     acc:0.488709
 lu:
     loss:13408.189062
     acc:0.915194
 lv:
     loss:16757.811523
     acc:0.187732
 le:
     loss:1448.859949
     acc:0.999083
 encoder:
     loss:0.203063
----------------------------


Epoch: [946/1000]:
train:
----------------------------
 tu:
     loss:18285.353675
     acc:0.854458
 tv:
     loss:19806.179665
     acc:0.541061
 lu:
     loss:14027.186092
     acc:0.924940
 lv:
     loss:17000.193609
     acc:0.311063
 le:
     loss:1517.998544
     acc:0.999612
 encoder:
     loss:0.237550
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.223340
     acc:0.852740
 tv:
     loss:19105.366602
     acc:0.489881
 lu:
     loss:13408.200146
     acc:0.914955
 lv:
     loss:16758.779102
     acc:0.187512
 le:
     loss:1448.311737
     acc:0.999189
 encoder:
     loss:0.225740
----------------------------


Epoch: [947/1000]:
train:
----------------------------
 tu:
     loss:18284.817394
     acc:0.854383
 tv:
     loss:19805.768339
     acc:0.541366
 lu:
     loss:14027.732081
     acc:0.924895
 lv:
     loss:17001.366370
     acc:0.311191
 le:
     loss:1517.761137
     acc:0.999668
 encoder:
     loss:0.224487
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.376172
     acc:0.852780
 tv:
     loss:19107.933789
     acc:0.489522
 lu:
     loss:13408.259521
     acc:0.915305
 lv:
     loss:16753.776172
     acc:0.188295
 le:
     loss:1448.326190
     acc:0.999189
 encoder:
     loss:0.258406
----------------------------


Epoch: [948/1000]:
train:
----------------------------
 tu:
     loss:18285.280398
     acc:0.854429
 tv:
     loss:19807.044172
     acc:0.540724
 lu:
     loss:14028.107683
     acc:0.924746
 lv:
     loss:17001.158669
     acc:0.310981
 le:
     loss:1517.833834
     acc:0.999650
 encoder:
     loss:0.256787
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.933105
     acc:0.852560
 tv:
     loss:19110.137207
     acc:0.488838
 lu:
     loss:13408.889844
     acc:0.914936
 lv:
     loss:16759.714844
     acc:0.187094
 le:
     loss:1448.624957
     acc:0.999065
 encoder:
     loss:0.238912
----------------------------


Epoch: [949/1000]:
train:
----------------------------
 tu:
     loss:18284.516942
     acc:0.854574
 tv:
     loss:19806.500295
     acc:0.540959
 lu:
     loss:14028.908828
     acc:0.924618
 lv:
     loss:16997.847168
     acc:0.311609
 le:
     loss:1517.928602
     acc:0.999630
 encoder:
     loss:0.231971
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.437988
     acc:0.852567
 tv:
     loss:19110.152148
     acc:0.488843
 lu:
     loss:13408.183984
     acc:0.915084
 lv:
     loss:16755.867188
     acc:0.188048
 le:
     loss:1448.968811
     acc:0.999062
 encoder:
     loss:0.229314
----------------------------


Epoch: [950/1000]:
train:
----------------------------
 tu:
     loss:18284.377918
     acc:0.854613
 tv:
     loss:19808.676712
     acc:0.540450
 lu:
     loss:14027.133880
     acc:0.924956
 lv:
     loss:17002.631291
     acc:0.310417
 le:
     loss:1517.782222
     acc:0.999653
 encoder:
     loss:0.228945
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.938770
     acc:0.852552
 tv:
     loss:19109.586719
     acc:0.488984
 lu:
     loss:13406.041211
     acc:0.915767
 lv:
     loss:16757.406445
     acc:0.187533
 le:
     loss:1448.265588
     acc:0.999189
 encoder:
     loss:0.235528
----------------------------


Epoch: [951/1000]:
train:
----------------------------
 tu:
     loss:18285.105616
     acc:0.854377
 tv:
     loss:19807.708984
     acc:0.540909
 lu:
     loss:14028.390171
     acc:0.924631
 lv:
     loss:17002.394259
     acc:0.310772
 le:
     loss:1517.789864
     acc:0.999661
 encoder:
     loss:0.277675
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.327637
     acc:0.852837
 tv:
     loss:19109.407129
     acc:0.488941
 lu:
     loss:13406.097314
     acc:0.915790
 lv:
     loss:16756.624023
     acc:0.187524
 le:
     loss:1448.429401
     acc:0.999146
 encoder:
     loss:0.297192
----------------------------


Epoch: [952/1000]:
train:
----------------------------
 tu:
     loss:18284.522313
     acc:0.854592
 tv:
     loss:19807.974428
     acc:0.540572
 lu:
     loss:14028.691872
     acc:0.924671
 lv:
     loss:17002.409032
     acc:0.310651
 le:
     loss:1517.936186
     acc:0.999630
 encoder:
     loss:0.301082
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.209375
     acc:0.852617
 tv:
     loss:19109.903613
     acc:0.488982
 lu:
     loss:13406.815039
     acc:0.915532
 lv:
     loss:16753.584082
     acc:0.188254
 le:
     loss:1448.472174
     acc:0.999169
 encoder:
     loss:0.277732
----------------------------


Epoch: [953/1000]:
train:
----------------------------
 tu:
     loss:18284.656988
     acc:0.854594
 tv:
     loss:19808.555289
     acc:0.540578
 lu:
     loss:14029.196550
     acc:0.924507
 lv:
     loss:17002.870072
     acc:0.310563
 le:
     loss:1517.807305
     acc:0.999659
 encoder:
     loss:0.258006
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.957813
     acc:0.852635
 tv:
     loss:19107.885352
     acc:0.489255
 lu:
     loss:13408.701465
     acc:0.915068
 lv:
     loss:16764.446094
     acc:0.185925
 le:
     loss:1449.282562
     acc:0.998957
 encoder:
     loss:0.222389
----------------------------


Epoch: [954/1000]:
train:
----------------------------
 tu:
     loss:18284.409180
     acc:0.854624
 tv:
     loss:19806.924044
     acc:0.540895
 lu:
     loss:14029.523948
     acc:0.924357
 lv:
     loss:17000.892714
     acc:0.311109
 le:
     loss:1517.788567
     acc:0.999660
 encoder:
     loss:0.273411
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.358398
     acc:0.852645
 tv:
     loss:19110.019238
     acc:0.488867
 lu:
     loss:13408.206738
     acc:0.915293
 lv:
     loss:16752.917480
     acc:0.187964
 le:
     loss:1448.630029
     acc:0.999104
 encoder:
     loss:0.215338
----------------------------


Epoch: [955/1000]:
train:
----------------------------
 tu:
     loss:18284.431572
     acc:0.854627
 tv:
     loss:19805.943723
     acc:0.541122
 lu:
     loss:14028.652832
     acc:0.924645
 lv:
     loss:17004.672034
     acc:0.309980
 le:
     loss:1518.014864
     acc:0.999612
 encoder:
     loss:0.215180
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.398047
     acc:0.852717
 tv:
     loss:19112.752734
     acc:0.488245
 lu:
     loss:13406.716406
     acc:0.915380
 lv:
     loss:16761.631348
     acc:0.186472
 le:
     loss:1448.667957
     acc:0.999106
 encoder:
     loss:0.217942
----------------------------


Epoch: [956/1000]:
train:
----------------------------
 tu:
     loss:18284.296443
     acc:0.854614
 tv:
     loss:19805.841876
     acc:0.540933
 lu:
     loss:14027.585517
     acc:0.924870
 lv:
     loss:17001.203897
     acc:0.310676
 le:
     loss:1517.868066
     acc:0.999630
 encoder:
     loss:0.259715
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.272070
     acc:0.852561
 tv:
     loss:19109.060742
     acc:0.488788
 lu:
     loss:13405.928271
     acc:0.915784
 lv:
     loss:16758.583496
     acc:0.187449
 le:
     loss:1448.722278
     acc:0.999105
 encoder:
     loss:0.316266
----------------------------


Epoch: [957/1000]:
train:
----------------------------
 tu:
     loss:18284.855787
     acc:0.854514
 tv:
     loss:19804.292401
     acc:0.541367
 lu:
     loss:14027.271428
     acc:0.924910
 lv:
     loss:17000.477857
     acc:0.310870
 le:
     loss:1517.828210
     acc:0.999653
 encoder:
     loss:0.312311
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.995410
     acc:0.852556
 tv:
     loss:19108.725098
     acc:0.489191
 lu:
     loss:13407.451514
     acc:0.915322
 lv:
     loss:16753.224707
     acc:0.188756
 le:
     loss:1448.705823
     acc:0.999084
 encoder:
     loss:0.326813
----------------------------


Epoch: [958/1000]:
train:
----------------------------
 tu:
     loss:18284.185013
     acc:0.854630
 tv:
     loss:19806.654581
     acc:0.541050
 lu:
     loss:14028.360567
     acc:0.924615
 lv:
     loss:16999.108648
     acc:0.311474
 le:
     loss:1517.848910
     acc:0.999653
 encoder:
     loss:0.275119
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.782422
     acc:0.852614
 tv:
     loss:19111.942871
     acc:0.488634
 lu:
     loss:13405.600928
     acc:0.915945
 lv:
     loss:16760.787012
     acc:0.187013
 le:
     loss:1448.712885
     acc:0.999103
 encoder:
     loss:0.291474
----------------------------


Epoch: [959/1000]:
train:
----------------------------
 tu:
     loss:18284.836028
     acc:0.854537
 tv:
     loss:19807.433775
     acc:0.540635
 lu:
     loss:14027.635799
     acc:0.924867
 lv:
     loss:17000.495617
     acc:0.310992
 le:
     loss:1517.883589
     acc:0.999641
 encoder:
     loss:0.261227
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.556641
     acc:0.852720
 tv:
     loss:19103.111621
     acc:0.490224
 lu:
     loss:13407.693457
     acc:0.915311
 lv:
     loss:16756.885449
     acc:0.187498
 le:
     loss:1448.574359
     acc:0.999163
 encoder:
     loss:0.228907
----------------------------


Epoch: [960/1000]:
train:
----------------------------
 tu:
     loss:18285.133641
     acc:0.854367
 tv:
     loss:19807.259766
     acc:0.540678
 lu:
     loss:14027.700638
     acc:0.924843
 lv:
     loss:17000.492449
     acc:0.310767
 le:
     loss:1517.806764
     acc:0.999649
 encoder:
     loss:0.240129
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.790918
     acc:0.852680
 tv:
     loss:19108.005664
     acc:0.489066
 lu:
     loss:13406.762744
     acc:0.915647
 lv:
     loss:16757.629004
     acc:0.187451
 le:
     loss:1448.926941
     acc:0.999041
 encoder:
     loss:0.275931
----------------------------


Epoch: [961/1000]:
train:
----------------------------
 tu:
     loss:18284.242449
     acc:0.854703
 tv:
     loss:19805.932436
     acc:0.541142
 lu:
     loss:14027.474871
     acc:0.924866
 lv:
     loss:16998.714401
     acc:0.311534
 le:
     loss:1518.022887
     acc:0.999602
 encoder:
     loss:0.281205
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.541895
     acc:0.852706
 tv:
     loss:19112.113672
     acc:0.488360
 lu:
     loss:13406.863281
     acc:0.915428
 lv:
     loss:16751.408398
     acc:0.188868
 le:
     loss:1448.560419
     acc:0.999123
 encoder:
     loss:0.447156
----------------------------


Epoch: [962/1000]:
train:
----------------------------
 tu:
     loss:18284.453795
     acc:0.854552
 tv:
     loss:19806.450093
     acc:0.541012
 lu:
     loss:14028.500114
     acc:0.924640
 lv:
     loss:17000.084711
     acc:0.311344
 le:
     loss:1517.779534
     acc:0.999653
 encoder:
     loss:0.382303
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.802539
     acc:0.852595
 tv:
     loss:19108.049219
     acc:0.488961
 lu:
     loss:13406.722217
     acc:0.915643
 lv:
     loss:16764.875098
     acc:0.186332
 le:
     loss:1448.937231
     acc:0.999064
 encoder:
     loss:0.262508
----------------------------


Epoch: [963/1000]:
train:
----------------------------
 tu:
     loss:18284.593523
     acc:0.854619
 tv:
     loss:19808.060876
     acc:0.540510
 lu:
     loss:14028.418309
     acc:0.924595
 lv:
     loss:17001.397257
     acc:0.310676
 le:
     loss:1517.747402
     acc:0.999664
 encoder:
     loss:0.286505
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.734961
     acc:0.852649
 tv:
     loss:19105.337402
     acc:0.490078
 lu:
     loss:13409.163037
     acc:0.915190
 lv:
     loss:16762.104297
     acc:0.186570
 le:
     loss:1448.563562
     acc:0.999148
 encoder:
     loss:0.284738
----------------------------


Epoch: [964/1000]:
train:
----------------------------
 tu:
     loss:18283.826024
     acc:0.854721
 tv:
     loss:19802.795149
     acc:0.541875
 lu:
     loss:14028.352562
     acc:0.924662
 lv:
     loss:17000.094658
     acc:0.311123
 le:
     loss:1517.925240
     acc:0.999635
 encoder:
     loss:0.266808
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.616309
     acc:0.852437
 tv:
     loss:19106.000977
     acc:0.490062
 lu:
     loss:13407.635596
     acc:0.915451
 lv:
     loss:16754.766113
     acc:0.187823
 le:
     loss:1448.299109
     acc:0.999187
 encoder:
     loss:0.228276
----------------------------


Epoch: [965/1000]:
train:
----------------------------
 tu:
     loss:18283.828159
     acc:0.854674
 tv:
     loss:19804.617029
     acc:0.541352
 lu:
     loss:14027.428177
     acc:0.924882
 lv:
     loss:16998.820029
     acc:0.311454
 le:
     loss:1517.778879
     acc:0.999662
 encoder:
     loss:0.227306
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.025488
     acc:0.852861
 tv:
     loss:19104.819629
     acc:0.489946
 lu:
     loss:13406.640332
     acc:0.915558
 lv:
     loss:16759.019336
     acc:0.187504
 le:
     loss:1448.532910
     acc:0.999129
 encoder:
     loss:0.210757
----------------------------


Epoch: [966/1000]:
train:
----------------------------
 tu:
     loss:18285.088470
     acc:0.854410
 tv:
     loss:19804.039823
     acc:0.541591
 lu:
     loss:14027.643998
     acc:0.924733
 lv:
     loss:16999.660633
     acc:0.311296
 le:
     loss:1517.831622
     acc:0.999654
 encoder:
     loss:0.395589
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.277344
     acc:0.852569
 tv:
     loss:19108.123047
     acc:0.489314
 lu:
     loss:13407.621826
     acc:0.915339
 lv:
     loss:16755.638086
     acc:0.188388
 le:
     loss:1448.409161
     acc:0.999206
 encoder:
     loss:0.619653
----------------------------


Epoch: [967/1000]:
train:
----------------------------
 tu:
     loss:18285.255348
     acc:0.854320
 tv:
     loss:19805.025538
     acc:0.541296
 lu:
     loss:14027.643600
     acc:0.924978
 lv:
     loss:16996.135515
     acc:0.312297
 le:
     loss:1517.882303
     acc:0.999641
 encoder:
     loss:0.455388
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.842383
     acc:0.852586
 tv:
     loss:19104.316699
     acc:0.490193
 lu:
     loss:13405.612695
     acc:0.915715
 lv:
     loss:16757.374414
     acc:0.187489
 le:
     loss:1448.610242
     acc:0.999145
 encoder:
     loss:0.331627
----------------------------


Epoch: [968/1000]:
train:
----------------------------
 tu:
     loss:18284.227051
     acc:0.854726
 tv:
     loss:19804.927269
     acc:0.541220
 lu:
     loss:14027.556402
     acc:0.924874
 lv:
     loss:16998.007449
     acc:0.311405
 le:
     loss:1517.805004
     acc:0.999653
 encoder:
     loss:0.281833
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.796582
     acc:0.852540
 tv:
     loss:19106.187402
     acc:0.489740
 lu:
     loss:13407.852930
     acc:0.915192
 lv:
     loss:16757.247461
     acc:0.186829
 le:
     loss:1448.309375
     acc:0.999207
 encoder:
     loss:0.211921
----------------------------


Epoch: [969/1000]:
train:
----------------------------
 tu:
     loss:18284.128952
     acc:0.854659
 tv:
     loss:19804.510924
     acc:0.541413
 lu:
     loss:14027.359489
     acc:0.924869
 lv:
     loss:16992.969568
     acc:0.312588
 le:
     loss:1517.664992
     acc:0.999690
 encoder:
     loss:0.210638
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.598242
     acc:0.852682
 tv:
     loss:19109.066699
     acc:0.489376
 lu:
     loss:13407.265527
     acc:0.915638
 lv:
     loss:16753.464062
     acc:0.188870
 le:
     loss:1448.626324
     acc:0.999145
 encoder:
     loss:0.185655
----------------------------


Epoch: [970/1000]:
train:
----------------------------
 tu:
     loss:18284.685933
     acc:0.854506
 tv:
     loss:19804.569540
     acc:0.541296
 lu:
     loss:14028.425554
     acc:0.924749
 lv:
     loss:16996.856275
     acc:0.311657
 le:
     loss:1517.758078
     acc:0.999662
 encoder:
     loss:0.222931
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.479004
     acc:0.852663
 tv:
     loss:19106.926172
     acc:0.489777
 lu:
     loss:13408.273877
     acc:0.915060
 lv:
     loss:16764.407031
     acc:0.186018
 le:
     loss:1448.516260
     acc:0.999166
 encoder:
     loss:0.239659
----------------------------


Epoch: [971/1000]:
train:
----------------------------
 tu:
     loss:18284.370219
     acc:0.854608
 tv:
     loss:19808.122570
     acc:0.540573
 lu:
     loss:14028.623070
     acc:0.924542
 lv:
     loss:16997.483705
     acc:0.311655
 le:
     loss:1517.823921
     acc:0.999658
 encoder:
     loss:0.270304
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.548242
     acc:0.852643
 tv:
     loss:19108.519043
     acc:0.488879
 lu:
     loss:13407.496777
     acc:0.915453
 lv:
     loss:16754.722266
     acc:0.188384
 le:
     loss:1448.730505
     acc:0.999123
 encoder:
     loss:0.307458
----------------------------


Epoch: [972/1000]:
train:
----------------------------
 tu:
     loss:18284.848349
     acc:0.854572
 tv:
     loss:19804.555233
     acc:0.541220
 lu:
     loss:14028.014376
     acc:0.924708
 lv:
     loss:16996.512150
     acc:0.311719
 le:
     loss:1517.821523
     acc:0.999647
 encoder:
     loss:0.280852
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.925977
     acc:0.852715
 tv:
     loss:19109.973145
     acc:0.488842
 lu:
     loss:13407.685498
     acc:0.915410
 lv:
     loss:16752.959766
     acc:0.188382
 le:
     loss:1448.507080
     acc:0.999182
 encoder:
     loss:0.240641
----------------------------


Epoch: [973/1000]:
train:
----------------------------
 tu:
     loss:18284.726086
     acc:0.854492
 tv:
     loss:19805.228947
     acc:0.541271
 lu:
     loss:14028.101052
     acc:0.924706
 lv:
     loss:16999.353175
     acc:0.311028
 le:
     loss:1517.830182
     acc:0.999646
 encoder:
     loss:0.224370
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.697070
     acc:0.852675
 tv:
     loss:19105.076465
     acc:0.490169
 lu:
     loss:13407.339941
     acc:0.915292
 lv:
     loss:16754.448828
     acc:0.188848
 le:
     loss:1448.411273
     acc:0.999185
 encoder:
     loss:0.195531
----------------------------


Epoch: [974/1000]:
train:
----------------------------
 tu:
     loss:18284.935138
     acc:0.854437
 tv:
     loss:19804.657499
     acc:0.541263
 lu:
     loss:14026.956850
     acc:0.924981
 lv:
     loss:16996.208178
     acc:0.311671
 le:
     loss:1517.957422
     acc:0.999627
 encoder:
     loss:0.204184
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.311914
     acc:0.852528
 tv:
     loss:19108.900098
     acc:0.489226
 lu:
     loss:13408.648389
     acc:0.915125
 lv:
     loss:16746.901367
     acc:0.190476
 le:
     loss:1448.559448
     acc:0.999149
 encoder:
     loss:0.213913
----------------------------


Epoch: [975/1000]:
train:
----------------------------
 tu:
     loss:18285.376624
     acc:0.854508
 tv:
     loss:19804.606195
     acc:0.541230
 lu:
     loss:14026.512150
     acc:0.924891
 lv:
     loss:16994.863792
     acc:0.312206
 le:
     loss:1517.769839
     acc:0.999662
 encoder:
     loss:0.231606
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.738574
     acc:0.852620
 tv:
     loss:19102.778223
     acc:0.490316
 lu:
     loss:13410.823193
     acc:0.914208
 lv:
     loss:16758.651562
     acc:0.187481
 le:
     loss:1448.769232
     acc:0.999101
 encoder:
     loss:0.205906
----------------------------


Epoch: [976/1000]:
train:
----------------------------
 tu:
     loss:18284.849223
     acc:0.854509
 tv:
     loss:19804.465434
     acc:0.541430
 lu:
     loss:14026.420467
     acc:0.925234
 lv:
     loss:16997.625386
     acc:0.311603
 le:
     loss:1517.832297
     acc:0.999654
 encoder:
     loss:0.278793
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.547070
     acc:0.852723
 tv:
     loss:19110.148730
     acc:0.488833
 lu:
     loss:13408.340430
     acc:0.915122
 lv:
     loss:16762.229199
     acc:0.186594
 le:
     loss:1448.561487
     acc:0.999144
 encoder:
     loss:0.243904
----------------------------


Epoch: [977/1000]:
train:
----------------------------
 tu:
     loss:18283.591649
     acc:0.854720
 tv:
     loss:19804.222895
     acc:0.541548
 lu:
     loss:14027.031693
     acc:0.924993
 lv:
     loss:16995.625602
     acc:0.312046
 le:
     loss:1517.736250
     acc:0.999668
 encoder:
     loss:0.282109
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.648828
     acc:0.852658
 tv:
     loss:19111.209180
     acc:0.488817
 lu:
     loss:13407.781006
     acc:0.915515
 lv:
     loss:16761.141406
     acc:0.187066
 le:
     loss:1448.279102
     acc:0.999208
 encoder:
     loss:0.287599
----------------------------


Epoch: [978/1000]:
train:
----------------------------
 tu:
     loss:18283.889932
     acc:0.854686
 tv:
     loss:19802.799827
     acc:0.541755
 lu:
     loss:14027.438386
     acc:0.924969
 lv:
     loss:16995.309945
     acc:0.312045
 le:
     loss:1517.697530
     acc:0.999682
 encoder:
     loss:0.277664
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.578711
     acc:0.852836
 tv:
     loss:19104.398145
     acc:0.490245
 lu:
     loss:13407.819385
     acc:0.915284
 lv:
     loss:16767.344434
     acc:0.185389
 le:
     loss:1448.657196
     acc:0.999121
 encoder:
     loss:0.251740
----------------------------


Epoch: [979/1000]:
train:
----------------------------
 tu:
     loss:18285.085801
     acc:0.854531
 tv:
     loss:19806.629678
     acc:0.540832
 lu:
     loss:14028.284384
     acc:0.924675
 lv:
     loss:17002.284838
     acc:0.310401
 le:
     loss:1517.827827
     acc:0.999648
 encoder:
     loss:0.244788
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.987695
     acc:0.852660
 tv:
     loss:19106.778516
     acc:0.489547
 lu:
     loss:13407.893018
     acc:0.915047
 lv:
     loss:16759.672656
     acc:0.187072
 le:
     loss:1448.639838
     acc:0.999143
 encoder:
     loss:0.226284
----------------------------


Epoch: [980/1000]:
train:
----------------------------
 tu:
     loss:18284.897768
     acc:0.854520
 tv:
     loss:19804.889626
     acc:0.541281
 lu:
     loss:14027.372104
     acc:0.924825
 lv:
     loss:16998.908998
     acc:0.311241
 le:
     loss:1517.783831
     acc:0.999662
 encoder:
     loss:0.241861
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.571191
     acc:0.852742
 tv:
     loss:19109.116504
     acc:0.489130
 lu:
     loss:13406.973682
     acc:0.915320
 lv:
     loss:16751.909375
     acc:0.188712
 le:
     loss:1448.746698
     acc:0.999084
 encoder:
     loss:0.212705
----------------------------


Epoch: [981/1000]:
train:
----------------------------
 tu:
     loss:18284.960824
     acc:0.854509
 tv:
     loss:19806.183957
     acc:0.541073
 lu:
     loss:14027.073492
     acc:0.924982
 lv:
     loss:16995.437738
     acc:0.311921
 le:
     loss:1517.916084
     acc:0.999636
 encoder:
     loss:0.270128
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.578320
     acc:0.852819
 tv:
     loss:19106.058789
     acc:0.489686
 lu:
     loss:13405.472998
     acc:0.915518
 lv:
     loss:16759.790039
     acc:0.186947
 le:
     loss:1448.687262
     acc:0.999085
 encoder:
     loss:0.333340
----------------------------


Epoch: [982/1000]:
train:
----------------------------
 tu:
     loss:18284.447300
     acc:0.854599
 tv:
     loss:19805.388150
     acc:0.541447
 lu:
     loss:14028.450865
     acc:0.924160
 lv:
     loss:16998.403377
     acc:0.311099
 le:
     loss:1517.913268
     acc:0.999638
 encoder:
     loss:0.248181
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.971094
     acc:0.852656
 tv:
     loss:19106.735938
     acc:0.489718
 lu:
     loss:13406.771924
     acc:0.915494
 lv:
     loss:16760.583203
     acc:0.187260
 le:
     loss:1448.769281
     acc:0.999085
 encoder:
     loss:0.216084
----------------------------


Epoch: [983/1000]:
train:
----------------------------
 tu:
     loss:18284.515432
     acc:0.854602
 tv:
     loss:19806.132370
     acc:0.540787
 lu:
     loss:14028.500432
     acc:0.924726
 lv:
     loss:16996.658623
     acc:0.311679
 le:
     loss:1517.971261
     acc:0.999617
 encoder:
     loss:0.348829
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.381738
     acc:0.852533
 tv:
     loss:19099.973145
     acc:0.491289
 lu:
     loss:13407.258008
     acc:0.915292
 lv:
     loss:16755.234277
     acc:0.188426
 le:
     loss:1448.851459
     acc:0.999065
 encoder:
     loss:0.225792
----------------------------


Epoch: [984/1000]:
train:
----------------------------
 tu:
     loss:18283.473292
     acc:0.854767
 tv:
     loss:19805.060501
     acc:0.541236
 lu:
     loss:14027.533180
     acc:0.924833
 lv:
     loss:16997.616234
     acc:0.311364
 le:
     loss:1517.893024
     acc:0.999641
 encoder:
     loss:0.222597
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.979492
     acc:0.852715
 tv:
     loss:19104.048535
     acc:0.490244
 lu:
     loss:13407.330078
     acc:0.915437
 lv:
     loss:16759.469141
     acc:0.188187
 le:
     loss:1448.462408
     acc:0.999185
 encoder:
     loss:0.198114
----------------------------


Epoch: [985/1000]:
train:
----------------------------
 tu:
     loss:18284.663688
     acc:0.854562
 tv:
     loss:19805.405398
     acc:0.541110
 lu:
     loss:14028.960790
     acc:0.924602
 lv:
     loss:16998.434945
     acc:0.311396
 le:
     loss:1517.857135
     acc:0.999648
 encoder:
     loss:0.188064
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.664746
     acc:0.852699
 tv:
     loss:19106.269531
     acc:0.489651
 lu:
     loss:13406.500049
     acc:0.915430
 lv:
     loss:16756.782813
     acc:0.187816
 le:
     loss:1448.873669
     acc:0.999101
 encoder:
     loss:0.193228
----------------------------


Epoch: [986/1000]:
train:
----------------------------
 tu:
     loss:18284.070789
     acc:0.854694
 tv:
     loss:19805.062693
     acc:0.541263
 lu:
     loss:14027.152207
     acc:0.924988
 lv:
     loss:17001.088765
     acc:0.310912
 le:
     loss:1517.806046
     acc:0.999658
 encoder:
     loss:0.247451
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.265137
     acc:0.852558
 tv:
     loss:19109.056836
     acc:0.488817
 lu:
     loss:13408.175439
     acc:0.915088
 lv:
     loss:16754.307129
     acc:0.188295
 le:
     loss:1448.566791
     acc:0.999129
 encoder:
     loss:0.286356
----------------------------


Epoch: [987/1000]:
train:
----------------------------
 tu:
     loss:18284.997252
     acc:0.854410
 tv:
     loss:19804.523267
     acc:0.541227
 lu:
     loss:14027.592955
     acc:0.924783
 lv:
     loss:16996.285293
     acc:0.311799
 le:
     loss:1517.841025
     acc:0.999648
 encoder:
     loss:0.269371
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17430.240625
     acc:0.852449
 tv:
     loss:19107.997754
     acc:0.489555
 lu:
     loss:13406.789209
     acc:0.915529
 lv:
     loss:16761.069238
     acc:0.186828
 le:
     loss:1448.580817
     acc:0.999144
 encoder:
     loss:0.237104
----------------------------


Epoch: [988/1000]:
train:
----------------------------
 tu:
     loss:18284.020292
     acc:0.854612
 tv:
     loss:19804.599394
     acc:0.541384
 lu:
     loss:14026.964106
     acc:0.925113
 lv:
     loss:16997.578307
     acc:0.311621
 le:
     loss:1517.840125
     acc:0.999656
 encoder:
     loss:0.287162
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.712988
     acc:0.852555
 tv:
     loss:19109.302930
     acc:0.489282
 lu:
     loss:13409.561963
     acc:0.914566
 lv:
     loss:16767.241211
     acc:0.184985
 le:
     loss:1448.662701
     acc:0.999108
 encoder:
     loss:0.276137
----------------------------


Epoch: [989/1000]:
train:
----------------------------
 tu:
     loss:18283.990359
     acc:0.854728
 tv:
     loss:19805.614042
     acc:0.541070
 lu:
     loss:14027.820676
     acc:0.924743
 lv:
     loss:16995.687591
     acc:0.311904
 le:
     loss:1517.743209
     acc:0.999672
 encoder:
     loss:0.273372
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.695020
     acc:0.852768
 tv:
     loss:19110.447168
     acc:0.488939
 lu:
     loss:13407.444141
     acc:0.915293
 lv:
     loss:16756.416211
     acc:0.188599
 le:
     loss:1448.621326
     acc:0.999145
 encoder:
     loss:0.292684
----------------------------


Epoch: [990/1000]:
train:
----------------------------
 tu:
     loss:18284.586346
     acc:0.854569
 tv:
     loss:19806.892737
     acc:0.540870
 lu:
     loss:14028.344715
     acc:0.924683
 lv:
     loss:17000.360181
     acc:0.311282
 le:
     loss:1517.845188
     acc:0.999645
 encoder:
     loss:0.258542
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.570801
     acc:0.852680
 tv:
     loss:19113.360449
     acc:0.488552
 lu:
     loss:13408.703467
     acc:0.915042
 lv:
     loss:16761.313281
     acc:0.186789
 le:
     loss:1448.296674
     acc:0.999226
 encoder:
     loss:0.211172
----------------------------


Epoch: [991/1000]:
train:
----------------------------
 tu:
     loss:18285.094931
     acc:0.854438
 tv:
     loss:19805.673011
     acc:0.541057
 lu:
     loss:14028.674316
     acc:0.924692
 lv:
     loss:16997.254906
     acc:0.311476
 le:
     loss:1517.782420
     acc:0.999665
 encoder:
     loss:0.201620
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.214355
     acc:0.852823
 tv:
     loss:19108.772168
     acc:0.489157
 lu:
     loss:13408.317627
     acc:0.915144
 lv:
     loss:16743.772461
     acc:0.190971
 le:
     loss:1449.064691
     acc:0.999022
 encoder:
     loss:0.196513
----------------------------


Epoch: [992/1000]:
train:
----------------------------
 tu:
     loss:18285.141976
     acc:0.854434
 tv:
     loss:19808.427689
     acc:0.540448
 lu:
     loss:14026.790936
     acc:0.925115
 lv:
     loss:16998.844409
     acc:0.311267
 le:
     loss:1517.843800
     acc:0.999640
 encoder:
     loss:0.236851
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.840723
     acc:0.852703
 tv:
     loss:19105.638574
     acc:0.490129
 lu:
     loss:13407.351416
     acc:0.915442
 lv:
     loss:16755.305371
     acc:0.187803
 le:
     loss:1448.584857
     acc:0.999182
 encoder:
     loss:0.219097
----------------------------


Epoch: [993/1000]:
train:
----------------------------
 tu:
     loss:18285.345567
     acc:0.854362
 tv:
     loss:19805.394043
     acc:0.541055
 lu:
     loss:14027.160031
     acc:0.924933
 lv:
     loss:16998.833678
     acc:0.311040
 le:
     loss:1517.936232
     acc:0.999632
 encoder:
     loss:0.255428
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.720508
     acc:0.852598
 tv:
     loss:19112.112793
     acc:0.488433
 lu:
     loss:13409.210645
     acc:0.915009
 lv:
     loss:16755.545801
     acc:0.188270
 le:
     loss:1448.733209
     acc:0.999103
 encoder:
     loss:0.273732
----------------------------


Epoch: [994/1000]:
train:
----------------------------
 tu:
     loss:18284.670898
     acc:0.854484
 tv:
     loss:19807.995708
     acc:0.540523
 lu:
     loss:14026.666663
     acc:0.925039
 lv:
     loss:16995.049918
     acc:0.312059
 le:
     loss:1517.774337
     acc:0.999665
 encoder:
     loss:0.237806
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.361816
     acc:0.852857
 tv:
     loss:19111.522168
     acc:0.488781
 lu:
     loss:13408.970557
     acc:0.914875
 lv:
     loss:16758.469336
     acc:0.187317
 le:
     loss:1448.343756
     acc:0.999207
 encoder:
     loss:0.185882
----------------------------


Epoch: [995/1000]:
train:
----------------------------
 tu:
     loss:18284.647802
     acc:0.854495
 tv:
     loss:19805.242131
     acc:0.541259
 lu:
     loss:14026.020769
     acc:0.925159
 lv:
     loss:16996.095533
     acc:0.312004
 le:
     loss:1517.918836
     acc:0.999637
 encoder:
     loss:0.209218
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.644434
     acc:0.852750
 tv:
     loss:19109.542969
     acc:0.489029
 lu:
     loss:13410.017480
     acc:0.914661
 lv:
     loss:16755.282129
     acc:0.188461
 le:
     loss:1448.688098
     acc:0.999104
 encoder:
     loss:0.217476
----------------------------


Epoch: [996/1000]:
train:
----------------------------
 tu:
     loss:18284.990836
     acc:0.854471
 tv:
     loss:19804.628077
     acc:0.541259
 lu:
     loss:14027.005258
     acc:0.924945
 lv:
     loss:16997.959484
     acc:0.311406
 le:
     loss:1517.746968
     acc:0.999664
 encoder:
     loss:0.312802
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.888672
     acc:0.852633
 tv:
     loss:19109.564648
     acc:0.488821
 lu:
     loss:13408.527686
     acc:0.914981
 lv:
     loss:16759.431934
     acc:0.187377
 le:
     loss:1448.225751
     acc:0.999226
 encoder:
     loss:0.260146
----------------------------


Epoch: [997/1000]:
train:
----------------------------
 tu:
     loss:18283.436137
     acc:0.854862
 tv:
     loss:19802.611146
     acc:0.541902
 lu:
     loss:14027.876181
     acc:0.924819
 lv:
     loss:16993.036258
     acc:0.312901
 le:
     loss:1517.713766
     acc:0.999684
 encoder:
     loss:0.252451
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.227246
     acc:0.852700
 tv:
     loss:19107.201562
     acc:0.489493
 lu:
     loss:13407.640186
     acc:0.915337
 lv:
     loss:16757.456152
     acc:0.187656
 le:
     loss:1448.532599
     acc:0.999165
 encoder:
     loss:0.225586
----------------------------


Epoch: [998/1000]:
train:
----------------------------
 tu:
     loss:18284.955430
     acc:0.854456
 tv:
     loss:19803.785417
     acc:0.541629
 lu:
     loss:14027.442201
     acc:0.924775
 lv:
     loss:16993.498058
     acc:0.312596
 le:
     loss:1517.763626
     acc:0.999672
 encoder:
     loss:0.238082
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.828320
     acc:0.852736
 tv:
     loss:19113.548926
     acc:0.488241
 lu:
     loss:13407.812402
     acc:0.915359
 lv:
     loss:16756.377637
     acc:0.188170
 le:
     loss:1448.760413
     acc:0.999086
 encoder:
     loss:0.240435
----------------------------


Epoch: [999/1000]:
train:
----------------------------
 tu:
     loss:18286.124693
     acc:0.854252
 tv:
     loss:19805.630621
     acc:0.540962
 lu:
     loss:14027.441577
     acc:0.924902
 lv:
     loss:16999.897518
     acc:0.310682
 le:
     loss:1518.019639
     acc:0.999618
 encoder:
     loss:0.296867
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.876855
     acc:0.852740
 tv:
     loss:19114.230469
     acc:0.488266
 lu:
     loss:13406.737988
     acc:0.915586
 lv:
     loss:16757.685547
     acc:0.187573
 le:
     loss:1448.645123
     acc:0.999145
 encoder:
     loss:0.517554
----------------------------


Epoch: [1000/1000]:
train:
----------------------------
 tu:
     loss:18285.112589
     acc:0.854401
 tv:
     loss:19805.333121
     acc:0.541107
 lu:
     loss:14029.160974
     acc:0.924443
 lv:
     loss:16997.890863
     acc:0.311307
 le:
     loss:1517.744542
     acc:0.999672
 encoder:
     loss:0.425971
----------------------------
valid:
step: [0/200]
----------------------------
 tu:
     loss:17429.327148
     acc:0.852736
 tv:
     loss:19106.898730
     acc:0.489597
 lu:
     loss:13406.916260
     acc:0.915307
 lv:
     loss:16751.126660
     acc:0.189162
 le:
     loss:1448.295642
     acc:0.999190
 encoder:
     loss:0.302314
----------------------------


